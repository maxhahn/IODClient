{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "INFO:rpy2.situation:cffi mode is CFFI_MODE.ANY\n",
      "INFO:rpy2.situation:R home found: /opt/homebrew/Caskroom/miniforge/base/envs/promotion/lib/R\n",
      "INFO:rpy2.situation:R library path: \n",
      "INFO:rpy2.situation:LD_LIBRARY_PATH: \n",
      "INFO:rpy2.rinterface_lib.embedded:Default options to initialize R: rpy2, --quiet, --no-save\n",
      "INFO:rpy2.rinterface_lib.embedded:R is already initialized. No need to initialize.\n"
     ]
    }
   ],
   "source": [
    "import fedci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain, combinations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from pgmpy.estimators import CITests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, x_label, s_labels, p_val):\n",
    "        self.y_label = y_label\n",
    "        self.x_label = x_label\n",
    "        self.s_labels = s_labels\n",
    "        self.p_val = p_val\n",
    "        \n",
    "class CategoricalLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, t0s, t1s, num_cats):\n",
    "        assert len(t0s) > 0\n",
    "        assert len(t1s) > 0\n",
    "        assert len(t0s[0].X_labels) + 1 == len(t1s[0].X_labels)\n",
    "        # TODO: assert more data integrity\n",
    "        #assert t0s[0].y_label == t1s[0].y_label\n",
    "        \n",
    "        self.y_label = y_label\n",
    "        self.x_label = (set(t1s[0].X_labels) - set(t0s[0].X_labels)).pop()\n",
    "        self.s_labels = t0s[0].X_labels\n",
    "        self.p_val = self._run_likelihood_test(t0s, t1s, num_cats)\n",
    "        self.p_val = round(self.p_val, 4)\n",
    "        \n",
    "    def _run_likelihood_test(self, t0s, t1s, num_cats):\n",
    "        \n",
    "        # t1 should always encompass more regressors -> less client can fulfill this\n",
    "        #assert len(self.t1.providing_clients) < len(self.t0.providing_clients)\n",
    "        \n",
    "        providing_clients = t1s[0].providing_clients\n",
    "        \n",
    "        t0_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t0s])\n",
    "        t1_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t1s])\n",
    "        \n",
    "        # d_y = num cats\n",
    "        # DOF Z = size cond set\n",
    "        # DOF X = 1\n",
    "        t0_dof = (num_cats-1)*(len(self.s_labels)+1) # (d_y - 1)*(DOF(Z)+1)\n",
    "        t1_dof = (num_cats-1)*(len(self.s_labels)+2) # (d_y - 1)*(DOF(Z)+DOF(X)+1)\n",
    "        t = -2*(t0_llf - t1_llf)\n",
    "        \n",
    "        p_val = stats.chi2.sf(t, t1_dof-t0_dof)\n",
    "        \n",
    "        return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SAMPLES = 1_000\n",
    "\n",
    "TOTAL_FEATURES = 4\n",
    "FEATURES_PER_CLIENT = 4\n",
    "\n",
    "possible_dags = [\n",
    "    \"pdsep_g\",\n",
    "    \"collider\",\n",
    "    \"fork\",\n",
    "    \"chain4\",\n",
    "    \"descColl\",\n",
    "    \"2descColl\",\n",
    "    \"iv\"\n",
    "]\n",
    "\n",
    "# TODO: possible_dags to dict or at least store num of vars for each one\n",
    "chosen_dag = possible_dags[3]\n",
    "\n",
    "\n",
    "server_id_pattern = 'dag_{}_{}c'\n",
    "\n",
    "client_configurations = [1,3, 5]\n",
    "\n",
    "max_regressors = None\n",
    "\n",
    "\n",
    "alpha_comparisons = [0.01, 0.05, 0.1]\n",
    "equality_tolerance = 1e-4\n",
    "\n",
    "\n",
    "log_filepattern = './log-{}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(dag_type, num_samples, num_vars):\n",
    "    with (ro.default_converter + pandas2ri.converter).context():\n",
    "        ro.r['source']('./app/scripts/example_data.r')\n",
    "        get_example_data_f = ro.globalenv['get_example_data']\n",
    "\n",
    "        result = get_example_data_f(dag_type, 1, num_samples, num_vars)\n",
    "        \n",
    "    return list(result.items())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_servers(client_configurations, data):\n",
    "    servers = {}    \n",
    "\n",
    "    for splits in client_configurations:\n",
    "        clients = {i:fedci.Client(pl.from_pandas(chunk)) for i,chunk in enumerate(np.array_split(data.to_pandas(), splits))}\n",
    "        servers[server_id_pattern.format(chosen_dag, splits)] = fedci.Server(clients, max_regressors=max_regressors)\n",
    "    return servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_tests(available_data):\n",
    "\n",
    "    possible_tests = []\n",
    "    max_conditioning_set_size = min(len(available_data), max_regressors) if max_regressors is not None else len(available_data)\n",
    "\n",
    "    for y_var in available_data:\n",
    "        set_of_regressors = available_data - {y_var}\n",
    "        for x_var in set_of_regressors:\n",
    "            set_of_conditioning_variables = set_of_regressors - {x_var}\n",
    "            conditioning_sets = chain.from_iterable(combinations(set_of_conditioning_variables, r) for r in range(0,max_conditioning_set_size))\n",
    "            possible_tests.extend([(y_var, x_var, sorted(list(s_labels))) for s_labels in conditioning_sets])\n",
    "            \n",
    "    return possible_tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycit import citest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mixed_independence(continuous, categorical):\n",
    "    # ANOVA\n",
    "    categories = np.unique(categorical)\n",
    "    groups = [continuous[categorical == category] for category in categories]\n",
    "    f_statistic, p_value = stats.f_oneway(*groups)\n",
    "    #print(f\"ANOVA F-statistic: {f_statistic}, p-value: {p_value}\")\n",
    "\n",
    "    # If categorical is binary, you can also use point-biserial correlation\n",
    "    #if len(categories) == 2:\n",
    "    #    point_biserial_corr, p_value = stats.pointbiserialr(categorical, continuous)\n",
    "    #    print(f\"Point-biserial correlation: {point_biserial_corr}, p-value: {p_value}\")\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_tests(data, possible_tests):  \n",
    "    ground_truth_tests = []\n",
    "\n",
    "    for test in possible_tests:\n",
    "        if len(test[2]) > 0:\n",
    "            if data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.String:\n",
    "                #print('A')\n",
    "                X = data[test[0]].to_numpy()\n",
    "                Y = data[test[1]].to_numpy()\n",
    "                Z = data[test[2]].to_numpy()\n",
    "                pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            elif data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.Float64:\n",
    "                #print('B')\n",
    "                X = data[test[0]].to_numpy()\n",
    "                Y = data[test[1]].to_numpy()\n",
    "                Z = data[test[2]].to_numpy()\n",
    "                pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.String:\n",
    "                #print('C')\n",
    "                X = data[test[0]].to_numpy()\n",
    "                Y = data[test[1]].to_numpy()\n",
    "                Z = data[test[2]].to_numpy()\n",
    "                pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.Float64:\n",
    "                #print('D')\n",
    "                _, pvalue = CITests.pearsonr(test[1], test[0], list(test[2]), data.cast(pl.Float64).to_pandas(), boolean=False)\n",
    "            else:\n",
    "                assert False, 'no fitting test'\n",
    "        else:\n",
    "            if data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.String:\n",
    "                print('CAT TO CAT WO COND SET')\n",
    "            elif data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.Float64:\n",
    "                #print('E')\n",
    "                X = data[test[0]].to_numpy()\n",
    "                Y = data[test[1]].to_numpy().astype(float)\n",
    "                pvalue = test_mixed_independence(Y, X)\n",
    "            elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.String:\n",
    "                #print('F')\n",
    "                X = data[test[0]].to_numpy().astype(float)\n",
    "                Y = data[test[1]].to_numpy()\n",
    "                pvalue = test_mixed_independence(X, Y)\n",
    "            elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.Float64:\n",
    "                #print('G')\n",
    "                v0 = data[test[0]]\n",
    "                v1 = data[test[1]]\n",
    "                _, pvalue = stats.pearsonr(v0, v1)\n",
    "            else:\n",
    "                assert False, 'no fitting test w/o conditiong set'\n",
    "        pvalue = round(pvalue,4)\n",
    "\n",
    "        #print(test, pvalue)\n",
    "                \n",
    "        ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), pvalue))\n",
    "    return ground_truth_tests\n",
    "# TODO: with and without conditioning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_tests(data, possible_tests):    \n",
    "    ground_truth_tests = []\n",
    "\n",
    "    for test in possible_tests:\n",
    "        print(test)\n",
    "\n",
    "        if len(test[2]) > 0:\n",
    "            X = data[test[0]].to_numpy()\n",
    "            Y = data[test[1]].to_numpy()\n",
    "            Z = data[test[2]].to_numpy()\n",
    "            pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 2})\n",
    "        else:\n",
    "            X = data[test[0]].to_numpy()\n",
    "            Y = data[test[1]].to_numpy().astype(float)\n",
    "            pvalue = test_mixed_independence(X, Y)\n",
    "\n",
    "        pvalue = round(pvalue,4)\n",
    "        \n",
    "        ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), pvalue))\n",
    "    return ground_truth_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_tests(data, possible_tests):\n",
    "    ground_truth_tests = []\n",
    "\n",
    "    for test in possible_tests:\n",
    "        if len(test[2]) > 0:\n",
    "            #v0 = data[test[0]].values\n",
    "            #v1 = data[test[1]].values\n",
    "            #s = data[list(test[2])].values\n",
    "            #p0 = test[3]\n",
    "            #p1 = citest(v0, v1, s, test_args={'statistic': 'ksg_cmi', 'n_jobs': 8})\n",
    "            \n",
    "            _, p1 = CITests.pearsonr(test[1], test[0], list(test[2]), data.cast(pl.Float64).to_pandas(), boolean=False)\n",
    "        else:\n",
    "            \n",
    "            #dummied_data = data.to_dummies(cs.string(), separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            #v0 = data[test[0]].cast(pl.Float64).to_pandas()\n",
    "            #v1 = data[test[1]].cast(pl.Float64).to_pandas()\n",
    "            \n",
    "            d0 = data[test[0]]\n",
    "            d1 = data[test[1]]\n",
    "\n",
    "            \n",
    "            #v0 = d0.to_dummies(cs.string(), separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            #v1 = d1.to_dummies(cs.string(), separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            \n",
    "            v0 = d0.to_dummies(separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            v1 = d1.to_dummies(separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            \n",
    "            \n",
    "            _, p1 = stats.pearsonr(v0, v1)\n",
    "            \n",
    "        p1 = round(p1,4)\n",
    "        \n",
    "        ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), p1))\n",
    "    return ground_truth_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_categorical_likelihood_tests(tests, category_expressions, reversed_category_expressions):\n",
    "    #category_expressions = servers['dag_chain4_1c'].category_expressions\n",
    "    #reversed_category_expressions = servers['dag_chain4_1c'].reversed_category_expressions\n",
    "    #tests = server_ci_tests['dag_chain4_1c']\n",
    "\n",
    "    updated_tests = []\n",
    "    for test in tests:\n",
    "        if test.y_label not in reversed_category_expressions:\n",
    "            updated_tests.append(test)\n",
    "            continue\n",
    "        \n",
    "        category_label = reversed_category_expressions[test.y_label]\n",
    "        \n",
    "        # Only run if the current test is the first category. This avoids duplicate tests\n",
    "        if category_expressions[category_label][0] != test.y_label:\n",
    "            continue\n",
    "        \n",
    "        categorical_test_group = []\n",
    "        for test_lookup in tests:\n",
    "            if test_lookup.y_label in category_expressions[category_label] and test_lookup.x_label == test.x_label and sorted(test_lookup.s_labels) == sorted(test.s_labels):\n",
    "                categorical_test_group.append(test_lookup)\n",
    "                \n",
    "        lrt = CategoricalLikelihoodRatioTest(category_label, [t.t0 for t in categorical_test_group], [t.t1 for t in categorical_test_group], len(category_expressions[category_label]))\n",
    "        updated_tests.append(lrt)\n",
    "        \n",
    "    return updated_tests\n",
    "\n",
    "\n",
    "def get_server_test_results(servers):\n",
    "    testing_rounds = {k:v.testing_engine.finished_rounds for k,v in servers.items()}\n",
    "    likelihood_tests = {k:fedci.get_likelihood_tests(v) for k,v in testing_rounds.items()}\n",
    "    # fix up categorical tests\n",
    "    likelihood_tests = {k:group_categorical_likelihood_tests(v, servers[k].category_expressions, servers[k].reversed_category_expressions) for k,v in likelihood_tests.items()}\n",
    "    return likelihood_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_server_evaluation(ground_truth_tests, server_ci_tests):\n",
    "    p_value_comparison = {k:[] for k in server_ci_tests.keys()}\n",
    "    missing_test = {k:0 for k in server_ci_tests.keys()}\n",
    "    \n",
    "    for test in ground_truth_tests:\n",
    "        for k in server_ci_tests.keys():\n",
    "            matching_test = [t for t in server_ci_tests[k] if t.y_label == test.y_label and t.x_label == test.x_label and sorted(t.s_labels) == sorted(test.s_labels)]\n",
    "            if len(matching_test) == 0:\n",
    "                print(f'No matching test in {k} for {test}')\n",
    "                missing_test[k] += 1\n",
    "                continue\n",
    "            assert len(matching_test) == 1\n",
    "            matching_test = matching_test[0]          \n",
    "            p_value_comparison[k].append((matching_test.p_val, test.p_val))\n",
    "        \n",
    "    missing_test = {k:(v+1)/(len(server_ci_tests[k])+1) for k,v in missing_test.items()}\n",
    "    return p_value_comparison, missing_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct_alpha_thresholdings(data, alpha):\n",
    "    c = sum([1 for a,b in data if (a < alpha and b < alpha) or (a > alpha and b > alpha)]) / len(data)\n",
    "    return c\n",
    "\n",
    "def count_correct_pval(data, tolerance=1e-4):\n",
    "    c = sum([1 for a,b in data if abs(a-b)<tolerance]) / len(data)\n",
    "    return c\n",
    "\n",
    "def evaluate_results(p_value_comparison, alphas, tolerance):\n",
    "    result_alpha = {}\n",
    "    result_equality = {}\n",
    "    for k,v in p_value_comparison.items():\n",
    "        result_alpha[k] = {}\n",
    "        result_equality[k] = count_correct_pval(v, tolerance)\n",
    "        for alpha in alphas:\n",
    "            result_alpha[k][alpha] = count_correct_alpha_thresholdings(v,alpha)\n",
    "    return result_alpha, result_equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records(servers, alpha_tests, equality_tests, missed_tests, total_features, features_per_client):\n",
    "    results = []\n",
    "    for server_id in servers.keys():\n",
    "        server = servers[server_id]\n",
    "        alpha_test = alpha_tests[server_id]\n",
    "        \n",
    "        r = {\n",
    "            'chosen_dag': chosen_dag,\n",
    "            'num_clients': len(server.clients),\n",
    "            'num_samples': TOTAL_SAMPLES,\n",
    "            'same_p_val': equality_tests[server_id],\n",
    "            'missed_tests': missed_tests[server_id],\n",
    "            'total_features': total_features,\n",
    "            'features_per_client': features_per_client\n",
    "        }\n",
    "        for alpha, alpha_result in alpha_test.items():\n",
    "            r[f'correctness_alpha_{alpha}'] = alpha_result\n",
    "        results.append(r)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_add_row(data, file):\n",
    "    with open(file, 'a') as f:\n",
    "        row = ','.join([str(d) for d in data]) + '\\n'\n",
    "        f.write(row)\n",
    "            \n",
    "\n",
    "def write_records(i, file, data):\n",
    "    if len(data) == 0:\n",
    "        return\n",
    "    curr_file = file.format(i)\n",
    "    if not os.path.exists(curr_file):\n",
    "        csv_add_row(list(data[0].keys()), curr_file)\n",
    "    for entry in data:\n",
    "        csv_add_row(entry.values(), curr_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(i):\n",
    "    print('Step 1/6 --> Setup')\n",
    "    data = pl.read_parquet('./fedci/testdata.parquet')#.to_dummies(cs.categorical()).cast(pl.Int32).to_pandas()\n",
    "    servers = get_servers(client_configurations, data)\n",
    "\n",
    "    print('Step 2/6 --> Run Tests')\n",
    "    for server in servers.values(): server.run_tests()\n",
    "\n",
    "    print('Step 3/6 --> Collect Results')\n",
    "    possible_tests = get_possible_tests(set(data.columns))\n",
    "    ground_truth_tests = get_ground_truth_tests(data, possible_tests)\n",
    "    server_ci_tests = get_server_test_results(servers) \n",
    "\n",
    "    print('Step 4/6 --> Prepare Evaluation')\n",
    "    p_val_comparisons, missed_tests = prepare_server_evaluation(ground_truth_tests, server_ci_tests)\n",
    "\n",
    "    print('Step 5/6 --> Run Evaluation')\n",
    "    alpha_tests, equality_tests = evaluate_results(p_val_comparisons, alpha_comparisons, equality_tolerance)\n",
    "\n",
    "    print('Step 6/6 --> Log Results')\n",
    "    records = get_records(servers, alpha_tests, equality_tests, missed_tests, TOTAL_FEATURES, FEATURES_PER_CLIENT)\n",
    "    \n",
    "    write_records(i, log_filepattern, records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/6 --> Setup\n",
      "Step 2/6 --> Run Tests\n",
      "Step 3/6 --> Collect Results\n",
      "Step 4/6 --> Prepare Evaluation\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: A, S: [], p: 0.2867\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: B, x: A, S: [], p: 0.2867\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: B, x: A, S: [], p: 0.2867\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.787\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.787\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.787\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: C, x: B, S: [], p: 0.0\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: C, x: B, S: [], p: 0.0\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: C, x: B, S: [], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: C, x: B, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: C, x: B, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: C, x: B, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: C, x: A, S: [], p: 0.0\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: C, x: A, S: [], p: 0.0\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: C, x: A, S: [], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: A, x: B, S: [], p: 0.2867\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: A, x: B, S: [], p: 0.2867\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: A, x: B, S: [], p: 0.2867\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: A, x: B, S: ['C'], p: 0.359\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: A, x: B, S: ['C'], p: 0.359\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: A, x: B, S: ['C'], p: 0.359\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: A, x: C, S: [], p: 0.0\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: A, x: C, S: [], p: 0.0\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: A, x: C, S: [], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 15\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     12\u001b[0m server_ci_tests \u001b[38;5;241m=\u001b[39m get_server_test_results(servers) \n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep 4/6 --> Prepare Evaluation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m p_val_comparisons, missed_tests \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_server_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth_tests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_ci_tests\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep 5/6 --> Run Evaluation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m alpha_tests, equality_tests \u001b[38;5;241m=\u001b[39m evaluate_results(p_val_comparisons, alpha_comparisons, equality_tolerance)\n",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m, in \u001b[0;36mprepare_server_evaluation\u001b[0;34m(ground_truth_tests, server_ci_tests)\u001b[0m\n\u001b[1;32m     13\u001b[0m         matching_test \u001b[38;5;241m=\u001b[39m matching_test[\u001b[38;5;241m0\u001b[39m]          \n\u001b[1;32m     14\u001b[0m         p_value_comparison[k]\u001b[38;5;241m.\u001b[39mappend((matching_test\u001b[38;5;241m.\u001b[39mp_val, test\u001b[38;5;241m.\u001b[39mp_val))\n\u001b[0;32m---> 16\u001b[0m missing_test \u001b[38;5;241m=\u001b[39m {k:v\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(server_ci_tests[k]) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m missing_test\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p_value_comparison, missing_test\n",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m         matching_test \u001b[38;5;241m=\u001b[39m matching_test[\u001b[38;5;241m0\u001b[39m]          \n\u001b[1;32m     14\u001b[0m         p_value_comparison[k]\u001b[38;5;241m.\u001b[39mappend((matching_test\u001b[38;5;241m.\u001b[39mp_val, test\u001b[38;5;241m.\u001b[39mp_val))\n\u001b[0;32m---> 16\u001b[0m missing_test \u001b[38;5;241m=\u001b[39m {k:\u001b[43mv\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mserver_ci_tests\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m missing_test\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p_value_comparison, missing_test\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    process(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n"
     ]
    }
   ],
   "source": [
    "data = pl.read_parquet('./fedci/testdata.parquet')#.to_dummies(cs.string()).with_columns((~cs.float()).cast(pl.Int16))\n",
    "servers = get_servers([1], data)\n",
    "\n",
    "for server in servers.values(): server.run_tests()\n",
    "\n",
    "possible_tests = get_possible_tests(set(data.columns))\n",
    "ground_truth_tests = get_ground_truth_tests(data, possible_tests)\n",
    "server_ci_tests = get_server_test_results(servers) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TestingRound - y: B__cat__1, X: [], total samples: 10000, beta: [0.5], current iteration: 3, current deviance: 2500.0, relative deviance change: 0.0, llf: -7257.913526447273, rss: 2500.0,\n",
       " TestingRound - y: B__cat__2, X: [], total samples: 10000, beta: [0.5], current iteration: 3, current deviance: 2500.0, relative deviance change: 0.0, llf: -7257.913526447273, rss: 2500.0,\n",
       " TestingRound - y: C, X: [], total samples: 10000, beta: [-0.1984008], current iteration: 3, current deviance: 10572.304410345243, relative deviance change: 0.0, llf: -14467.64881879784, rss: 10572.304410345221,\n",
       " TestingRound - y: A, X: [], total samples: 10000, beta: [-0.01384051], current iteration: 3, current deviance: 10204.25968181569, relative deviance change: 0.0, llf: -14290.486111962866, rss: 10204.25968181565,\n",
       " TestingRound - y: B__cat__1, X: ['C'], total samples: 10000, beta: [0.5156922  0.07909343], current iteration: 3, current deviance: 2433.862080154411, relative deviance change: 0.0, llf: -7123.856512554629, rss: 2433.8620801544134,\n",
       " TestingRound - y: B__cat__1, X: ['A'], total samples: 10000, beta: [0.50007299 0.00527397], current iteration: 3, current deviance: 2499.716171424183, relative deviance change: 0.0, llf: -7257.345837069736, rss: 2499.7161714241875,\n",
       " TestingRound - y: B__cat__2, X: ['C'], total samples: 10000, beta: [ 0.4843078  -0.07909343], current iteration: 3, current deviance: 2433.862080154411, relative deviance change: 0.0, llf: -7123.85651255463, rss: 2433.862080154408,\n",
       " TestingRound - y: B__cat__2, X: ['A'], total samples: 10000, beta: [ 0.49992701 -0.00527397], current iteration: 3, current deviance: 2499.716171424183, relative deviance change: 0.0, llf: -7257.345837069736, rss: 2499.716171424186,\n",
       " TestingRound - y: C, X: ['B__cat__2'], total samples: 10000, beta: [-0.03116082 -0.33447995], current iteration: 3, current deviance: 10292.612321675413, relative deviance change: 0.0, llf: -14333.591804905194, rss: 10292.61232167538,\n",
       " TestingRound - y: C, X: ['A'], total samples: 10000, beta: [-0.20026933 -0.13500471], current iteration: 3, current deviance: 10386.318792189775, relative deviance change: 0.0, llf: -14378.90706390469, rss: 10386.31879218975,\n",
       " TestingRound - y: A, X: ['B__cat__2'], total samples: 10000, beta: [-0.00307713 -0.02152677], current iteration: 3, current deviance: 10203.101177618588, relative deviance change: 0.0, llf: -14289.918422585331, rss: 10203.10117761854,\n",
       " TestingRound - y: A, X: ['C'], total samples: 10000, beta: [-0.03969311 -0.13030491], current iteration: 3, current deviance: 10024.74862433192, relative deviance change: 0.0, llf: -14201.744357069718, rss: 10024.74862433191,\n",
       " TestingRound - y: B__cat__1, X: ['A', 'C'], total samples: 10000, beta: [0.51633672 0.0162376  0.08120927], current iteration: 3, current deviance: 2431.2189582416, relative deviance change: 0.0, llf: -7118.423669500224, rss: 2431.2189582416054,\n",
       " TestingRound - y: B__cat__2, X: ['A', 'C'], total samples: 10000, beta: [ 0.48366328 -0.0162376  -0.08120927], current iteration: 3, current deviance: 2431.2189582416, relative deviance change: 0.0, llf: -7118.423669500223, rss: 2431.2189582416063,\n",
       " TestingRound - y: C, X: ['A', 'B__cat__2'], total samples: 10000, beta: [-0.03158173 -0.13678428 -0.33742447], current iteration: 3, current deviance: 10101.712923482064, relative deviance change: 1.8006563943760467e-16, llf: -14239.984896335176, rss: 10101.712923482113,\n",
       " TestingRound - y: A, X: ['B__cat__2', 'C'], total samples: 10000, beta: [-0.00730237 -0.06688048 -0.13559471], current iteration: 3, current deviance: 10013.861962768206, relative deviance change: 0.0, llf: -14196.311514015311, rss: 10013.861962768222]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "servers['dag_chain4_1c'].testing_engine.finished_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dag_chain4_1c': [LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: [], p: 0.1318,\n",
       "  LikelihoodRatioTest - y: C, x: B__cat__2, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: B__cat__2, S: [], p: 0.2866,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: B__cat__2, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['B__cat__2'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['B__cat__2'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: B__cat__2, S: ['C'], p: 0.001]}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_ci_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0,\n",
       " LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0,\n",
       " LikelihoodRatioTest - y: B, x: A, S: [], p: 0.2867,\n",
       " LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.781,\n",
       " LikelihoodRatioTest - y: C, x: B, S: [], p: 0.0,\n",
       " LikelihoodRatioTest - y: C, x: B, S: ['A'], p: 0.0,\n",
       " LikelihoodRatioTest - y: C, x: A, S: [], p: 0.0,\n",
       " LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0,\n",
       " LikelihoodRatioTest - y: A, x: B, S: [], p: 0.2867,\n",
       " LikelihoodRatioTest - y: A, x: B, S: ['C'], p: 0.335,\n",
       " LikelihoodRatioTest - y: A, x: C, S: [], p: 0.0,\n",
       " LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: A, S: [], p: 0.2867\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.791\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: C, x: B, S: [], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: C, x: B, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: A, x: B, S: [], p: 0.2867\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: A, x: B, S: ['C'], p: 0.366\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p_val_comparisons, missed_tests = prepare_server_evaluation(ground_truth_tests, server_ci_tests)\n",
    "\n",
    "alpha_tests, equality_tests = evaluate_results(p_val_comparisons, alpha_comparisons, equality_tolerance)\n",
    "\n",
    "records = get_records(servers, alpha_tests, equality_tests, missed_tests, TOTAL_FEATURES, FEATURES_PER_CLIENT)\n",
    "\n",
    "#write_records(99, log_filepattern, records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chosen_dag': 'chain4',\n",
       "  'num_clients': 1,\n",
       "  'num_samples': 1000,\n",
       "  'same_p_val': 1.0,\n",
       "  'missed_tests': 0.6470588235294118,\n",
       "  'total_features': 4,\n",
       "  'features_per_client': 4,\n",
       "  'correctness_alpha_0.01': 1.0,\n",
       "  'correctness_alpha_0.05': 1.0,\n",
       "  'correctness_alpha_0.1': 1.0}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dag_chain4_1c': [LikelihoodRatioTest - y: C, x: B, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: [], p: 0.2132,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: [], p: 0.2132,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.0072,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: ['C'], p: 0.0072,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0],\n",
       " 'dag_chain4_3c': [LikelihoodRatioTest - y: C, x: B, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: [], p: 0.6634,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: [], p: 0.6634,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.4404,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: ['C'], p: 0.4404,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0],\n",
       " 'dag_chain4_5c': [LikelihoodRatioTest - y: C, x: B, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: [], p: 0.9476,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: [], p: 0.9476,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.2983,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: ['C'], p: 0.2983,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_ci_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
