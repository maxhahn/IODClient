{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fedci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain, combinations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from pgmpy.estimators import CITests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, x_label, s_labels, p_val):\n",
    "        self.y_label = y_label\n",
    "        self.x_label = x_label\n",
    "        self.s_labels = s_labels\n",
    "        self.p_val = p_val\n",
    "        \n",
    "class CategoricalLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, t0s, t1s, num_cats):\n",
    "        assert len(t0s) > 0\n",
    "        assert len(t1s) > 0\n",
    "        assert len(t0s[0].X_labels) + 1 == len(t1s[0].X_labels)\n",
    "        # TODO: assert more data integrity\n",
    "        #assert t0s[0].y_label == t1s[0].y_label\n",
    "        \n",
    "        self.t0 = t0s\n",
    "        self.t1 = t1s\n",
    "        \n",
    "        self.y_label = y_label\n",
    "        self.x_label = (set(t1s[0].X_labels) - set(t0s[0].X_labels)).pop()\n",
    "        self.s_labels = t0s[0].X_labels\n",
    "        self.t0_params = len(t0s[0].beta)\n",
    "        self.t1_params = len(t1s[0].beta)\n",
    "        self.p_val = self._run_likelihood_test(t0s, t1s, num_cats)\n",
    "        self.p_val = round(self.p_val, 4)\n",
    "        \n",
    "    def _run_likelihood_test(self, t0s, t1s, num_cats):\n",
    "        \n",
    "        # t1 should always encompass more regressors -> less client can fulfill this\n",
    "        #assert len(self.t1.providing_clients) < len(self.t0.providing_clients)\n",
    "        \n",
    "        providing_clients = t1s[0].providing_clients\n",
    "\n",
    "        t0_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t0s])\n",
    "        t1_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t1s])\n",
    "        \n",
    "        # d_y = num cats\n",
    "        # DOF Z = size cond set\n",
    "        # DOF X = 1\n",
    "        t0_dof = (num_cats-1)*self.t0_params # (d_y - 1)*(DOF(Z)+1)\n",
    "        t1_dof = (num_cats-1)*self.t1_params # (d_y - 1)*(DOF(Z)+DOF(X)+1)\n",
    "        t = -2*(t0_llf - t1_llf)\n",
    "        \n",
    "        p_val = stats.chi2.sf(t, t1_dof-t0_dof)\n",
    "        \n",
    "        # print(f'Regressing {self.y_label} ~ {self.x_label} + {self.s_labels}')\n",
    "        # print(f'T0 params: {self.t0_params}, T1 params: {self.t1_params}, Num Categories: {num_cats}')\n",
    "        # print(f'T0 llf: {t0_llf}, T1 llf: {t1_llf}')\n",
    "        # print(f'DOF M0: {t0_dof}, DOF M1: {t1_dof} -> Test DOF = {t1_dof-t0_dof}')\n",
    "        # print(f'Test Statistic: {t}, p val: {p_val}')\n",
    "        \n",
    "        return p_val\n",
    "    \n",
    "class OrdinalLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, t0s, t1s, num_cats):\n",
    "        assert len(t0s) > 0\n",
    "        assert len(t1s) > 0\n",
    "        #assert len(t0s) == len(t1s)\n",
    "        assert len(t0s[0].X_labels) + 1 == len(t1s[0].X_labels)\n",
    "        # TODO: assert more data integrity\n",
    "        #assert t0s[0].y_label == t1s[0].y_label\n",
    "        \n",
    "        t0s = sorted(t0s, key=lambda x: int(x.y_label.split('__ord__')[-1]))\n",
    "        t1s = sorted(t1s, key=lambda x: int(x.y_label.split('__ord__')[-1]))\n",
    "        \n",
    "        self.y_label = y_label\n",
    "        self.x_label = (set(t1s[0].X_labels) - set(t0s[0].X_labels)).pop()\n",
    "        self.s_labels = t0s[0].X_labels\n",
    "        self.t0_params = len(t0s[0].beta)\n",
    "        self.t1_params = len(t1s[0].beta)\n",
    "        self.p_val = self._run_likelihood_test(t0s, t1s, num_cats)\n",
    "        self.p_val = round(self.p_val, 4)\n",
    "        \n",
    "    def _run_likelihood_test(self, t0s, t1s, num_cats):\n",
    "        \n",
    "        # t1 should always encompass more regressors -> less client can fulfill this\n",
    "        #assert len(self.t1.providing_clients) < len(self.t0.providing_clients)\n",
    "        \n",
    "        providing_clients = t1s[0].providing_clients\n",
    "        \n",
    "        t0_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t0s])\n",
    "        t1_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t1s])\n",
    "        \n",
    "        # d_y = num cats\n",
    "        # DOF Z = size cond set\n",
    "        # DOF X = 1\n",
    "        t0_dof = (num_cats-1)*self.t0_params # (d_y - 1)*(DOF(Z)+1)\n",
    "        t1_dof = (num_cats-1)*self.t1_params # (d_y - 1)*(DOF(Z)+DOF(X)+1)\n",
    "        t = -2*(t0_llf - t1_llf)\n",
    "        \n",
    "        p_val = stats.chi2.sf(t, t1_dof-t0_dof)\n",
    "        \n",
    "        return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_id_pattern = '{}_{}c'\n",
    "\n",
    "max_regressors = 0#None\n",
    "\n",
    "alpha_comparisons = [0.01, 0.05, 0.1]\n",
    "equality_tolerance = 1e-4\n",
    "\n",
    "\n",
    "log_filepattern = 'log-{}.ndjson'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_independence_tests_collider = [\n",
    "    EmptyLikelihoodRatioTest('A', 'B', [], 1),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A'], 0),\n",
    "]\n",
    "\n",
    "real_independence_tests_fork = [\n",
    "    EmptyLikelihoodRatioTest('A', 'B', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A'], 1),\n",
    "]\n",
    "\n",
    "real_independence_tests_diamond = [\n",
    "    # cond set 0\n",
    "    EmptyLikelihoodRatioTest('A', 'B', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', [], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', [], 0),\n",
    "    # cond set 1\n",
    "    # start a\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['B'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['C'], 0),\n",
    "    # start b\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A'], 1),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['C'], 0),\n",
    "    # start c\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['B'], 0),\n",
    "    # cond set 2\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['B', 'C'], 1),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['A', 'C'], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['A', 'B'], 0),\n",
    "]\n",
    "\n",
    "real_independence_tests_chain = [\n",
    "    # cond set 0\n",
    "    EmptyLikelihoodRatioTest('A', 'B', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', [], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', [], 0),\n",
    "    # cond set 1\n",
    "    # start a\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B'], 1),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['B'], 1),\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['C'], 1),\n",
    "    # start b\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['C'], 1),\n",
    "    # start c\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['B'], 0),\n",
    "    # cond set 2\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['B', 'C'], 1),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['A', 'C'], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['A', 'B'], 0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgp\n",
    "\n",
    "# fork\n",
    "node1 = dgp.GenericNode('A')\n",
    "node2 = dgp.GenericNode('B', parents=[node1])\n",
    "node3 = dgp.GenericNode('C', parents=[node1])\n",
    "nc1 = dgp.NodeCollection('generic_fork', [node1, node2, node3])\n",
    "\n",
    "# collider\n",
    "node1 = dgp.GenericNode('A')\n",
    "node2 = dgp.GenericNode('B')\n",
    "node3 = dgp.GenericNode('C', parents=[node1, node2])\n",
    "nc2 = dgp.NodeCollection('generic_collider', [node1, node2, node3])\n",
    "\n",
    "# diamond\n",
    "node1 = dgp.GenericNode('A')\n",
    "node2 = dgp.GenericNode('B', parents=[node1])\n",
    "node3 = dgp.GenericNode('C', parents=[node1])\n",
    "node4 = dgp.GenericNode('D', parents=[node2, node3])\n",
    "nc3 = dgp.NodeCollection('generic_diamond', [node1, node2, node3, node4])\n",
    "\n",
    "# chain\n",
    "node1 = dgp.GenericNode('A')\n",
    "node2 = dgp.GenericNode('B', parents=[node1])\n",
    "node3 = dgp.GenericNode('C', parents=[node2])\n",
    "node4 = dgp.GenericNode('D', parents=[node3])\n",
    "nc4 = dgp.NodeCollection('generic_chain', [node1, node2, node3, node4])\n",
    "\n",
    "\n",
    "### Categorical data test\n",
    "# fork\n",
    "node_restr = [dgp.CategoricalNode]\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1], node_restrictions=node_restr)\n",
    "nc61 = dgp.NodeCollection('categorical_fork', [node1, node2, node3])\n",
    "\n",
    "# collider\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1, node2], node_restrictions=node_restr)\n",
    "nc62 = dgp.NodeCollection('categorical_collider', [node1, node2, node3])\n",
    "\n",
    "# diamond\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1], node_restrictions=node_restr)\n",
    "node4 = dgp.GenericNode('D', parents=[node2, node3], node_restrictions=node_restr)\n",
    "nc63 = dgp.NodeCollection('categorical_diamond', [node1, node2, node3, node4])\n",
    "\n",
    "### Ordinal data test\n",
    "# fork\n",
    "node_restr = [dgp.OrdinalNode]\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1], node_restrictions=node_restr)\n",
    "nc71 = dgp.NodeCollection('ordinal_fork', [node1, node2, node3])\n",
    "\n",
    "# collider\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1, node2], node_restrictions=node_restr)\n",
    "nc72 = dgp.NodeCollection('ordinal_collider', [node1, node2, node3])\n",
    "\n",
    "# diamond\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1], node_restrictions=node_restr)\n",
    "node4 = dgp.GenericNode('D', parents=[node2, node3], node_restrictions=node_restr)\n",
    "nc73 = dgp.NodeCollection('ordinal_diamond', [node1, node2, node3, node4])\n",
    "\n",
    "\n",
    "### ONLY CONTINUOS DATA FOR COMPARISON PURPOSES (TIKHONOV REG.)\n",
    "# fork\n",
    "node_restr = [dgp.Node]\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1], node_restrictions=node_restr)\n",
    "nc81 = dgp.NodeCollection('continuos_fork', [node1, node2, node3])\n",
    "\n",
    "# collider\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1, node2], node_restrictions=node_restr)\n",
    "nc82 = dgp.NodeCollection('continuos_collider', [node1, node2, node3])\n",
    "\n",
    "# diamond\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1], node_restrictions=node_restr)\n",
    "node4 = dgp.GenericNode('D', parents=[node2, node3], node_restrictions=node_restr)\n",
    "nc83 = dgp.NodeCollection('continuos_diamond', [node1, node2, node3, node4])\n",
    "\n",
    "# chain\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node2], node_restrictions=node_restr)\n",
    "node4 = dgp.GenericNode('D', parents=[node3], node_restrictions=node_restr)\n",
    "nc84 = dgp.NodeCollection('continuos_chain', [node1, node2, node3, node4])\n",
    "\n",
    "\n",
    "# experiments\n",
    "node_restr = [dgp.CategoricalNode]\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node2], node_restrictions=node_restr)\n",
    "node4 = dgp.GenericNode('D', parents=[node3], node_restrictions=node_restr)\n",
    "nc99 = dgp.NodeCollection('experimental', [node1, node2, node3, node4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We considered five combinations of variable types and corresponding regression models:  \n",
    "  * (a) linear-binary (L-B),  \n",
    "  * (b) linear -multinomial (L-M),  \n",
    "  * (c) linear-ordinal (L-O),  \n",
    "  * (d) binary-ordinal (B-O), and  \n",
    "  * (e) multinomial-ordinal (M-O).  \n",
    "For each case, we considered the following simple BN mod- els:   \n",
    "  * (a) X Y (unconditional independence),  \n",
    "  * (b) X → Y and X ← Y (unconditional dependence),  \n",
    "  * (c) X → Z ← Y (conditional dependence of X and Y given Z), also known as collider [37], and  \n",
    "  * (d) X ← Z → Y (conditional indepen- dence of X and Y given Z).  \n",
    "In all cases, Z is continuous.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## L-B CASE\n",
    "# Unc. Indep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "nc911 = dgp.NodeCollection('L-B Unc. Indep.', [node1, node2])\n",
    "# Unc. Dep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "nc912 = dgp.NodeCollection('L-B Unc. Dep.', [node1, node2])\n",
    "# Con. Dep. Case given Z\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "node3 = dgp.GenericNode('Z', parents=[node1, node2], node_restrictions=[dgp.Node])\n",
    "nc913 = dgp.NodeCollection('L-B Con. Dep.', [node1, node2, node3])\n",
    "# Con. Indep. Case given Z\n",
    "node1 = dgp.GenericNode('Z', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('X', parents=[node1], node_restrictions=[dgp.Node])\n",
    "node3 = dgp.GenericNode('Y', parents=[node1], node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "nc914 = dgp.NodeCollection('L-B Con. Indep.', [node1, node2, node3])\n",
    "\n",
    "## L-M CASE\n",
    "# Unc. Indep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.CategoricalNode], min_categories=3)\n",
    "nc921 = dgp.NodeCollection('L-M Unc. Indep.', [node1, node2])\n",
    "# Unc. Dep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.CategoricalNode], min_categories=3)\n",
    "nc922 = dgp.NodeCollection('L-M Unc. Dep.', [node1, node2])\n",
    "# Con. Dep. Case given Z\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.CategoricalNode], min_categories=3)\n",
    "node3 = dgp.GenericNode('Z', parents=[node1, node2], node_restrictions=[dgp.Node])\n",
    "nc923 = dgp.NodeCollection('L-M Con. Dep.', [node1, node2, node3])\n",
    "# Con. Indep. Case given Z\n",
    "node1 = dgp.GenericNode('Z', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('X', parents=[node1], node_restrictions=[dgp.Node])\n",
    "node3 = dgp.GenericNode('Y', parents=[node1], node_restrictions=[dgp.CategoricalNode], min_categories=3)\n",
    "nc924 = dgp.NodeCollection('L-M Con. Indep.', [node1, node2, node3])\n",
    "\n",
    "## L-O CASE\n",
    "# Unc. Indep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "nc931 = dgp.NodeCollection('L-O Unc. Indep.', [node1, node2])\n",
    "# Unc. Dep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "nc932 = dgp.NodeCollection('L-O Unc. Dep.', [node1, node2])\n",
    "# Con. Dep. Case given Z\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "node3 = dgp.GenericNode('Z', parents=[node1, node2], node_restrictions=[dgp.Node])\n",
    "nc933 = dgp.NodeCollection('L-O Con. Dep.', [node1, node2, node3])\n",
    "# Con. Indep. Case given Z\n",
    "node1 = dgp.GenericNode('Z', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('X', parents=[node1], node_restrictions=[dgp.Node])\n",
    "node3 = dgp.GenericNode('Y', parents=[node1], node_restrictions=[dgp.OrdinalNode])\n",
    "nc934 = dgp.NodeCollection('L-O Con. Indep.', [node1, node2, node3])\n",
    "\n",
    "## B-O CASE\n",
    "# Unc. Indep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "nc941 = dgp.NodeCollection('B-O Unc. Indep.', [node1, node2])\n",
    "# Unc. Dep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "nc942 = dgp.NodeCollection('B-O Unc. Dep.', [node1, node2])\n",
    "# Con. Dep. Case given Z\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "node3 = dgp.GenericNode('Z', parents=[node1, node2], node_restrictions=[dgp.Node])\n",
    "nc943 = dgp.NodeCollection('B-O Con. Dep.', [node1, node2, node3])\n",
    "# Con. Indep. Case given Z\n",
    "node1 = dgp.GenericNode('Z', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('X', parents=[node1], node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "node3 = dgp.GenericNode('Y', parents=[node1], node_restrictions=[dgp.OrdinalNode])\n",
    "nc944 = dgp.NodeCollection('B-O Con. Indep.', [node1, node2, node3])\n",
    "\n",
    "## M-O CASE\n",
    "# Unc. Indep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.CategoricalNode])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "nc951 = dgp.NodeCollection('M-O Unc. Indep.', [node1, node2])\n",
    "# Unc. Dep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.CategoricalNode])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "nc952 = dgp.NodeCollection('M-O Unc. Dep.', [node1, node2])\n",
    "# Con. Dep. Case given Z\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.CategoricalNode])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "node3 = dgp.GenericNode('Z', parents=[node1, node2], node_restrictions=[dgp.Node])\n",
    "nc953 = dgp.NodeCollection('M-O Con. Dep.', [node1, node2, node3])\n",
    "# Con. Indep. Case given Z\n",
    "node1 = dgp.GenericNode('Z', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('X', parents=[node1], node_restrictions=[dgp.CategoricalNode])\n",
    "node3 = dgp.GenericNode('Y', parents=[node1], node_restrictions=[dgp.OrdinalNode])\n",
    "nc954 = dgp.NodeCollection('M-O Con. Indep.', [node1, node2, node3])\n",
    "\n",
    "\n",
    "## L-L CASE\n",
    "# Unc. Indep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.Node])\n",
    "nc961 = dgp.NodeCollection('L-L Unc. Indep.', [node1, node2])\n",
    "# Unc. Dep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.Node])\n",
    "nc962 = dgp.NodeCollection('L-L Unc. Dep.', [node1, node2])\n",
    "# Con. Dep. Case given Z\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.Node])\n",
    "node3 = dgp.GenericNode('Z', parents=[node1, node2], node_restrictions=[dgp.Node])\n",
    "nc963 = dgp.NodeCollection('L-L Con. Dep.', [node1, node2, node3])\n",
    "# Con. Indep. Case given Z\n",
    "node1 = dgp.GenericNode('Z', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('X', parents=[node1], node_restrictions=[dgp.Node])\n",
    "node3 = dgp.GenericNode('Y', parents=[node1], node_restrictions=[dgp.Node])\n",
    "nc964 = dgp.NodeCollection('L-L Con. Indep.', [node1, node2, node3])\n",
    "\n",
    "\n",
    "real_independence_tests_unc_ind = [\n",
    "    EmptyLikelihoodRatioTest('X', 'Y', [], 1)\n",
    "]\n",
    "\n",
    "real_independence_tests_unc_dep = [\n",
    "    EmptyLikelihoodRatioTest('X', 'Y', [], 0)\n",
    "]\n",
    "\n",
    "real_independence_tests_con_dep = [\n",
    "    EmptyLikelihoodRatioTest('X', 'Y', [], 0),\n",
    "    EmptyLikelihoodRatioTest('X', 'Z', [], 0),\n",
    "    EmptyLikelihoodRatioTest('Y', 'Z', [], 0),\n",
    "    EmptyLikelihoodRatioTest('X', 'Y', ['Z'], 1),\n",
    "    EmptyLikelihoodRatioTest('X', 'Z', ['Y'], 0),\n",
    "    EmptyLikelihoodRatioTest('Y', 'Z', ['X'], 0),\n",
    "]\n",
    "\n",
    "real_independence_tests_con_ind = [\n",
    "    EmptyLikelihoodRatioTest('X', 'Y', [], 0),\n",
    "    EmptyLikelihoodRatioTest('X', 'Z', [], 0),\n",
    "    EmptyLikelihoodRatioTest('Y', 'Z', [], 0),\n",
    "    EmptyLikelihoodRatioTest('X', 'Y', ['Z'], 1),\n",
    "    EmptyLikelihoodRatioTest('X', 'Z', ['Y'], 0),\n",
    "    EmptyLikelihoodRatioTest('Y', 'Z', ['X'], 0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ncs = {\n",
    "    1: nc1,\n",
    "    2: nc2,\n",
    "    3: nc3,\n",
    "    4: nc4,\n",
    "    61: nc61,\n",
    "    62: nc62,\n",
    "    63: nc63,\n",
    "    71: nc71,\n",
    "    72: nc72,\n",
    "    73: nc73,\n",
    "    81: nc81,\n",
    "    82: nc82,\n",
    "    83: nc83,\n",
    "    84: nc84,\n",
    "    99: nc99,\n",
    "    # paper tests\n",
    "    911: nc911,\n",
    "    912: nc912,\n",
    "    913: nc913,\n",
    "    914: nc914,\n",
    "    \n",
    "    921: nc921,\n",
    "    922: nc922,\n",
    "    923: nc923,\n",
    "    924: nc924,\n",
    "    \n",
    "    931: nc931,\n",
    "    932: nc932,\n",
    "    933: nc933,\n",
    "    934: nc934,\n",
    "    \n",
    "    941: nc941,\n",
    "    942: nc942,\n",
    "    943: nc943,\n",
    "    944: nc944,\n",
    "    \n",
    "    951: nc951,\n",
    "    952: nc952,\n",
    "    953: nc953,\n",
    "    954: nc954,\n",
    "    \n",
    "    961: nc961,\n",
    "    962: nc962,\n",
    "    963: nc963,\n",
    "    964: nc964,\n",
    "    }\n",
    "\n",
    "ncs_independences = {\n",
    "    1: real_independence_tests_fork,\n",
    "    2: real_independence_tests_collider,\n",
    "    3: real_independence_tests_diamond,\n",
    "    4: real_independence_tests_chain,\n",
    "    61: real_independence_tests_fork,\n",
    "    62: real_independence_tests_collider,\n",
    "    63: real_independence_tests_diamond,\n",
    "    71: real_independence_tests_fork,\n",
    "    72: real_independence_tests_collider,\n",
    "    73: real_independence_tests_diamond,\n",
    "    81: real_independence_tests_fork,\n",
    "    82: real_independence_tests_collider,\n",
    "    83: real_independence_tests_diamond,\n",
    "    84: real_independence_tests_chain,\n",
    "    99: real_independence_tests_chain,\n",
    "    # paper data\n",
    "    911: real_independence_tests_unc_ind,\n",
    "    912: real_independence_tests_unc_dep,\n",
    "    913: real_independence_tests_con_dep,\n",
    "    914: real_independence_tests_con_ind,\n",
    "    \n",
    "    921: real_independence_tests_unc_ind,\n",
    "    922: real_independence_tests_unc_dep,\n",
    "    923: real_independence_tests_con_dep,\n",
    "    924: real_independence_tests_con_ind,\n",
    "    \n",
    "    931: real_independence_tests_unc_ind,\n",
    "    932: real_independence_tests_unc_dep,\n",
    "    933: real_independence_tests_con_dep,\n",
    "    934: real_independence_tests_con_ind,\n",
    "    \n",
    "    941: real_independence_tests_unc_ind,\n",
    "    942: real_independence_tests_unc_dep,\n",
    "    943: real_independence_tests_con_dep,\n",
    "    944: real_independence_tests_con_ind,\n",
    "    \n",
    "    951: real_independence_tests_unc_ind,\n",
    "    952: real_independence_tests_unc_dep,\n",
    "    953: real_independence_tests_con_dep,\n",
    "    954: real_independence_tests_con_ind,\n",
    "    \n",
    "    961: real_independence_tests_unc_ind,\n",
    "    962: real_independence_tests_unc_dep,\n",
    "    963: real_independence_tests_con_dep,\n",
    "    964: real_independence_tests_con_ind,\n",
    "}\n",
    "\n",
    "def get_sample_data(node_collection, num_samples):\n",
    "    node_collection.reset()\n",
    "    return node_collection.get(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def partition_dataframe(df, n):\n",
    "    total_rows = len(df)\n",
    "    partition_size = math.ceil(total_rows / n)\n",
    "    \n",
    "    partitions = []\n",
    "    for i in range(n):\n",
    "        start_idx = i * partition_size\n",
    "        end_idx = min((i + 1) * partition_size, total_rows)\n",
    "        partition = df[start_idx:end_idx]\n",
    "        partitions.append(partition)\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_servers(client_configurations, experiment_name, data, tikhonov_lambda=0, features_per_client=None, max_regressors=None):\n",
    "    servers = {}    \n",
    "\n",
    "    for splits in client_configurations:\n",
    "        if features_per_client is None:\n",
    "            clients = {i:fedci.Client(chunk) for i,chunk in enumerate(partition_dataframe(data, splits))}\n",
    "            #clients = {i:fedci.Client(pl.from_pandas(chunk)) for i,chunk in enumerate(np.array_split(data.to_pandas(), splits))}\n",
    "        else:\n",
    "            clients = {i:fedci.Client(chunk[random.sample(list(chunk.columns),features_per_client)])\n",
    "                       for i,chunk in enumerate(partition_dataframe(data, splits))}\n",
    "            #clients = {i:fedci.Client(pl.from_pandas(chunk[random.sample(list(chunk.columns),features_per_client)]))\n",
    "            #           for i,chunk in enumerate(np.array_split(data.to_pandas(), splits))}\n",
    "\n",
    "        servers[server_id_pattern.format(experiment_name, splits)] = fedci.Server(clients, tikhonov_lambda=tikhonov_lambda, max_regressors=max_regressors+1 if max_regressors else None)\n",
    "    return servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_tests(available_data):\n",
    "\n",
    "    possible_tests = []\n",
    "    max_conditioning_set_size = min(len(available_data), max_regressors) if max_regressors is not None else len(available_data)\n",
    "\n",
    "    for y_var in available_data:\n",
    "        set_of_regressors = available_data - {y_var}\n",
    "        for x_var in set_of_regressors:\n",
    "            set_of_conditioning_variables = set_of_regressors - {x_var}\n",
    "            conditioning_sets = chain.from_iterable(combinations(set_of_conditioning_variables, r) for r in range(0,max_conditioning_set_size))\n",
    "            possible_tests.extend([(y_var, x_var, sorted(list(s_labels))) for s_labels in conditioning_sets])\n",
    "            \n",
    "    return possible_tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycit import citest, itest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mixed_independence(continuous, categorical):\n",
    "    # ANOVA\n",
    "    categories = np.unique(categorical)\n",
    "    groups = [continuous[categorical == category] for category in categories]\n",
    "    _, p_value = stats.f_oneway(*groups)\n",
    "    #print(f\"ANOVA F-statistic: {f_statistic}, p-value: {p_value}\")\n",
    "\n",
    "    # If categorical is binary, you can also use point-biserial correlation\n",
    "    #if len(categories) == 2:\n",
    "    #    point_biserial_corr, p_value = stats.pointbiserialr(categorical, continuous)\n",
    "    #    print(f\"Point-biserial correlation: {point_biserial_corr}, p-value: {p_value}\")\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_local_fci(df, labels, alpha=0.05):\n",
    "    with (ro.default_converter + pandas2ri.converter).context():\n",
    "        ro.r['source']('../scripts/aggregation.r')\n",
    "        aggregate_ci_results_f = ro.globalenv['aggregate_ci_results']\n",
    "        \n",
    "        d = [('citestResults', ro.conversion.get_conversion().py2rpy(df)), ('labels', ro.StrVector(labels))]\n",
    "        od = OrderedDict(d)\n",
    "        lv = ro.ListVector(od)\n",
    "\n",
    "        result = aggregate_ci_results_f([lv], alpha)\n",
    "\n",
    "        pag = [x[1].tolist() for x in result['G_PAG_List'].items()][0]\n",
    "        pag_labels = [list(x[1]) for x in result['G_PAG_Label_List'].items()][0]\n",
    "        \n",
    "    return pag,pag_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from collections import OrderedDict\n",
    "import rpy2.robjects as robjs\n",
    "\n",
    "def get_ground_truth_tests_mxm(data, possible_tests, do_symmetric_tests=True):\n",
    "    ground_truth_tests = []\n",
    "\n",
    "    with (ro.default_converter + pandas2ri.converter).context():\n",
    "        ro.r['source']('./ci.r')\n",
    "        calculate_independence_f = ro.globalenv['independence_test']\n",
    "        \n",
    "        # iterate tests\n",
    "        for test in possible_tests:\n",
    "            if do_symmetric_tests and test[0] > test[1]:\n",
    "                continue\n",
    "            x,y,z = test\n",
    "            z = robjs.r(\"NULL\") if len(z) == 0 else z\n",
    "            \n",
    "            pvalue = calculate_independence_f(data.to_pandas(), x, y, ro.StrVector(z))[0]\n",
    "            pvalue = round(pvalue,4)\n",
    "            ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), pvalue))\n",
    "    return ground_truth_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rpy2.robjects import pandas2ri, Formula\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as ro\n",
    "\n",
    "# Enable automatic conversion between pandas and R data frames\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Import necessary R packages\n",
    "base = importr('base')\n",
    "#r_stats = importr('stats')\n",
    "\n",
    "def transform_dataframe(df):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Initialize an empty R list to store our columns\n",
    "    r_list = {}#ro.ListVector({})\n",
    "    \n",
    "    # Iterate through columns and convert based on data type\n",
    "    for col in df_copy.columns:\n",
    "        if df_copy[col].dtype == 'float64':\n",
    "            # Float columns become numeric (already handled by pandas2ri)\n",
    "            r_list[col] = pandas2ri.py2rpy(df_copy[col])\n",
    "        elif df_copy[col].dtype == 'object':\n",
    "            # String columns become factors\n",
    "            r_list[col] = base.as_factor(pandas2ri.py2rpy(df_copy[col]))\n",
    "        elif df_copy[col].dtype == 'int64':\n",
    "            # Integer columns become ordered factors\n",
    "            unique_values = sorted(df_copy[col].unique())\n",
    "            r_list[col] = base.factor(pandas2ri.py2rpy(df_copy[col]), \n",
    "                                      levels=ro.IntVector(unique_values), \n",
    "                                      ordered=True)\n",
    "        else:\n",
    "            print(col)\n",
    "            print(df_copy[col].dtype)\n",
    "            assert False\n",
    "            \n",
    "    r_list = ro.ListVector(r_list)\n",
    "    \n",
    "    # Convert the R list to an R data frame\n",
    "    r_dataframe = base.as_data_frame(r_list)\n",
    "    \n",
    "    return r_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_riod_tests(data, possible_tests, do_symmetric_tests=True):\n",
    "    \n",
    "    data = data.with_columns(cs.integer().cast(pl.Int64))\n",
    "    \n",
    "    ground_truth_tests = []\n",
    "    \n",
    "    if do_symmetric_tests is False:\n",
    "        return ground_truth_tests\n",
    "        \n",
    "    # Call R function\n",
    "    with (ro.default_converter + pandas2ri.converter).context():\n",
    "        # load local-ci script\n",
    "        ro.r['source']('./local-ci.r')\n",
    "        # load function from R script\n",
    "        run_ci_test_f = ro.globalenv['run_ci_test']\n",
    "        \n",
    "        #print(data.to_pandas())\n",
    "\n",
    "        df_r = transform_dataframe(data.to_pandas())\n",
    "        \n",
    "        #converting it into r object for passing into r function\n",
    "        #df_r = ro.conversion.get_conversion().py2rpy(data.to_pandas())\n",
    "        \n",
    "        #Invoking the R function and getting the result\n",
    "        if os.path.exists('./tmp/citestResults_dummy.csv'):\n",
    "            os.remove('./tmp/citestResults_dummy.csv')\n",
    "        result = run_ci_test_f(df_r, 999, \"./tmp/\", 'dummy')\n",
    "        #Converting it back to a pandas dataframe.\n",
    "        df_pvals = ro.conversion.get_conversion().rpy2py(result['citestResults'])\n",
    "        labels = list(result['labels'])\n",
    "        \n",
    "    df = pl.from_pandas(df_pvals)\n",
    "    df = df.drop('ord')\n",
    "    df = df.with_columns(pl.col('S').str.split(',').cast(pl.List(pl.Int64)))\n",
    "    df = df.with_columns(pl.col('X', 'Y').cast(pl.Int64))\n",
    "    \n",
    "    for row in df.rows():\n",
    "        x = labels[row[0]-1]\n",
    "        y = labels[row[1]-1]\n",
    "        if x > y:\n",
    "            x,y = y,x\n",
    "        s = [labels[r-1] for r in row[2] if r is not None]\n",
    "        pval = round(row[3],4)\n",
    "        \n",
    "        ground_truth_tests.append(EmptyLikelihoodRatioTest(x, y, s, pval))\n",
    "        \n",
    "    return ground_truth_tests\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_tests(data, possible_tests, do_symmetric_tests=True):  \n",
    "    ground_truth_tests = []\n",
    "\n",
    "    for test in possible_tests:\n",
    "        if do_symmetric_tests and test[0] > test[1]:\n",
    "            continue\n",
    "        if len(test[2]) > 0:\n",
    "            X = data[test[0]].to_numpy()\n",
    "            Y = data[test[1]].to_numpy()\n",
    "            Z = data[test[2]].to_numpy()\n",
    "            pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            # if data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.String:\n",
    "            #     #print('A')\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     Z = data[test[2]].to_numpy()\n",
    "            #     pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            # elif data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.Float64:\n",
    "            #     #print('B')\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     Z = data[test[2]].to_numpy()\n",
    "            #     pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            # elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.String:\n",
    "            #     #print('C')\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     Z = data[test[2]].to_numpy()\n",
    "            #     pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            # elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.Float64:\n",
    "            #     #print('D')\n",
    "            #     _, pvalue = CITests.pearsonr(test[1], test[0], list(test[2]), data.cast(pl.Float64).to_pandas(), boolean=False)\n",
    "            # else:\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     Z = data[test[2]].to_numpy()\n",
    "            #     pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            #     #assert False, 'no fitting test'\n",
    "        else:\n",
    "            #print(test[0], test[1])\n",
    "            X = data[test[0]].to_numpy().astype(float)\n",
    "            Y = data[test[1]].to_numpy().astype(float)\n",
    "            #print(test[0], test[1])\n",
    "            #print(X, Y)\n",
    "            pvalue = itest(X, Y, test_args={'statistic': 'mixed_mi', 'n_jobs': 8})\n",
    "            \n",
    "            # if data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.String:\n",
    "            #     crosstab = pd.crosstab(data.to_pandas()[test[0]], data.to_pandas()[test[1]])\n",
    "            #     _, pvalue, _, _ = stats.chi2_contingency(crosstab)\n",
    "            # elif data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.Float64:\n",
    "            #     #print('E')\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy().astype(float)\n",
    "            #     pvalue = test_mixed_independence(Y, X)\n",
    "            # elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.String:\n",
    "            #     #print('F')\n",
    "            #     X = data[test[0]].to_numpy().astype(float)\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     pvalue = test_mixed_independence(X, Y)\n",
    "            # elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.Float64:\n",
    "            #     #print('G')\n",
    "            #     v0 = data[test[0]]\n",
    "            #     v1 = data[test[1]]\n",
    "            #     _, pvalue = stats.pearsonr(v0, v1)\n",
    "            # #elif data.schema[test[0]] == pl.Int32 and data.schema[test[1]] == pl.Float64:\n",
    "            # else:\n",
    "            #     X = data[test[0]].to_numpy().astype(float)\n",
    "            #     Y = data[test[1]].to_numpy().astype(float)\n",
    "            #     pvalue = itest(X, Y, test_args={'statistic': 'mixed_mi', 'n_jobs': 8})\n",
    "            #     #assert False, 'no fitting test w/o conditiong set'\n",
    "        pvalue = round(pvalue,4)\n",
    "\n",
    "        #print(test, pvalue)\n",
    "                \n",
    "        ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), pvalue))\n",
    "    return ground_truth_tests\n",
    "# TODO: with and without conditioning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_categories_in_regression_sets(tests, reversed_category_expressions):\n",
    "    #updated_tests = []\n",
    "    for test in tests:\n",
    "        test.X_labels = sorted(list(set([reversed_category_expressions[l] if l in reversed_category_expressions else l for l in test.X_labels])))\n",
    "    return tests\n",
    "\n",
    "def group_categorical_likelihood_tests(tests, category_expressions, reversed_category_expressions):\n",
    "    #category_expressions = servers['dag_chain4_1c'].category_expressions\n",
    "    #reversed_category_expressions = servers['dag_chain4_1c'].reversed_category_expressions\n",
    "    #tests = server_ci_tests['dag_chain4_1c']\n",
    "\n",
    "    updated_tests = []\n",
    "    for test in tests:\n",
    "        if test.y_label not in reversed_category_expressions:\n",
    "            updated_tests.append(test)\n",
    "            continue\n",
    "        \n",
    "        category_label = reversed_category_expressions[test.y_label]\n",
    "        \n",
    "        # Only run if the current test is the first category. This avoids duplicate tests\n",
    "        if category_expressions[category_label][0] != test.y_label:\n",
    "            continue\n",
    "        \n",
    "        categorical_test_group = []\n",
    "        for test_lookup in tests:\n",
    "            if test_lookup.y_label in category_expressions[category_label] and test_lookup.x_label == test.x_label and sorted(test_lookup.s_labels) == sorted(test.s_labels):\n",
    "                categorical_test_group.append(test_lookup)\n",
    "                \n",
    "        lrt = CategoricalLikelihoodRatioTest(category_label, [t.t0 for t in categorical_test_group], [t.t1 for t in categorical_test_group], len(category_expressions[category_label]))\n",
    "        updated_tests.append(lrt)\n",
    "        \n",
    "    return updated_tests\n",
    "\n",
    "\n",
    "def group_ordinal_likelihood_tests(tests, ordinal_expressions, reversed_ordinal_expressions):\n",
    "    #category_expressions = servers['dag_chain4_1c'].category_expressions\n",
    "    #reversed_category_expressions = servers['dag_chain4_1c'].reversed_category_expressions\n",
    "    #tests = server_ci_tests['dag_chain4_1c']\n",
    "\n",
    "    updated_tests = []\n",
    "    for test in tests:\n",
    "        if test.y_label not in reversed_ordinal_expressions:\n",
    "            updated_tests.append(test)\n",
    "            continue\n",
    "        \n",
    "        category_label = reversed_ordinal_expressions[test.y_label]\n",
    "        #print(category_label)\n",
    "        \n",
    "        # Only run if the current test is the first category. This avoids duplicate tests\n",
    "        if ordinal_expressions[category_label][0] != test.y_label:\n",
    "            continue\n",
    "        \n",
    "        categorical_test_group = []\n",
    "        for test_lookup in tests:\n",
    "            if test_lookup.y_label in ordinal_expressions[category_label] and test_lookup.x_label == test.x_label and sorted(test_lookup.s_labels) == sorted(test.s_labels):\n",
    "                categorical_test_group.append(test_lookup)\n",
    "                \n",
    "        lrt = OrdinalLikelihoodRatioTest(category_label, [t.t0 for t in categorical_test_group], [t.t1 for t in categorical_test_group], len(ordinal_expressions[category_label]))\n",
    "        updated_tests.append(lrt)\n",
    "        \n",
    "    return updated_tests\n",
    "\n",
    "\n",
    "def get_server_test_results(servers, do_symmetric_tests=True):\n",
    "    testing_rounds = {k:v.testing_engine.finished_rounds for k,v in servers.items()}\n",
    "    testing_rounds = {k:join_categories_in_regression_sets(v, servers[k].reversed_category_expressions) for k,v in testing_rounds.items()}\n",
    "    likelihood_tests = {k:fedci.get_likelihood_tests(v) for k,v in testing_rounds.items()}\n",
    "    #print(likelihood_tests)\n",
    "    # fix up categorical tests\n",
    "    #likelihood_tests = {k:group_categorical_likelihood_tests(v, servers[k].category_expressions, servers[k].reversed_category_expressions) for k,v in likelihood_tests.items()}\n",
    "    \n",
    "    likelihood_tests = {k:group_ordinal_likelihood_tests(v, servers[k].ordinal_expressions, servers[k].reversed_ordinal_expressions) for k,v in likelihood_tests.items()}\n",
    "    \n",
    "    if do_symmetric_tests:\n",
    "        likelihood_tests = {k:fedci.get_symmetric_likelihood_tests(v) for k,v in likelihood_tests.items()}\n",
    "    \n",
    "    return likelihood_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_server_evaluation(ground_truth_tests, server_ci_tests, max_regressors):\n",
    "    p_value_comparison = {k:[] for k in server_ci_tests.keys()}\n",
    "    missing_test = {k:0 for k in server_ci_tests.keys()}\n",
    "    \n",
    "    for test in ground_truth_tests:\n",
    "        for k in server_ci_tests.keys():\n",
    "            if max_regressors is not None and max_regressors < len(test.s_labels):\n",
    "                continue\n",
    "            matching_test = [t for t in server_ci_tests[k] if t.y_label == test.y_label and t.x_label == test.x_label and sorted(t.s_labels) == sorted(test.s_labels)]\n",
    "            if len(matching_test) == 0:\n",
    "                print(f'No matching test in {k} for {test}')\n",
    "                #print(test)\n",
    "                #print(server_ci_tests[k])\n",
    "                #raise Exception('lol')\n",
    "                missing_test[k] += 1\n",
    "                continue\n",
    "            assert len(matching_test) == 1\n",
    "            matching_test = matching_test[0]          \n",
    "            p_value_comparison[k].append((matching_test.p_val, test.p_val))\n",
    "        \n",
    "    missing_test = {k:v/len(ground_truth_tests) if len(ground_truth_tests) > 0 else 0 for k,v in missing_test.items()}\n",
    "    return p_value_comparison, missing_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct_alpha_thresholdings(data, alpha):\n",
    "    # first element in data is from fedci\n",
    "    tp = sum([1 for a,b in data if (a > alpha and b > alpha)]) / len(data)\n",
    "    tn = sum([1 for a,b in data if (a < alpha and b < alpha)]) / len(data)\n",
    "    # t1: false positives\n",
    "    # positive: calling independence\n",
    "    fp = sum([1 for a,b in data if (a > alpha and b < alpha)]) / len(data)\n",
    "    # t2: false negatives\n",
    "    fn = sum([1 for a,b in data if a < alpha and b > alpha]) / len(data)\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def count_correct_pval(data, tolerance=1e-4):\n",
    "    c = sum([1 for a,b in data if abs(a-b)<tolerance]) / len(data) #if len(data) > 0 else 1\n",
    "    return c\n",
    "\n",
    "def evaluate_results(p_value_comparison, alphas, tolerance):\n",
    "    result_alpha = {}\n",
    "    result_equality = {}\n",
    "    for k,v in p_value_comparison.items():\n",
    "        result_alpha[k] = {}\n",
    "        result_equality[k] = count_correct_pval(v, tolerance)\n",
    "        for alpha in alphas:\n",
    "            result_alpha[k][alpha] = count_correct_alpha_thresholdings(v,alpha)\n",
    "            \n",
    "    return result_alpha, result_equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records(servers, name, total_samples, alpha_tests, equality_tests, p_val_comparisons, missed_tests, total_features, features_per_client, comparison_category, tikhonov_lambda, do_symmetric_tests, max_regressors, llf_neg_prob_fix):\n",
    "    results = []\n",
    "    for server_id in servers.keys():\n",
    "        server = servers[server_id]\n",
    "        alpha_test = alpha_tests[server_id]\n",
    "        \n",
    "        r = {\n",
    "            'name': name,\n",
    "            'num_clients': len(server.clients),\n",
    "            'num_samples': total_samples,\n",
    "            'comparison_category': comparison_category,\n",
    "            'same_p_val': equality_tests[server_id],\n",
    "            'missed_tests': missed_tests[server_id],\n",
    "            'max_regressors': max_regressors,\n",
    "            'total_features': total_features,\n",
    "            'features_per_client': features_per_client,\n",
    "            'tikhonov_lambda': tikhonov_lambda,\n",
    "            'llf_neg_prob_fix': llf_neg_prob_fix,\n",
    "            'symmetric': do_symmetric_tests,\n",
    "            'predicted_p_vals': [p[0] for p in p_val_comparisons[server_id]],\n",
    "            'true_p_vals': [p[1] for p in p_val_comparisons[server_id]],\n",
    "        }\n",
    "\n",
    "        for alpha, alpha_result in alpha_test.items():\n",
    "            rc = r.copy()\n",
    "            rc['alpha'] = alpha\n",
    "            tp, tn, fp, fn = alpha_result\n",
    "            rc['tp'] = tp\n",
    "            rc['tn'] = tn\n",
    "            rc['fp'] = fp\n",
    "            rc['fn'] = fn\n",
    "            results.append(rc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def write_records(i, file, data, path):\n",
    "    if len(data) == 0:\n",
    "        return\n",
    "    with open(path + file.format(i), 'a') as f:\n",
    "        for d in data:\n",
    "            ds = json.dumps(d) + '\\n'\n",
    "            f.write(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def multinomial_logistic_regression(df, dependent_var, independent_vars):\n",
    "    # Add a constant (intercept) to the independent variables\n",
    "    X = sm.add_constant(df[independent_vars])\n",
    "    \n",
    "    # The dependent variable (must be categorical)\n",
    "    y = df[dependent_var]\n",
    "    \n",
    "    # Fit the multinomial logistic regression model\n",
    "    model = sm.MNLogit(y, X)\n",
    "    result = model.fit()\n",
    "    \n",
    "    # Get the log-likelihood function (LLF) and coefficients\n",
    "    llf = result.llf\n",
    "    coefficients = result.params\n",
    "    \n",
    "    return llf, coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(i, client_configurations, llf_neg_prob_fix=None, total_samples=1000, max_regressors=None, features_to_reduce=None, tikhonov_lambda=0, do_symmetric_tests=True, do_write_records=True):\n",
    "    #print('Step 1/6 --> Setup')\n",
    "    #data = pl.read_parquet(f'./fedci/testdata-{i}.parquet')\n",
    "    #TOTAL_SAMPLES = len(data)\n",
    "    data = get_sample_data(ncs[i], total_samples)\n",
    "    \n",
    "    total_features = len(ncs[i].nodes)\n",
    "    features_per_client = total_features if features_to_reduce is None else total_features - features_to_reduce\n",
    "    \n",
    "    experiment_name = ncs[i].name\n",
    "    \n",
    "    servers = get_servers(client_configurations, experiment_name, data, tikhonov_lambda, features_per_client, max_regressors)\n",
    "    #servers = get_servers([3], experiment_name, data, tikhonov_lambda, features_per_client)\n",
    "\n",
    "    #print([len(c.data) for c in servers[f'dag_{experiment_name}_{3}c'].clients.values()])\n",
    "\n",
    "    #print('Step 2/6 --> Run Tests')\n",
    "    for server in servers.values(): server.run_tests()\n",
    "\n",
    "    #print('Step 3/6 --> Collect Results')\n",
    "    possible_tests = get_possible_tests(set(data.columns))\n",
    "    server_ci_tests = get_server_test_results(servers, do_symmetric_tests=do_symmetric_tests) \n",
    "    \n",
    "    comparison_tests_collection = []\n",
    "    #ground_truth_tests = get_ground_truth_tests_mxm(data, possible_tests, do_symmetric_tests=do_symmetric_tests)\n",
    "    #comparison_tests_collection.append(('pooled_citest', ground_truth_tests))\n",
    "    \n",
    "    # get real p values from riod code\n",
    "    riod_tests = get_riod_tests(data, possible_tests, do_symmetric_tests=do_symmetric_tests)\n",
    "    comparison_tests_collection.append(('riod_pooled_citest', riod_tests))\n",
    "    \n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.scatter(data['X'], data['Y'], alpha=0.3)\n",
    "    # plt.show()\n",
    "    \n",
    "    llf0, coeff0 = multinomial_logistic_regression(data.to_pandas(), 'Y', ['Z'])\n",
    "    llf1, coeff1 = multinomial_logistic_regression(data.to_pandas(), 'Y', ['X', 'Z'])\n",
    "\n",
    "    print('MN Regression on Intercept')\n",
    "    print(llf0)\n",
    "    print(coeff0)\n",
    "    print('MN Regression on X')\n",
    "    print(llf1)\n",
    "    print(coeff1)\n",
    "    print(f'MN Tested pval: {stats.chi2.sf(-2*(llf0-llf1), 2)}, Test Statistic {-2*(llf0 - llf1)}')\n",
    "    \n",
    "    print('Misc...')\n",
    "    print(data.schema)\n",
    "    print('---')\n",
    "    for v in list(server_ci_tests.values())[0]:\n",
    "        if v.y_label == 'X' and v.x_label == 'Y' and len(v.s_labels) == 1 and v.s_labels[0] == 'Z':\n",
    "            print(v)\n",
    "    print('---')\n",
    "    for v in riod_tests:\n",
    "        if v.y_label == 'X' and v.x_label == 'Y' and len(v.s_labels) == 1 and v.s_labels[0] == 'Z':\n",
    "            print(v)\n",
    "    #print(riod_tests)\n",
    "    \n",
    "    # if i in ncs_independences:\n",
    "    #     real_independences = ncs_independences[i].copy()\n",
    "    #     if not do_symmetric_tests:\n",
    "    #         real_independences += [EmptyLikelihoodRatioTest(t.x_label, t.y_label, t.s_labels, t.p_val) for t in real_independences]\n",
    "    #     comparison_tests_collection.append(('ground_truth', real_independences))\n",
    "    #ground_truth_tests = real_indep3 # todo: maybe add addition call of prepare_server_evaluation with prefix for different types of ground truth tests\n",
    "    \n",
    "    for comparison_name, comparison_tests in comparison_tests_collection:\n",
    "\n",
    "        #print('Step 4/6 --> Prepare Evaluation')\n",
    "        p_val_comparisons, missed_tests = prepare_server_evaluation(comparison_tests, server_ci_tests, max_regressors)\n",
    "\n",
    "        #print('Step 5/6 --> Run Evaluation')\n",
    "        alpha_tests, equality_tests = evaluate_results(p_val_comparisons, alpha_comparisons, equality_tolerance)\n",
    "\n",
    "        #print('Step 6/6 --> Log Results')\n",
    "        records = get_records(servers,\n",
    "                              experiment_name,\n",
    "                              total_samples,\n",
    "                              alpha_tests,\n",
    "                              equality_tests,\n",
    "                              p_val_comparisons,\n",
    "                              missed_tests,\n",
    "                              total_features,\n",
    "                              features_per_client,\n",
    "                              comparison_name,\n",
    "                              tikhonov_lambda,\n",
    "                              do_symmetric_tests,\n",
    "                              max_regressors,\n",
    "                              llf_neg_prob_fix)\n",
    "        \n",
    "        if do_write_records:\n",
    "            write_records(i, log_filepattern, records, './experiments/base/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove non-zero correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIKHONOV_LAMBDA = 1e-7\n",
    "TIKHONOV_LAMBDA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_tests = [\n",
    "    911,912,913,914,\n",
    "    921,922,923,924,\n",
    "    #931,932,933,934,\n",
    "    #941,942,943,944,\n",
    "    #951,952,953,954,\n",
    "    961,962,963,964\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_regressors = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llf_neg_prob_fix = 'clipping'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[100, 200, 400, 750, 1500, 2000, 2500,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6d19b48ee14e23938edb8445ecfdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressing Y ~ X + ['Z']\n",
      "T0 CategoricalTestingRound - y: Y, X: ['Z'], total samples: None, beta: {'Y__cat__2': array([1.22492679, 0.28735301]), 'Y__cat__3': array([ 2.54546297, -0.2859271 ])}, current iteration: 47, current deviance: 507.89620682112854, relative deviance change: 5.839462727581616e-09, llf: {0: -253.94810341056427}, rss: None\n",
      "T1 CategoricalTestingRound - y: Y, X: ['X', 'Z'], total samples: None, beta: {'Y__cat__2': array([-0.07848562,  1.16501528,  0.29997291]), 'Y__cat__3': array([-0.1878416 ,  2.38873746, -0.24105419])}, current iteration: 40, current deviance: 505.66844455121975, relative deviance change: 9.010519144607549e-09, llf: {0: -252.83422227560987}, rss: None\n",
      "Num Categories: 3\n",
      "T0 llf: -253.94810341056427, T1 llf: -252.83422227560987\n",
      "DOF M0: 4, DOF M1: 6 -> Test DOF = 2\n",
      "Test Statistic: 2.2277622699087942, p val: 0.3282823771714845\n",
      "Testing if X= 1 is indep of Y= 2 given S={  }\n",
      "Running Gaussian linear regression for  2 \n",
      "Running multinomial regression for  1 \n",
      "p1: 2.143432e-12 and p2: 1.508797e-12 \n",
      "Testing if X= 1 is indep of Y= 3 given S={  }\n",
      "Running Gaussian linear regression for  3 \n",
      "Running multinomial regression for  1 \n",
      "p1: 1.612134e-32 and p2: 3.395007e-33 \n",
      "Testing if X= 2 is indep of Y= 3 given S={  }\n",
      "Running Gaussian linear regression for  3 \n",
      "Running Gaussian linear regression for  2 \n",
      "p1: 3.671961e-26 and p2: 3.671961e-26 \n",
      "Testing if X= 1 is indep of Y= 2 given S={ 3 }\n",
      "Running Gaussian linear regression for  2 \n",
      "Running multinomial regression for  1 \n",
      "p1: 0.2896916 and p2: 0.2626335 \n",
      "Testing if X= 1 is indep of Y= 3 given S={ 2 }\n",
      "Running Gaussian linear regression for  3 \n",
      "Running multinomial regression for  1 \n",
      "p1: 2.546736e-21 and p2: 5.909626e-22 \n",
      "Testing if X= 2 is indep of Y= 3 given S={ 1 }\n",
      "Running Gaussian linear regression for  3 \n",
      "Running Gaussian linear regression for  2 \n",
      "p1: 8.582356e-15 and p2: 8.582356e-15 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.846349\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.841892\n",
      "         Iterations 7\n",
      "MN Regression on Intercept\n",
      "-253.90470762224783\n",
      "              0         1\n",
      "const  0.262200 -0.321602\n",
      "Z      1.244026  2.539761\n",
      "MN Regression on X\n",
      "-252.56771296010567\n",
      "              0         1\n",
      "const  0.294315 -0.272170\n",
      "X     -0.140803 -0.307597\n",
      "Z      1.172156  2.361845\n",
      "MN Tested pval: 0.26263378696884254, Test Statistic 2.673989324284321\n",
      "Misc...\n",
      "Schema([('Y', String), ('X', Float64), ('Z', Float64)])\n",
      "---\n",
      "SymmetricLikelihoodRatioTest - v0: X, v1: Y, conditioning set: ['Z'], p: 0.3283\n",
      "\t-LikelihoodRatioTest - y: X, x: Y, S: ['Z'], p: 0.2849\n",
      "\t-LikelihoodRatioTest - y: Y, x: X, S: ['Z'], p: 0.3283\n",
      "---\n",
      "LikelihoodRatioTest - y: X, x: Y, S: ['Z'], p: 0.2897\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'asdasd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[541], line 18\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m id_ \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m924\u001b[39m]:\u001b[38;5;66;03m#paper_tests:#[1,2,3,4,61,62,63,64,71,72,73,74,81,82,83,84]:#paper_tests: #1,2,3,4,81,82,83,84  # 72 # ,81,82,83,84\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#for reductions in [0,1]:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#print(f'Running {id_}')\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     process(id_,\n\u001b[1;32m      9\u001b[0m             [\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     10\u001b[0m             total_samples\u001b[38;5;241m=\u001b[39msamples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m             do_write_records\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\u001b[38;5;66;03m#TODO set to true\u001b[39;00m\n\u001b[1;32m     17\u001b[0m             )\n\u001b[0;32m---> 18\u001b[0m     \u001b[43masdasd\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asdasd' is not defined"
     ]
    }
   ],
   "source": [
    "for tkl in [1e-10]:#, 1e-7]:\n",
    "    for i in tqdm(range(100), leave=False):\n",
    "        for sym in [True]:\n",
    "            for samples in [300,500,1000,3000]:#[100, 300, 600, 1000, 3000, 6000, 10000]:#, 500, 600, 700, 800, 900, 1000]:\n",
    "                for id_ in [924]:#paper_tests:#[1,2,3,4,61,62,63,64,71,72,73,74,81,82,83,84]:#paper_tests: #1,2,3,4,81,82,83,84  # 72 # ,81,82,83,84\n",
    "                    #for reductions in [0,1]:\n",
    "                    #print(f'Running {id_}')\n",
    "                    process(id_,\n",
    "                            [1],\n",
    "                            total_samples=samples,\n",
    "                            do_symmetric_tests=sym,\n",
    "                            tikhonov_lambda=tkl,\n",
    "                            features_to_reduce=None,\n",
    "                            max_regressors=max_regressors,\n",
    "                            llf_neg_prob_fix=llf_neg_prob_fix,\n",
    "                            do_write_records=False#TODO set to true\n",
    "                            )\n",
    "                    asdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 1000\n",
    "features_to_reduce = None\n",
    "client_configurations = [3]\n",
    "tikhonov_lambda = 0\n",
    "do_symmetric_tests = True\n",
    "i = 63\n",
    "\n",
    "\n",
    "\n",
    "data = get_sample_data(ncs[i], total_samples)\n",
    "    \n",
    "total_features = len(ncs[i].nodes)\n",
    "features_per_client = total_features if features_to_reduce is None else total_features - features_to_reduce\n",
    "\n",
    "experiment_name = ncs[i].name\n",
    "\n",
    "servers = get_servers(client_configurations, experiment_name, data, tikhonov_lambda, features_per_client)\n",
    "#servers = get_servers([3], experiment_name, data, tikhonov_lambda, features_per_client)\n",
    "\n",
    "#print([len(c.data) for c in servers[f'dag_{experiment_name}_{3}c'].clients.values()])\n",
    "\n",
    "#print('Step 2/6 --> Run Tests')\n",
    "for server in servers.values(): server.run_tests()\n",
    "\n",
    "#print('Step 3/6 --> Collect Results')\n",
    "possible_tests = get_possible_tests(set(data.columns))\n",
    "server_ci_tests = get_server_test_results(servers, do_symmetric_tests=do_symmetric_tests) \n",
    "\n",
    "comparison_tests_collection = []\n",
    "ground_truth_tests = get_ground_truth_tests_mxm(data, possible_tests, do_symmetric_tests=do_symmetric_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc3.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3_000, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>D</th><th>B</th><th>C</th><th>A</th></tr><tr><td>str</td><td>f64</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>-0.116479</td><td>1</td><td>-0.816289</td></tr><tr><td>&quot;1&quot;</td><td>-0.609824</td><td>1</td><td>1.344061</td></tr><tr><td>&quot;2&quot;</td><td>1.99322</td><td>2</td><td>-0.160638</td></tr><tr><td>&quot;1&quot;</td><td>0.11312</td><td>1</td><td>-0.291801</td></tr><tr><td>&quot;1&quot;</td><td>-1.337795</td><td>2</td><td>0.215637</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;1&quot;</td><td>-0.959623</td><td>2</td><td>1.372909</td></tr><tr><td>&quot;1&quot;</td><td>-0.937223</td><td>2</td><td>0.875208</td></tr><tr><td>&quot;1&quot;</td><td>-0.535653</td><td>1</td><td>0.036071</td></tr><tr><td>&quot;1&quot;</td><td>-0.186797</td><td>1</td><td>0.326439</td></tr><tr><td>&quot;2&quot;</td><td>0.280547</td><td>1</td><td>-1.700486</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3_000, 4)\n",
       "┌─────┬───────────┬─────┬───────────┐\n",
       "│ D   ┆ B         ┆ C   ┆ A         │\n",
       "│ --- ┆ ---       ┆ --- ┆ ---       │\n",
       "│ str ┆ f64       ┆ i32 ┆ f64       │\n",
       "╞═════╪═══════════╪═════╪═══════════╡\n",
       "│ 1   ┆ -0.116479 ┆ 1   ┆ -0.816289 │\n",
       "│ 1   ┆ -0.609824 ┆ 1   ┆ 1.344061  │\n",
       "│ 2   ┆ 1.99322   ┆ 2   ┆ -0.160638 │\n",
       "│ 1   ┆ 0.11312   ┆ 1   ┆ -0.291801 │\n",
       "│ 1   ┆ -1.337795 ┆ 2   ┆ 0.215637  │\n",
       "│ …   ┆ …         ┆ …   ┆ …         │\n",
       "│ 1   ┆ -0.959623 ┆ 2   ┆ 1.372909  │\n",
       "│ 1   ┆ -0.937223 ┆ 2   ┆ 0.875208  │\n",
       "│ 1   ┆ -0.535653 ┆ 1   ┆ 0.036071  │\n",
       "│ 1   ┆ -0.186797 ┆ 1   ┆ 0.326439  │\n",
       "│ 2   ┆ 0.280547  ┆ 1   ┆ -1.700486 │\n",
       "└─────┴───────────┴─────┴───────────┘"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc3.get(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc3.get(3000)[:1000].write_parquet('./wicked-data-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# for f in glob.glob('./log-9*.ndjson'):\n",
    "#     df = pl.read_ndjson(f)\n",
    "#     if 'max_regressors' in df.schema:\n",
    "#         df = df.filter(pl.col('max_regressors') != 1)\n",
    "#     df.write_ndjson(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (342447083.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[311], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    The following code is used to aggregate multiple logistic regression models, each predicting a category of a categorical variable, in order to get the overall multinomial llf.T\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The following code is used to aggregates binary logistic regression models, each predicting a category of a categorical variable, in order to get the overall multinomial llf. Please explain to me if there is anything wrong with this code.\n",
    "\n",
    "def compute_categorical_llf2(self, y_label, X_labels, betas):\n",
    "    def _get_prob(X_labels: List[str], beta):\n",
    "        _data = self.data            \n",
    "        _data = _data.with_columns(__dummy_data=pl.lit(0.0))\n",
    "\n",
    "        X = _data.to_pandas()[X_labels]\n",
    "        X = X.to_numpy().astype(float)\n",
    "        X = sm.tools.add_constant(X) \n",
    "        \n",
    "        y = _data.to_pandas()['__dummy_data']\n",
    "        y = y.to_numpy().astype(float)\n",
    "        \n",
    "        glm_model = sm.GLM(y, X, family=family.Binomial())\n",
    "        glm_results = GLMResults(glm_model, beta, normalized_cov_params=None, scale=None)\n",
    "        prob = glm_results.predict()\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    probs = {cat:np.clip(_get_prob(X_labels, beta),1e-15,1-1e-15) for cat, beta in betas.items()}\n",
    "    denominator = sum(list(probs.values()))\n",
    "    probs = {cat:probs[cat] / denominator for cat in probs.keys()}\n",
    "    \n",
    "    def get_cat_index(data, y_label, cat):\n",
    "        cat_val = cat.split('__cat__')[-1]\n",
    "        return data.with_row_index().filter(pl.col(y_label) == cat_val)['index'].to_list()\n",
    "    \n",
    "    cat_indexes = {cat: get_cat_index(self.data, y_label, cat) for cat in probs.keys()}\n",
    "    \n",
    "    llf = 0 \n",
    "    for cat in cat_indexes.keys():\n",
    "\n",
    "        llf += np.sum(np.log(np.take(probs[cat], cat_indexes[cat])))\n",
    "    \n",
    "    return llf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
