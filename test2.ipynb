{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "INFO:rpy2.situation:cffi mode is CFFI_MODE.ANY\n",
      "INFO:rpy2.situation:R home found: /opt/homebrew/Caskroom/miniforge/base/envs/promotion/lib/R\n",
      "INFO:rpy2.situation:R library path: \n",
      "INFO:rpy2.situation:LD_LIBRARY_PATH: \n",
      "INFO:rpy2.rinterface_lib.embedded:Default options to initialize R: rpy2, --quiet, --no-save\n",
      "INFO:rpy2.rinterface_lib.embedded:R is already initialized. No need to initialize.\n"
     ]
    }
   ],
   "source": [
    "import fedci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain, combinations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from pgmpy.estimators import CITests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, x_label, s_labels, p_val):\n",
    "        self.y_label = y_label\n",
    "        self.x_label = x_label\n",
    "        self.s_labels = s_labels\n",
    "        self.p_val = p_val\n",
    "        \n",
    "class CategoricalLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, t0s, t1s, num_cats):\n",
    "        assert len(t0s) > 0\n",
    "        assert len(t1s) > 0\n",
    "        assert len(t0s[0].X_labels) + 1 == len(t1s[0].X_labels)\n",
    "        # TODO: assert more data integrity\n",
    "        #assert t0s[0].y_label == t1s[0].y_label\n",
    "        \n",
    "        self.y_label = y_label\n",
    "        self.x_label = (set(t1s[0].X_labels) - set(t0s[0].X_labels)).pop()\n",
    "        self.s_labels = t0s[0].X_labels\n",
    "        self.p_val = self._run_likelihood_test(t0s, t1s, num_cats)\n",
    "        self.p_val = round(self.p_val, 4)\n",
    "        \n",
    "    def _run_likelihood_test(self, t0s, t1s, num_cats):\n",
    "        \n",
    "        # t1 should always encompass more regressors -> less client can fulfill this\n",
    "        #assert len(self.t1.providing_clients) < len(self.t0.providing_clients)\n",
    "        \n",
    "        providing_clients = t1s[0].providing_clients\n",
    "        \n",
    "        t0_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t0s])\n",
    "        t1_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t1s])\n",
    "        \n",
    "        # d_y = num cats\n",
    "        # DOF Z = size cond set\n",
    "        # DOF X = 1\n",
    "        t0_dof = (num_cats-1)*(len(self.s_labels)+1) # (d_y - 1)*(DOF(Z)+1)\n",
    "        t1_dof = (num_cats-1)*(len(self.s_labels)+2) # (d_y - 1)*(DOF(Z)+DOF(X)+1)\n",
    "        t = -2*(t0_llf - t1_llf)\n",
    "        \n",
    "        p_val = stats.chi2.sf(t, t1_dof-t0_dof)\n",
    "        \n",
    "        return p_val\n",
    "    \n",
    "class OrdinalLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, t0s, t1s, num_cats):\n",
    "        assert len(t0s) > 0\n",
    "        assert len(t1s) > 0\n",
    "        assert len(t0s[0].X_labels) + 1 == len(t1s[0].X_labels)\n",
    "        # TODO: assert more data integrity\n",
    "        #assert t0s[0].y_label == t1s[0].y_label\n",
    "        \n",
    "        t0s = sorted(t0s, key=lambda x: int(x.y_label.split('__ord__')[-1]))\n",
    "        t1s = sorted(t1s, key=lambda x: int(x.y_label.split('__ord__')[-1]))\n",
    "        \n",
    "        self.y_label = y_label\n",
    "        self.x_label = (set(t1s[0].X_labels) - set(t0s[0].X_labels)).pop()\n",
    "        self.s_labels = t0s[0].X_labels\n",
    "        self.p_val = self._run_likelihood_test(t0s, t1s, num_cats)\n",
    "        self.p_val = round(self.p_val, 4)\n",
    "        \n",
    "    def _run_likelihood_test(self, t0s, t1s, num_cats):\n",
    "        \n",
    "        # t1 should always encompass more regressors -> less client can fulfill this\n",
    "        #assert len(self.t1.providing_clients) < len(self.t0.providing_clients)\n",
    "        \n",
    "        providing_clients = t1s[0].providing_clients\n",
    "        \n",
    "        t0_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t0s])\n",
    "        t1_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t1s])\n",
    "        \n",
    "        # d_y = num cats\n",
    "        # DOF Z = size cond set\n",
    "        # DOF X = 1\n",
    "        t0_dof = (num_cats-1)*(len(self.s_labels)+1) # (d_y - 1)*(DOF(Z)+1)\n",
    "        t1_dof = (num_cats-1)*(len(self.s_labels)+2) # (d_y - 1)*(DOF(Z)+DOF(X)+1)\n",
    "        t = -2*(t0_llf - t1_llf)\n",
    "        \n",
    "        p_val = stats.chi2.sf(t, t1_dof-t0_dof)\n",
    "        \n",
    "        return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SAMPLES = 1000\n",
    "\n",
    "#TOTAL_FEATURES = 4\n",
    "#FEATURES_PER_CLIENT = 4\n",
    "\n",
    "possible_dags = [\n",
    "    \"pdsep_g\",\n",
    "    \"collider\",\n",
    "    \"fork\",\n",
    "    \"chain4\",\n",
    "    \"descColl\",\n",
    "    \"2descColl\",\n",
    "    \"iv\"\n",
    "]\n",
    "\n",
    "# TODO: possible_dags to dict or at least store num of vars for each one\n",
    "chosen_dag = possible_dags[3]\n",
    "\n",
    "\n",
    "server_id_pattern = 'dag_{}_{}c'\n",
    "\n",
    "client_configurations = [1,3,5,10]\n",
    "\n",
    "max_regressors = None\n",
    "\n",
    "\n",
    "alpha_comparisons = [0.01, 0.05, 0.1]\n",
    "equality_tolerance = 1e-4\n",
    "\n",
    "\n",
    "log_filepattern = './log-{}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_independence_tests_collider = [\n",
    "    EmptyLikelihoodRatioTest('A', 'B', [], 1),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A'], 0),\n",
    "]\n",
    "\n",
    "real_independence_tests_fork = [\n",
    "    EmptyLikelihoodRatioTest('A', 'B', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A'], 1),\n",
    "]\n",
    "\n",
    "real_independence_tests_diamond = [\n",
    "    # cond set 0\n",
    "    EmptyLikelihoodRatioTest('A', 'B', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', [], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', [], 0),\n",
    "    # cond set 1\n",
    "    # start a\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['B'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['C'], 0),\n",
    "    # start b\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A'], 1),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['C'], 0),\n",
    "    # start c\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['B'], 0),\n",
    "    # cond set 2\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['B', 'C'], 1),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['A', 'C'], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['A', 'B'], 0),\n",
    "]\n",
    "\n",
    "real_independence_tests_chain = [\n",
    "    # cond set 0\n",
    "    EmptyLikelihoodRatioTest('A', 'B', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', [], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', [], 0),\n",
    "    # cond set 1\n",
    "    # start a\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B'], 1),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['B'], 1),\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['C'], 1),\n",
    "    # start b\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['C'], 1),\n",
    "    # start c\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['B'], 0),\n",
    "    # cond set 2\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['B', 'C'], 1),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['A', 'C'], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['A', 'B'], 0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgp\n",
    "\n",
    "# fork\n",
    "node1 = dgp.GenericNode('A')\n",
    "node2 = dgp.GenericNode('B', parents=[node1])\n",
    "node3 = dgp.GenericNode('C', parents=[node1])\n",
    "nc1 = dgp.NodeCollection([node1, node2, node3])\n",
    "\n",
    "# collider\n",
    "node1 = dgp.GenericNode('A')\n",
    "node2 = dgp.GenericNode('B')\n",
    "node3 = dgp.GenericNode('C', parents=[node1, node2])\n",
    "nc2 = dgp.NodeCollection([node1, node2, node3])\n",
    "\n",
    "# diamond\n",
    "node1 = dgp.GenericNode('A')\n",
    "node2 = dgp.GenericNode('B', parents=[node1])\n",
    "node3 = dgp.GenericNode('C', parents=[node1])\n",
    "node4 = dgp.GenericNode('D', parents=[node2, node3])\n",
    "nc3 = dgp.NodeCollection([node1, node2, node3, node4])\n",
    "\n",
    "# chain\n",
    "node1 = dgp.GenericNode('A')\n",
    "node2 = dgp.GenericNode('B', parents=[node1])\n",
    "node3 = dgp.GenericNode('C', parents=[node2])\n",
    "node4 = dgp.GenericNode('D', parents=[node3])\n",
    "nc4 = dgp.NodeCollection([node1, node2, node3, node4])\n",
    "\n",
    "\n",
    "node1 = dgp.Node('A')\n",
    "node2 = dgp.Node('B', parents=[node1])\n",
    "node3 = dgp.Node('C', parents=[node1])\n",
    "nc51 = dgp.NodeCollection([node1, node2, node3])\n",
    "\n",
    "node1 = dgp.CategoricalNode('A')\n",
    "node2 = dgp.CategoricalNode('B', parents=[node1])\n",
    "node3 = dgp.CategoricalNode('C', parents=[node1])\n",
    "nc52 = dgp.NodeCollection([node1, node2, node3])\n",
    "\n",
    "node1 = dgp.OrdinalNode('A')\n",
    "node2 = dgp.OrdinalNode('B', parents=[node1])\n",
    "node3 = dgp.OrdinalNode('C', parents=[node1])\n",
    "nc53 = dgp.NodeCollection([node1, node2, node3])\n",
    "\n",
    "\n",
    "ncs = {\n",
    "    1: nc1,\n",
    "    2: nc2,\n",
    "    3: nc3,\n",
    "    4: nc4,\n",
    "    51: nc51,\n",
    "    52: nc52,\n",
    "    53: nc53\n",
    "    }\n",
    "\n",
    "ncs_independences = {\n",
    "    1: real_independence_tests_fork,\n",
    "    2: real_independence_tests_collider,\n",
    "    3: real_independence_tests_diamond,\n",
    "    4: real_independence_tests_chain,\n",
    "    51: real_independence_tests_fork,\n",
    "    52: real_independence_tests_fork,\n",
    "    53: real_independence_tests_fork\n",
    "}\n",
    "\n",
    "def get_sample_data(node_collection, num_samples):\n",
    "    node_collection.reset()\n",
    "    return node_collection.get(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_servers(client_configurations, data):\n",
    "    servers = {}    \n",
    "\n",
    "    for splits in client_configurations:\n",
    "        clients = {i:fedci.Client(pl.from_pandas(chunk)) for i,chunk in enumerate(np.array_split(data.to_pandas(), splits))}\n",
    "        servers[server_id_pattern.format(chosen_dag, splits)] = fedci.Server(clients, max_regressors=max_regressors)\n",
    "    return servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_tests(available_data):\n",
    "\n",
    "    possible_tests = []\n",
    "    max_conditioning_set_size = min(len(available_data), max_regressors) if max_regressors is not None else len(available_data)\n",
    "\n",
    "    for y_var in available_data:\n",
    "        set_of_regressors = available_data - {y_var}\n",
    "        for x_var in set_of_regressors:\n",
    "            set_of_conditioning_variables = set_of_regressors - {x_var}\n",
    "            conditioning_sets = chain.from_iterable(combinations(set_of_conditioning_variables, r) for r in range(0,max_conditioning_set_size))\n",
    "            possible_tests.extend([(y_var, x_var, sorted(list(s_labels))) for s_labels in conditioning_sets])\n",
    "            \n",
    "    return possible_tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycit import citest, itest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mixed_independence(continuous, categorical):\n",
    "    # ANOVA\n",
    "    categories = np.unique(categorical)\n",
    "    groups = [continuous[categorical == category] for category in categories]\n",
    "    _, p_value = stats.f_oneway(*groups)\n",
    "    #print(f\"ANOVA F-statistic: {f_statistic}, p-value: {p_value}\")\n",
    "\n",
    "    # If categorical is binary, you can also use point-biserial correlation\n",
    "    #if len(categories) == 2:\n",
    "    #    point_biserial_corr, p_value = stats.pointbiserialr(categorical, continuous)\n",
    "    #    print(f\"Point-biserial correlation: {point_biserial_corr}, p-value: {p_value}\")\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_tests(data, possible_tests):  \n",
    "    ground_truth_tests = []\n",
    "\n",
    "    for test in possible_tests:\n",
    "        if len(test[2]) > 0:\n",
    "            X = data[test[0]].to_numpy()\n",
    "            Y = data[test[1]].to_numpy()\n",
    "            Z = data[test[2]].to_numpy()\n",
    "            pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            # if data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.String:\n",
    "            #     #print('A')\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     Z = data[test[2]].to_numpy()\n",
    "            #     pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            # elif data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.Float64:\n",
    "            #     #print('B')\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     Z = data[test[2]].to_numpy()\n",
    "            #     pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            # elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.String:\n",
    "            #     #print('C')\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     Z = data[test[2]].to_numpy()\n",
    "            #     pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            # elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.Float64:\n",
    "            #     #print('D')\n",
    "            #     _, pvalue = CITests.pearsonr(test[1], test[0], list(test[2]), data.cast(pl.Float64).to_pandas(), boolean=False)\n",
    "            # else:\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     Z = data[test[2]].to_numpy()\n",
    "            #     pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            #     #assert False, 'no fitting test'\n",
    "        else:\n",
    "            X = data[test[0]].to_numpy().astype(float)\n",
    "            Y = data[test[1]].to_numpy().astype(float)\n",
    "            pvalue = itest(X, Y, test_args={'statistic': 'mixed_mi', 'n_jobs': 8})\n",
    "            \n",
    "            # if data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.String:\n",
    "            #     crosstab = pd.crosstab(data.to_pandas()[test[0]], data.to_pandas()[test[1]])\n",
    "            #     _, pvalue, _, _ = stats.chi2_contingency(crosstab)\n",
    "            # elif data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.Float64:\n",
    "            #     #print('E')\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy().astype(float)\n",
    "            #     pvalue = test_mixed_independence(Y, X)\n",
    "            # elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.String:\n",
    "            #     #print('F')\n",
    "            #     X = data[test[0]].to_numpy().astype(float)\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     pvalue = test_mixed_independence(X, Y)\n",
    "            # elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.Float64:\n",
    "            #     #print('G')\n",
    "            #     v0 = data[test[0]]\n",
    "            #     v1 = data[test[1]]\n",
    "            #     _, pvalue = stats.pearsonr(v0, v1)\n",
    "            # #elif data.schema[test[0]] == pl.Int32 and data.schema[test[1]] == pl.Float64:\n",
    "            # else:\n",
    "            #     X = data[test[0]].to_numpy().astype(float)\n",
    "            #     Y = data[test[1]].to_numpy().astype(float)\n",
    "            #     pvalue = itest(X, Y, test_args={'statistic': 'mixed_mi', 'n_jobs': 8})\n",
    "            #     #assert False, 'no fitting test w/o conditiong set'\n",
    "        pvalue = round(pvalue,4)\n",
    "\n",
    "        #print(test, pvalue)\n",
    "                \n",
    "        ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), pvalue))\n",
    "    return ground_truth_tests\n",
    "# TODO: with and without conditioning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_tests_old(data, possible_tests):    \n",
    "    ground_truth_tests = []\n",
    "\n",
    "    for test in possible_tests:\n",
    "        print(test)\n",
    "\n",
    "        if len(test[2]) > 0:\n",
    "            X = data[test[0]].to_numpy()\n",
    "            Y = data[test[1]].to_numpy()\n",
    "            Z = data[test[2]].to_numpy()\n",
    "            pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 2})\n",
    "        else:\n",
    "            X = data[test[0]].to_numpy()\n",
    "            Y = data[test[1]].to_numpy().astype(float)\n",
    "            pvalue = test_mixed_independence(X, Y)\n",
    "\n",
    "        pvalue = round(pvalue,4)\n",
    "        \n",
    "        ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), pvalue))\n",
    "    return ground_truth_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_tests_old(data, possible_tests):\n",
    "    ground_truth_tests = []\n",
    "\n",
    "    for test in possible_tests:\n",
    "        if len(test[2]) > 0:\n",
    "            #v0 = data[test[0]].values\n",
    "            #v1 = data[test[1]].values\n",
    "            #s = data[list(test[2])].values\n",
    "            #p0 = test[3]\n",
    "            #p1 = citest(v0, v1, s, test_args={'statistic': 'ksg_cmi', 'n_jobs': 8})\n",
    "            \n",
    "            _, p1 = CITests.pearsonr(test[1], test[0], list(test[2]), data.cast(pl.Float64).to_pandas(), boolean=False)\n",
    "        else:\n",
    "            \n",
    "            #dummied_data = data.to_dummies(cs.string(), separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            #v0 = data[test[0]].cast(pl.Float64).to_pandas()\n",
    "            #v1 = data[test[1]].cast(pl.Float64).to_pandas()\n",
    "            \n",
    "            d0 = data[test[0]]\n",
    "            d1 = data[test[1]]\n",
    "\n",
    "            \n",
    "            #v0 = d0.to_dummies(cs.string(), separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            #v1 = d1.to_dummies(cs.string(), separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            \n",
    "            v0 = d0.to_dummies(separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            v1 = d1.to_dummies(separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            \n",
    "            \n",
    "            _, p1 = stats.pearsonr(v0, v1)\n",
    "            \n",
    "        p1 = round(p1,4)\n",
    "        \n",
    "        ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), p1))\n",
    "    return ground_truth_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_categories_in_regression_sets(tests, reversed_category_expressions):\n",
    "    #updated_tests = []\n",
    "    for test in tests:\n",
    "        test.X_labels = sorted(list(set([reversed_category_expressions[l] if l in reversed_category_expressions else l for l in test.X_labels])))\n",
    "    return tests\n",
    "\n",
    "def group_categorical_likelihood_tests(tests, category_expressions, reversed_category_expressions):\n",
    "    #category_expressions = servers['dag_chain4_1c'].category_expressions\n",
    "    #reversed_category_expressions = servers['dag_chain4_1c'].reversed_category_expressions\n",
    "    #tests = server_ci_tests['dag_chain4_1c']\n",
    "\n",
    "    updated_tests = []\n",
    "    for test in tests:\n",
    "        if test.y_label not in reversed_category_expressions:\n",
    "            updated_tests.append(test)\n",
    "            continue\n",
    "        \n",
    "        category_label = reversed_category_expressions[test.y_label]\n",
    "        \n",
    "        # Only run if the current test is the first category. This avoids duplicate tests\n",
    "        if category_expressions[category_label][0] != test.y_label:\n",
    "            continue\n",
    "        \n",
    "        categorical_test_group = []\n",
    "        for test_lookup in tests:\n",
    "            if test_lookup.y_label in category_expressions[category_label] and test_lookup.x_label == test.x_label and sorted(test_lookup.s_labels) == sorted(test.s_labels):\n",
    "                categorical_test_group.append(test_lookup)\n",
    "                \n",
    "        lrt = CategoricalLikelihoodRatioTest(category_label, [t.t0 for t in categorical_test_group], [t.t1 for t in categorical_test_group], len(category_expressions[category_label]))\n",
    "        updated_tests.append(lrt)\n",
    "        \n",
    "    return updated_tests\n",
    "\n",
    "\n",
    "def group_ordinal_likelihood_tests(tests, ordinal_expressions, reversed_ordinal_expressions):\n",
    "    #category_expressions = servers['dag_chain4_1c'].category_expressions\n",
    "    #reversed_category_expressions = servers['dag_chain4_1c'].reversed_category_expressions\n",
    "    #tests = server_ci_tests['dag_chain4_1c']\n",
    "\n",
    "    updated_tests = []\n",
    "    for test in tests:\n",
    "        if test.y_label not in reversed_ordinal_expressions:\n",
    "            updated_tests.append(test)\n",
    "            continue\n",
    "        \n",
    "        category_label = reversed_ordinal_expressions[test.y_label]\n",
    "        #print(category_label)\n",
    "        \n",
    "        # Only run if the current test is the first category. This avoids duplicate tests\n",
    "        if ordinal_expressions[category_label][0] != test.y_label:\n",
    "            continue\n",
    "        \n",
    "        categorical_test_group = []\n",
    "        for test_lookup in tests:\n",
    "            if test_lookup.y_label in ordinal_expressions[category_label] and test_lookup.x_label == test.x_label and sorted(test_lookup.s_labels) == sorted(test.s_labels):\n",
    "                categorical_test_group.append(test_lookup)\n",
    "                \n",
    "        lrt = OrdinalLikelihoodRatioTest(category_label, [t.t0 for t in categorical_test_group], [t.t1 for t in categorical_test_group], len(ordinal_expressions[category_label]))\n",
    "        updated_tests.append(lrt)\n",
    "        \n",
    "    return updated_tests\n",
    "\n",
    "\n",
    "def get_server_test_results(servers):\n",
    "    testing_rounds = {k:v.testing_engine.finished_rounds for k,v in servers.items()}\n",
    "    testing_rounds = {k:join_categories_in_regression_sets(v, servers[k].reversed_category_expressions) for k,v in testing_rounds.items()}\n",
    "    likelihood_tests = {k:fedci.get_likelihood_tests(v) for k,v in testing_rounds.items()}\n",
    "    # fix up categorical tests\n",
    "    likelihood_tests = {k:group_categorical_likelihood_tests(v, servers[k].category_expressions, servers[k].reversed_category_expressions) for k,v in likelihood_tests.items()}\n",
    "    \n",
    "    likelihood_tests = {k:group_ordinal_likelihood_tests(v, servers[k].ordinal_expressions, servers[k].reversed_ordinal_expressions) for k,v in likelihood_tests.items()}\n",
    "    \n",
    "    return likelihood_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_server_evaluation(ground_truth_tests, server_ci_tests):\n",
    "    p_value_comparison = {k:[] for k in server_ci_tests.keys()}\n",
    "    missing_test = {k:0 for k in server_ci_tests.keys()}\n",
    "    \n",
    "    for test in ground_truth_tests:\n",
    "        for k in server_ci_tests.keys():\n",
    "            matching_test = [t for t in server_ci_tests[k] if t.y_label == test.y_label and t.x_label == test.x_label and sorted(t.s_labels) == sorted(test.s_labels)]\n",
    "            if len(matching_test) == 0:\n",
    "                print(f'No matching test in {k} for {test}')\n",
    "                missing_test[k] += 1\n",
    "                continue\n",
    "            assert len(matching_test) == 1\n",
    "            matching_test = matching_test[0]          \n",
    "            p_value_comparison[k].append((matching_test.p_val, test.p_val))\n",
    "        \n",
    "    missing_test = {k:v/len(server_ci_tests[k]) if len(server_ci_tests[k]) > 0 else 0 for k,v in missing_test.items()}\n",
    "    return p_value_comparison, missing_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct_alpha_thresholdings(data, alpha):\n",
    "    c = sum([1 for a,b in data if (a < alpha and b < alpha) or (a > alpha and b > alpha)]) / len(data)\n",
    "    return c\n",
    "\n",
    "def count_correct_pval(data, tolerance=1e-4):\n",
    "    c = sum([1 for a,b in data if abs(a-b)<tolerance]) / len(data)\n",
    "    return c\n",
    "\n",
    "def evaluate_results(p_value_comparison, alphas, tolerance):\n",
    "    result_alpha = {}\n",
    "    result_equality = {}\n",
    "    for k,v in p_value_comparison.items():\n",
    "        result_alpha[k] = {}\n",
    "        result_equality[k] = count_correct_pval(v, tolerance)\n",
    "        for alpha in alphas:\n",
    "            result_alpha[k][alpha] = count_correct_alpha_thresholdings(v,alpha)\n",
    "            \n",
    "    return result_alpha, result_equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records(servers, alpha_tests, equality_tests, missed_tests, total_features, features_per_client, comparison_category):\n",
    "    results = []\n",
    "    for server_id in servers.keys():\n",
    "        server = servers[server_id]\n",
    "        alpha_test = alpha_tests[server_id]\n",
    "        \n",
    "        r = {\n",
    "            'chosen_dag': chosen_dag,\n",
    "            'num_clients': len(server.clients),\n",
    "            'num_samples': TOTAL_SAMPLES,\n",
    "            'comparison_category': comparison_category,\n",
    "            'same_p_val': equality_tests[server_id],\n",
    "            'missed_tests': missed_tests[server_id],\n",
    "            'total_features': total_features,\n",
    "            'features_per_client': features_per_client\n",
    "        }\n",
    "        for alpha, alpha_result in alpha_test.items():\n",
    "            r[f'correctness_alpha_{alpha}'] = alpha_result\n",
    "        results.append(r)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_add_row(data, file):\n",
    "    with open(file, 'a') as f:\n",
    "        row = ','.join([str(d) for d in data]) + '\\n'\n",
    "        f.write(row)\n",
    "            \n",
    "\n",
    "def write_records(i, file, data):\n",
    "    if len(data) == 0:\n",
    "        return\n",
    "    curr_file = file.format(i)\n",
    "    if not os.path.exists(curr_file):\n",
    "        csv_add_row(list(data[0].keys()), curr_file)\n",
    "    for entry in data:\n",
    "        csv_add_row(entry.values(), curr_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(i):\n",
    "    #print('Step 1/6 --> Setup')\n",
    "    #data = pl.read_parquet(f'./fedci/testdata-{i}.parquet')\n",
    "    #TOTAL_SAMPLES = len(data)\n",
    "    data = get_sample_data(ncs[i], TOTAL_SAMPLES)\n",
    "    servers = get_servers(client_configurations, data)\n",
    "\n",
    "    #print('Step 2/6 --> Run Tests')\n",
    "    for server in servers.values(): server.run_tests()\n",
    "\n",
    "    #print('Step 3/6 --> Collect Results')\n",
    "    possible_tests = get_possible_tests(set(data.columns))\n",
    "    server_ci_tests = get_server_test_results(servers) \n",
    "    \n",
    "    comparison_tests_collection = []\n",
    "    ground_truth_tests = get_ground_truth_tests(data, possible_tests)\n",
    "    comparison_tests_collection.append(('ground_truth', ground_truth_tests))\n",
    "    \n",
    "    if i in ncs_independences:\n",
    "        real_independences = ncs_independences[i]\n",
    "        comparison_tests_collection.append(('real', real_independences))\n",
    "    #ground_truth_tests = real_indep3 # todo: maybe add addition call of prepare_server_evaluation with prefix for different types of ground truth tests\n",
    "    \n",
    "    for comparison_name, comparison_tests in comparison_tests_collection:\n",
    "\n",
    "        #print('Step 4/6 --> Prepare Evaluation')\n",
    "        p_val_comparisons, missed_tests = prepare_server_evaluation(comparison_tests, server_ci_tests)\n",
    "\n",
    "        #print('Step 5/6 --> Run Evaluation')\n",
    "        alpha_tests, equality_tests = evaluate_results(p_val_comparisons, alpha_comparisons, equality_tolerance)\n",
    "\n",
    "        #print('Step 6/6 --> Log Results')\n",
    "        records = get_records(servers, alpha_tests, equality_tests, missed_tests, len(ncs[i].nodes), len(ncs[i].nodes), comparison_name)\n",
    "        \n",
    "        write_records(i, log_filepattern, records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove non-zero correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    process(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n"
     ]
    }
   ],
   "source": [
    "data = get_sample_data(ncs[i], TOTAL_SAMPLES)\n",
    "servers = get_servers(client_configurations, data)\n",
    "\n",
    "#print('Step 2/6 --> Run Tests')\n",
    "for server in servers.values(): server.run_tests()\n",
    "\n",
    "#print('Step 3/6 --> Collect Results')\n",
    "possible_tests = get_possible_tests(set(data.columns))\n",
    "server_ci_tests = get_server_test_results(servers) \n",
    "\n",
    "comparison_tests_collection = []\n",
    "ground_truth_tests = get_ground_truth_tests(data, possible_tests)\n",
    "comparison_tests_collection.append(('ground_truth', ground_truth_tests))\n",
    "\n",
    "if i in ncs_independences:\n",
    "    real_independences = ncs_independences[i]\n",
    "    comparison_tests_collection.append(('real', real_independences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dag_chain4_1c': [LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: D, S: [], p: 0.0011,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: B, S: [], p: 0.0011,\n",
       "  LikelihoodRatioTest - y: D, x: C, S: [], p: 0.0408,\n",
       "  LikelihoodRatioTest - y: D, x: A, S: [], p: 0.3085,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: D, S: [], p: 0.0408,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: D, S: [], p: 0.3085,\n",
       "  LikelihoodRatioTest - y: B, x: D, S: ['C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0281,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.0004,\n",
       "  LikelihoodRatioTest - y: B, x: D, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: B, S: ['C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: C, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: B, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: A, S: ['B'], p: 0.0001,\n",
       "  LikelihoodRatioTest - y: D, x: C, S: ['A'], p: 0.0598,\n",
       "  LikelihoodRatioTest - y: D, x: A, S: ['C'], p: 0.5293,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: D, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: ['A'], p: 0.0281,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: D, S: ['A'], p: 0.0598,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: ['C'], p: 0.0004,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: D, S: ['C'], p: 0.5293,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: D, S: ['B'], p: 0.0001,\n",
       "  LikelihoodRatioTest - y: B, x: D, S: ['A', 'C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['A', 'D'], p: 0.0003,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['C', 'D'], p: 0.0001,\n",
       "  LikelihoodRatioTest - y: D, x: B, S: ['A', 'C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: C, S: ['A', 'B'], p: 0.0005,\n",
       "  LikelihoodRatioTest - y: D, x: A, S: ['B', 'C'], p: 0.1367,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: ['A', 'D'], p: 0.0003,\n",
       "  LikelihoodRatioTest - y: C, x: D, S: ['A', 'B'], p: 0.0005,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['B', 'D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: ['C', 'D'], p: 0.0001,\n",
       "  LikelihoodRatioTest - y: A, x: D, S: ['B', 'C'], p: 0.1367,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['B', 'D'], p: 0.0],\n",
       " 'dag_chain4_3c': [LikelihoodRatioTest - y: A, x: B, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: D, S: [], p: 0.397,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: D, S: [], p: 0.004,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: D, S: [], p: 0.0419,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: B, S: [], p: 0.0018,\n",
       "  LikelihoodRatioTest - y: D, x: C, S: [], p: 0.0327,\n",
       "  LikelihoodRatioTest - y: D, x: A, S: [], p: 0.2434,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: D, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: ['C'], p: 0.0003,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: D, S: ['C'], p: 0.4197,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: D, S: ['C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: D, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0111,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.0006,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: D, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: ['A'], p: 0.0379,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: D, S: ['A'], p: 0.0498,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: B, S: ['C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: C, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: B, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: A, S: ['B'], p: 0.0001,\n",
       "  LikelihoodRatioTest - y: D, x: C, S: ['A'], p: 0.0696,\n",
       "  LikelihoodRatioTest - y: D, x: A, S: ['C'], p: 0.7616,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: ['C', 'D'], p: 0.0001,\n",
       "  LikelihoodRatioTest - y: A, x: D, S: ['B', 'C'], p: 0.1559,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['B', 'D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: D, S: ['A', 'C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['A', 'D'], p: 0.0002,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['C', 'D'], p: 0.0001,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: ['A', 'D'], p: 0.0004,\n",
       "  LikelihoodRatioTest - y: C, x: D, S: ['A', 'B'], p: 0.0006,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['B', 'D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: B, S: ['A', 'C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: C, S: ['A', 'B'], p: 0.0006,\n",
       "  LikelihoodRatioTest - y: D, x: A, S: ['B', 'C'], p: 0.1252],\n",
       " 'dag_chain4_5c': [LikelihoodRatioTest - y: A, x: B, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: D, S: [], p: 0.4737,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: D, S: [], p: 0.023,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: D, S: [], p: 0.0413,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: [], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: B, S: [], p: 0.0022,\n",
       "  LikelihoodRatioTest - y: D, x: C, S: [], p: 0.0326,\n",
       "  LikelihoodRatioTest - y: D, x: A, S: [], p: 0.2472,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: D, S: ['B'], p: 0.0001,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: ['C'], p: 0.0003,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: A, x: D, S: ['C'], p: 0.497,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: D, S: ['C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: D, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0155,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.0002,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: D, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: ['A'], p: 0.0285,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: C, x: D, S: ['A'], p: 0.0764,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: B, S: ['C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: C, S: ['B'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: B, S: ['A'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: A, S: ['B'], p: 0.0001,\n",
       "  LikelihoodRatioTest - y: D, x: C, S: ['A'], p: 0.0674,\n",
       "  LikelihoodRatioTest - y: D, x: A, S: ['C'], p: 0.7328,\n",
       "  LikelihoodRatioTest - y: A, x: B, S: ['C', 'D'], p: 0.0001,\n",
       "  LikelihoodRatioTest - y: A, x: D, S: ['B', 'C'], p: 0.1251,\n",
       "  LikelihoodRatioTest - y: A, x: C, S: ['B', 'D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: D, S: ['A', 'C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: B, x: C, S: ['A', 'D'], p: 0.0005,\n",
       "  LikelihoodRatioTest - y: B, x: A, S: ['C', 'D'], p: 0.0001,\n",
       "  LikelihoodRatioTest - y: C, x: B, S: ['A', 'D'], p: 0.0002,\n",
       "  LikelihoodRatioTest - y: C, x: D, S: ['A', 'B'], p: 0.0005,\n",
       "  LikelihoodRatioTest - y: C, x: A, S: ['B', 'D'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: B, S: ['A', 'C'], p: 0.0,\n",
       "  LikelihoodRatioTest - y: D, x: C, S: ['A', 'B'], p: 0.0004,\n",
       "  LikelihoodRatioTest - y: D, x: A, S: ['B', 'C'], p: 0.0929]}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_ci_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
