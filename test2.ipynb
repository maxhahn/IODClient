{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "INFO:rpy2.situation:cffi mode is CFFI_MODE.ANY\n",
      "INFO:rpy2.situation:R home found: /opt/homebrew/Caskroom/miniforge/base/envs/promotion/lib/R\n",
      "INFO:rpy2.situation:R library path: \n",
      "INFO:rpy2.situation:LD_LIBRARY_PATH: \n",
      "INFO:rpy2.rinterface_lib.embedded:Default options to initialize R: rpy2, --quiet, --no-save\n",
      "INFO:rpy2.rinterface_lib.embedded:R is already initialized. No need to initialize.\n"
     ]
    }
   ],
   "source": [
    "import fedci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain, combinations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from pgmpy.estimators import CITests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, x_label, s_labels, p_val):\n",
    "        self.y_label = y_label\n",
    "        self.x_label = x_label\n",
    "        self.s_labels = s_labels\n",
    "        self.p_val = p_val\n",
    "        \n",
    "class CategoricalLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, t0s, t1s, num_cats):\n",
    "        assert len(t0s) > 0\n",
    "        assert len(t1s) > 0\n",
    "        assert len(t0s[0].X_labels) + 1 == len(t1s[0].X_labels)\n",
    "        # TODO: assert more data integrity\n",
    "        #assert t0s[0].y_label == t1s[0].y_label\n",
    "        \n",
    "        self.y_label = y_label\n",
    "        self.x_label = (set(t1s[0].X_labels) - set(t0s[0].X_labels)).pop()\n",
    "        self.s_labels = t0s[0].X_labels\n",
    "        self.p_val = self._run_likelihood_test(t0s, t1s, num_cats)\n",
    "        self.p_val = round(self.p_val, 4)\n",
    "        \n",
    "    def _run_likelihood_test(self, t0s, t1s, num_cats):\n",
    "        \n",
    "        # t1 should always encompass more regressors -> less client can fulfill this\n",
    "        #assert len(self.t1.providing_clients) < len(self.t0.providing_clients)\n",
    "        \n",
    "        providing_clients = t1s[0].providing_clients\n",
    "        \n",
    "        t0_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t0s])\n",
    "        t1_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t1s])\n",
    "        \n",
    "        # d_y = num cats\n",
    "        # DOF Z = size cond set\n",
    "        # DOF X = 1\n",
    "        t0_dof = (num_cats-1)*(len(self.s_labels)+1) # (d_y - 1)*(DOF(Z)+1)\n",
    "        t1_dof = (num_cats-1)*(len(self.s_labels)+2) # (d_y - 1)*(DOF(Z)+DOF(X)+1)\n",
    "        t = -2*(t0_llf - t1_llf)\n",
    "        \n",
    "        p_val = stats.chi2.sf(t, t1_dof-t0_dof)\n",
    "        \n",
    "        return p_val\n",
    "    \n",
    "class OrdinalLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, t0s, t1s, num_cats):\n",
    "        assert len(t0s) > 0\n",
    "        assert len(t1s) > 0\n",
    "        assert len(t0s[0].X_labels) + 1 == len(t1s[0].X_labels)\n",
    "        # TODO: assert more data integrity\n",
    "        #assert t0s[0].y_label == t1s[0].y_label\n",
    "        \n",
    "        t0s = sorted(t0s, key=lambda x: int(x.split('__ord__')[-1]))\n",
    "        t1s = sorted(t1s, key=lambda x: int(x.split('__ord__')[-1]))\n",
    "        \n",
    "        self.y_label = y_label\n",
    "        self.x_label = (set(t1s[0].X_labels) - set(t0s[0].X_labels)).pop()\n",
    "        self.s_labels = t0s[0].X_labels\n",
    "        self.p_val = self._run_likelihood_test(t0s, t1s, num_cats)\n",
    "        self.p_val = round(self.p_val, 4)\n",
    "        \n",
    "    def _run_likelihood_test(self, t0s, t1s, num_cats):\n",
    "        \n",
    "        # t1 should always encompass more regressors -> less client can fulfill this\n",
    "        #assert len(self.t1.providing_clients) < len(self.t0.providing_clients)\n",
    "        \n",
    "        providing_clients = t1s[0].providing_clients\n",
    "        \n",
    "        t0_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t0s])\n",
    "        t1_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t1s])\n",
    "        \n",
    "        # d_y = num cats\n",
    "        # DOF Z = size cond set\n",
    "        # DOF X = 1\n",
    "        t0_dof = (num_cats-1)*(len(self.s_labels)+1) # (d_y - 1)*(DOF(Z)+1)\n",
    "        t1_dof = (num_cats-1)*(len(self.s_labels)+2) # (d_y - 1)*(DOF(Z)+DOF(X)+1)\n",
    "        t = -2*(t0_llf - t1_llf)\n",
    "        \n",
    "        p_val = stats.chi2.sf(t, t1_dof-t0_dof)\n",
    "        \n",
    "        return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SAMPLES = 100\n",
    "\n",
    "TOTAL_FEATURES = 4\n",
    "FEATURES_PER_CLIENT = 4\n",
    "\n",
    "possible_dags = [\n",
    "    \"pdsep_g\",\n",
    "    \"collider\",\n",
    "    \"fork\",\n",
    "    \"chain4\",\n",
    "    \"descColl\",\n",
    "    \"2descColl\",\n",
    "    \"iv\"\n",
    "]\n",
    "\n",
    "# TODO: possible_dags to dict or at least store num of vars for each one\n",
    "chosen_dag = possible_dags[3]\n",
    "\n",
    "\n",
    "server_id_pattern = 'dag_{}_{}c'\n",
    "\n",
    "client_configurations = [1,3, 5]\n",
    "\n",
    "max_regressors = None\n",
    "\n",
    "\n",
    "alpha_comparisons = [0.01, 0.05, 0.1]\n",
    "equality_tolerance = 1e-4\n",
    "\n",
    "\n",
    "log_filepattern = './log-{}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data_old(dag_type, num_samples, num_vars):\n",
    "    with (ro.default_converter + pandas2ri.converter).context():\n",
    "        ro.r['source']('./app/scripts/example_data.r')\n",
    "        get_example_data_f = ro.globalenv['get_example_data']\n",
    "\n",
    "        result = get_example_data_f(dag_type, 1, num_samples, num_vars)\n",
    "        \n",
    "    return list(result.items())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgp\n",
    "\n",
    "node1 = dgp.Node('A')\n",
    "node2 = dgp.Node('B')\n",
    "node3 = dgp.Node('C', parents=[node1, node2])\n",
    "nc1 = dgp.NodeCollection([node1, node2, node3])\n",
    "\n",
    "node1 = dgp.Node('A')\n",
    "node2 = dgp.Node('B')\n",
    "node3 = dgp.CategoricalNode('C', parents=[node1, node2])\n",
    "nc3 = dgp.NodeCollection([node1, node2, node3])\n",
    "\n",
    "node1 = dgp.Node('A')\n",
    "node2 = dgp.OrdinalNode('B')\n",
    "node3 = dgp.CategoricalNode('C', parents=[node1, node2])\n",
    "nc4 = dgp.NodeCollection([node1, node2, node3])\n",
    "\n",
    "ncs = {\n",
    "    1: nc1,\n",
    "    3: nc3,\n",
    "    4: nc4\n",
    "    }\n",
    "\n",
    "def get_sample_data(node_collection, num_samples):\n",
    "    node_collection.reset()\n",
    "    return node_collection.get(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_servers(client_configurations, data):\n",
    "    servers = {}    \n",
    "\n",
    "    for splits in client_configurations:\n",
    "        clients = {i:fedci.Client(pl.from_pandas(chunk)) for i,chunk in enumerate(np.array_split(data.to_pandas(), splits))}\n",
    "        servers[server_id_pattern.format(chosen_dag, splits)] = fedci.Server(clients, max_regressors=max_regressors)\n",
    "    return servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_tests(available_data):\n",
    "\n",
    "    possible_tests = []\n",
    "    max_conditioning_set_size = min(len(available_data), max_regressors) if max_regressors is not None else len(available_data)\n",
    "\n",
    "    for y_var in available_data:\n",
    "        set_of_regressors = available_data - {y_var}\n",
    "        for x_var in set_of_regressors:\n",
    "            set_of_conditioning_variables = set_of_regressors - {x_var}\n",
    "            conditioning_sets = chain.from_iterable(combinations(set_of_conditioning_variables, r) for r in range(0,max_conditioning_set_size))\n",
    "            possible_tests.extend([(y_var, x_var, sorted(list(s_labels))) for s_labels in conditioning_sets])\n",
    "            \n",
    "    return possible_tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycit import citest, itest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mixed_independence(continuous, categorical):\n",
    "    # ANOVA\n",
    "    categories = np.unique(categorical)\n",
    "    groups = [continuous[categorical == category] for category in categories]\n",
    "    _, p_value = stats.f_oneway(*groups)\n",
    "    #print(f\"ANOVA F-statistic: {f_statistic}, p-value: {p_value}\")\n",
    "\n",
    "    # If categorical is binary, you can also use point-biserial correlation\n",
    "    #if len(categories) == 2:\n",
    "    #    point_biserial_corr, p_value = stats.pointbiserialr(categorical, continuous)\n",
    "    #    print(f\"Point-biserial correlation: {point_biserial_corr}, p-value: {p_value}\")\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_tests(data, possible_tests):  \n",
    "    ground_truth_tests = []\n",
    "\n",
    "    for test in possible_tests:\n",
    "        if len(test[2]) > 0:\n",
    "            if data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.String:\n",
    "                #print('A')\n",
    "                X = data[test[0]].to_numpy()\n",
    "                Y = data[test[1]].to_numpy()\n",
    "                Z = data[test[2]].to_numpy()\n",
    "                pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            elif data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.Float64:\n",
    "                #print('B')\n",
    "                X = data[test[0]].to_numpy()\n",
    "                Y = data[test[1]].to_numpy()\n",
    "                Z = data[test[2]].to_numpy()\n",
    "                pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.String:\n",
    "                #print('C')\n",
    "                X = data[test[0]].to_numpy()\n",
    "                Y = data[test[1]].to_numpy()\n",
    "                Z = data[test[2]].to_numpy()\n",
    "                pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.Float64:\n",
    "                #print('D')\n",
    "                _, pvalue = CITests.pearsonr(test[1], test[0], list(test[2]), data.cast(pl.Float64).to_pandas(), boolean=False)\n",
    "            else:\n",
    "                X = data[test[0]].to_numpy()\n",
    "                Y = data[test[1]].to_numpy()\n",
    "                Z = data[test[2]].to_numpy()\n",
    "                pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "                #assert False, 'no fitting test'\n",
    "        else:\n",
    "            if data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.String:\n",
    "                crosstab = pd.crosstab(data.to_pandas()[test[0]], data.to_pandas()[test[1]])\n",
    "                _, pvalue, _, _ = stats.chi2_contingency(crosstab)\n",
    "            elif data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.Float64:\n",
    "                #print('E')\n",
    "                X = data[test[0]].to_numpy()\n",
    "                Y = data[test[1]].to_numpy().astype(float)\n",
    "                pvalue = test_mixed_independence(Y, X)\n",
    "            elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.String:\n",
    "                #print('F')\n",
    "                X = data[test[0]].to_numpy().astype(float)\n",
    "                Y = data[test[1]].to_numpy()\n",
    "                pvalue = test_mixed_independence(X, Y)\n",
    "            elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.Float64:\n",
    "                #print('G')\n",
    "                v0 = data[test[0]]\n",
    "                v1 = data[test[1]]\n",
    "                _, pvalue = stats.pearsonr(v0, v1)\n",
    "            #elif data.schema[test[0]] == pl.Int32 and data.schema[test[1]] == pl.Float64:\n",
    "            else:\n",
    "                X = data[test[0]].to_numpy().astype(float)\n",
    "                Y = data[test[1]].to_numpy().astype(float)\n",
    "                pvalue = itest(X, Y, test_args={'statistic': 'mixed_mi', 'n_jobs': 8})\n",
    "                #assert False, 'no fitting test w/o conditiong set'\n",
    "        pvalue = round(pvalue,4)\n",
    "\n",
    "        #print(test, pvalue)\n",
    "                \n",
    "        ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), pvalue))\n",
    "    return ground_truth_tests\n",
    "# TODO: with and without conditioning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_tests_old(data, possible_tests):    \n",
    "    ground_truth_tests = []\n",
    "\n",
    "    for test in possible_tests:\n",
    "        print(test)\n",
    "\n",
    "        if len(test[2]) > 0:\n",
    "            X = data[test[0]].to_numpy()\n",
    "            Y = data[test[1]].to_numpy()\n",
    "            Z = data[test[2]].to_numpy()\n",
    "            pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 2})\n",
    "        else:\n",
    "            X = data[test[0]].to_numpy()\n",
    "            Y = data[test[1]].to_numpy().astype(float)\n",
    "            pvalue = test_mixed_independence(X, Y)\n",
    "\n",
    "        pvalue = round(pvalue,4)\n",
    "        \n",
    "        ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), pvalue))\n",
    "    return ground_truth_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_tests_old(data, possible_tests):\n",
    "    ground_truth_tests = []\n",
    "\n",
    "    for test in possible_tests:\n",
    "        if len(test[2]) > 0:\n",
    "            #v0 = data[test[0]].values\n",
    "            #v1 = data[test[1]].values\n",
    "            #s = data[list(test[2])].values\n",
    "            #p0 = test[3]\n",
    "            #p1 = citest(v0, v1, s, test_args={'statistic': 'ksg_cmi', 'n_jobs': 8})\n",
    "            \n",
    "            _, p1 = CITests.pearsonr(test[1], test[0], list(test[2]), data.cast(pl.Float64).to_pandas(), boolean=False)\n",
    "        else:\n",
    "            \n",
    "            #dummied_data = data.to_dummies(cs.string(), separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            #v0 = data[test[0]].cast(pl.Float64).to_pandas()\n",
    "            #v1 = data[test[1]].cast(pl.Float64).to_pandas()\n",
    "            \n",
    "            d0 = data[test[0]]\n",
    "            d1 = data[test[1]]\n",
    "\n",
    "            \n",
    "            #v0 = d0.to_dummies(cs.string(), separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            #v1 = d1.to_dummies(cs.string(), separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            \n",
    "            v0 = d0.to_dummies(separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            v1 = d1.to_dummies(separator='__cat__', drop_first=True).cast(pl.Float64).to_pandas()\n",
    "            \n",
    "            \n",
    "            _, p1 = stats.pearsonr(v0, v1)\n",
    "            \n",
    "        p1 = round(p1,4)\n",
    "        \n",
    "        ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), p1))\n",
    "    return ground_truth_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_categories_in_regression_sets(tests, reversed_category_expressions):\n",
    "    #updated_tests = []\n",
    "    for test in tests:\n",
    "        test.X_labels = sorted(list(set([reversed_category_expressions[l] if l in reversed_category_expressions else l for l in test.X_labels])))\n",
    "    return tests\n",
    "\n",
    "def group_categorical_likelihood_tests(tests, category_expressions, reversed_category_expressions):\n",
    "    #category_expressions = servers['dag_chain4_1c'].category_expressions\n",
    "    #reversed_category_expressions = servers['dag_chain4_1c'].reversed_category_expressions\n",
    "    #tests = server_ci_tests['dag_chain4_1c']\n",
    "\n",
    "    updated_tests = []\n",
    "    for test in tests:\n",
    "        if test.y_label not in reversed_category_expressions:\n",
    "            updated_tests.append(test)\n",
    "            continue\n",
    "        \n",
    "        category_label = reversed_category_expressions[test.y_label]\n",
    "        \n",
    "        # Only run if the current test is the first category. This avoids duplicate tests\n",
    "        if category_expressions[category_label][0] != test.y_label:\n",
    "            continue\n",
    "        \n",
    "        categorical_test_group = []\n",
    "        for test_lookup in tests:\n",
    "            if test_lookup.y_label in category_expressions[category_label] and test_lookup.x_label == test.x_label and sorted(test_lookup.s_labels) == sorted(test.s_labels):\n",
    "                categorical_test_group.append(test_lookup)\n",
    "                \n",
    "        lrt = CategoricalLikelihoodRatioTest(category_label, [t.t0 for t in categorical_test_group], [t.t1 for t in categorical_test_group], len(category_expressions[category_label]))\n",
    "        updated_tests.append(lrt)\n",
    "        \n",
    "    return updated_tests\n",
    "\n",
    "\n",
    "def group_ordinal_likelihood_tests(tests, ordinal_expressions, reversed_ordinal_expressions):\n",
    "    #category_expressions = servers['dag_chain4_1c'].category_expressions\n",
    "    #reversed_category_expressions = servers['dag_chain4_1c'].reversed_category_expressions\n",
    "    #tests = server_ci_tests['dag_chain4_1c']\n",
    "\n",
    "    updated_tests = []\n",
    "    for test in tests:\n",
    "        if test.y_label not in reversed_ordinal_expressions:\n",
    "            updated_tests.append(test)\n",
    "            continue\n",
    "        \n",
    "        category_label = reversed_ordinal_expressions[test.y_label]\n",
    "        \n",
    "        # Only run if the current test is the first category. This avoids duplicate tests\n",
    "        if ordinal_expressions[category_label][0] != test.y_label:\n",
    "            continue\n",
    "        \n",
    "        categorical_test_group = []\n",
    "        for test_lookup in tests:\n",
    "            if test_lookup.y_label in ordinal_expressions[category_label] and test_lookup.x_label == test.x_label and sorted(test_lookup.s_labels) == sorted(test.s_labels):\n",
    "                categorical_test_group.append(test_lookup)\n",
    "                \n",
    "        lrt = OrdinalLikelihoodRatioTest(category_label, [t.t0 for t in categorical_test_group], [t.t1 for t in categorical_test_group], len(ordinal_expressions[category_label]))\n",
    "        updated_tests.append(lrt)\n",
    "        \n",
    "    return updated_tests\n",
    "\n",
    "\n",
    "def get_server_test_results(servers):\n",
    "    testing_rounds = {k:v.testing_engine.finished_rounds for k,v in servers.items()}\n",
    "    testing_rounds = {k:join_categories_in_regression_sets(v, servers[k].reversed_category_expressions) for k,v in testing_rounds.items()}\n",
    "    likelihood_tests = {k:fedci.get_likelihood_tests(v) for k,v in testing_rounds.items()}\n",
    "    # fix up categorical tests\n",
    "    likelihood_tests = {k:group_categorical_likelihood_tests(v, servers[k].category_expressions, servers[k].reversed_category_expressions) for k,v in likelihood_tests.items()}\n",
    "    \n",
    "    likelihood_tests = {k:group_ordinal_likelihood_tests(v, servers[k].category_expressions, servers[k].reversed_category_expressions) for k,v in likelihood_tests.items()}\n",
    "    \n",
    "    return likelihood_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_server_evaluation(ground_truth_tests, server_ci_tests):\n",
    "    p_value_comparison = {k:[] for k in server_ci_tests.keys()}\n",
    "    missing_test = {k:0 for k in server_ci_tests.keys()}\n",
    "    \n",
    "    for test in ground_truth_tests:\n",
    "        for k in server_ci_tests.keys():\n",
    "            matching_test = [t for t in server_ci_tests[k] if t.y_label == test.y_label and t.x_label == test.x_label and sorted(t.s_labels) == sorted(test.s_labels)]\n",
    "            if len(matching_test) == 0:\n",
    "                print(f'No matching test in {k} for {test}')\n",
    "                missing_test[k] += 1\n",
    "                continue\n",
    "            assert len(matching_test) == 1\n",
    "            matching_test = matching_test[0]          \n",
    "            p_value_comparison[k].append((matching_test.p_val, test.p_val))\n",
    "        \n",
    "    missing_test = {k:v/len(server_ci_tests[k]) if len(server_ci_tests[k]) > 0 else 0 for k,v in missing_test.items()}\n",
    "    return p_value_comparison, missing_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct_alpha_thresholdings(data, alpha):\n",
    "    c = sum([1 for a,b in data if (a < alpha and b < alpha) or (a > alpha and b > alpha)]) / len(data)\n",
    "    return c\n",
    "\n",
    "def count_correct_non_zero_alpha_thresholdings(data, alpha):\n",
    "    non_zero_data = [(a,b) for a,b in data if a != 0 and b != 0]\n",
    "    return count_correct_alpha_thresholdings(non_zero_data, alpha)\n",
    "\n",
    "def count_correct_pval(data, tolerance=1e-4):\n",
    "    c = sum([1 for a,b in data if abs(a-b)<tolerance]) / len(data)\n",
    "    return c\n",
    "\n",
    "def evaluate_results(p_value_comparison, alphas, tolerance):\n",
    "    result_alpha = {}\n",
    "    result_non_zero_alpha = {}\n",
    "    result_equality = {}\n",
    "    for k,v in p_value_comparison.items():\n",
    "        result_alpha[k] = {}\n",
    "        result_non_zero_alpha[k] = {}\n",
    "        result_equality[k] = count_correct_pval(v, tolerance)\n",
    "        for alpha in alphas:\n",
    "            result_alpha[k][alpha] = count_correct_alpha_thresholdings(v,alpha)\n",
    "            result_non_zero_alpha[k][alpha] = count_correct_non_zero_alpha_thresholdings(v,alpha)\n",
    "            \n",
    "    return result_alpha, result_non_zero_alpha, result_equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records(servers, alpha_tests, non_zero_pval_alpha_tests, equality_tests, missed_tests, total_features, features_per_client):\n",
    "    results = []\n",
    "    for server_id in servers.keys():\n",
    "        server = servers[server_id]\n",
    "        alpha_test = alpha_tests[server_id]\n",
    "        non_zero_pval_alpha_test = non_zero_pval_alpha_tests[server_id]\n",
    "        \n",
    "        r = {\n",
    "            'chosen_dag': chosen_dag,\n",
    "            'num_clients': len(server.clients),\n",
    "            'num_samples': TOTAL_SAMPLES,\n",
    "            'same_p_val': equality_tests[server_id],\n",
    "            'missed_tests': missed_tests[server_id],\n",
    "            'total_features': total_features,\n",
    "            'features_per_client': features_per_client\n",
    "        }\n",
    "        for alpha, alpha_result in alpha_test.items():\n",
    "            r[f'correctness_alpha_{alpha}'] = alpha_result\n",
    "        for alpha, alpha_result in non_zero_pval_alpha_test.items():\n",
    "            r[f'correctness_non_zero_alpha_{alpha}'] = alpha_result\n",
    "        results.append(r)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_add_row(data, file):\n",
    "    with open(file, 'a') as f:\n",
    "        row = ','.join([str(d) for d in data]) + '\\n'\n",
    "        f.write(row)\n",
    "            \n",
    "\n",
    "def write_records(i, file, data):\n",
    "    if len(data) == 0:\n",
    "        return\n",
    "    curr_file = file.format(i)\n",
    "    if not os.path.exists(curr_file):\n",
    "        csv_add_row(list(data[0].keys()), curr_file)\n",
    "    for entry in data:\n",
    "        csv_add_row(entry.values(), curr_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(i):\n",
    "    #print('Step 1/6 --> Setup')\n",
    "    #data = pl.read_parquet(f'./fedci/testdata-{i}.parquet')\n",
    "    #TOTAL_SAMPLES = len(data)\n",
    "    data = get_sample_data(ncs[i], TOTAL_SAMPLES)\n",
    "    servers = get_servers(client_configurations, data)\n",
    "\n",
    "    #print('Step 2/6 --> Run Tests')\n",
    "    for server in servers.values(): server.run_tests()\n",
    "\n",
    "    #print('Step 3/6 --> Collect Results')\n",
    "    possible_tests = get_possible_tests(set(data.columns))\n",
    "    ground_truth_tests = get_ground_truth_tests(data, possible_tests)\n",
    "    server_ci_tests = get_server_test_results(servers) \n",
    "\n",
    "    #print('Step 4/6 --> Prepare Evaluation')\n",
    "    p_val_comparisons, missed_tests = prepare_server_evaluation(ground_truth_tests, server_ci_tests)\n",
    "\n",
    "    #print('Step 5/6 --> Run Evaluation')\n",
    "    alpha_tests, non_zero_alpha_tests, equality_tests = evaluate_results(p_val_comparisons, alpha_comparisons, equality_tolerance)\n",
    "\n",
    "    #print('Step 6/6 --> Log Results')\n",
    "    records = get_records(servers, alpha_tests, non_zero_alpha_tests, equality_tests, missed_tests, TOTAL_FEATURES, FEATURES_PER_CLIENT)\n",
    "    \n",
    "    write_records(i, log_filepattern, records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: A, S: [], p: 0.578\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: B, x: A, S: [], p: 0.578\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: B, x: A, S: [], p: 0.578\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.996\n",
      "No matching test in dag_chain4_3c for LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.996\n",
      "No matching test in dag_chain4_5c for LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.996\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    process(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n"
     ]
    }
   ],
   "source": [
    "data = get_sample_data(ncs[4], TOTAL_SAMPLES)\n",
    "servers = get_servers(client_configurations, data)\n",
    "\n",
    "#print('Step 2/6 --> Run Tests')\n",
    "for server in servers.values(): server.run_tests()\n",
    "\n",
    "#print('Step 3/6 --> Collect Results')\n",
    "possible_tests = get_possible_tests(set(data.columns))\n",
    "ground_truth_tests = get_ground_truth_tests(data, possible_tests)\n",
    "server_ci_tests = get_server_test_results(servers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TestingRound - y: B__cat__1, X: [], total samples: 10000, beta: [0.35], current iteration: 3, current deviance: 2275.0, relative deviance change: 0.0, llf: -6786.3601290910665, rss: 2274.9999999996303,\n",
       " TestingRound - y: B__cat__2, X: [], total samples: 10000, beta: [0.34], current iteration: 3, current deviance: 2244.0, relative deviance change: 0.0, llf: -6717.759805378749, rss: 2244.0000000002447,\n",
       " TestingRound - y: B__cat__3, X: [], total samples: 10000, beta: [0.31], current iteration: 3, current deviance: 2138.999999999999, relative deviance change: 0.0, llf: -6478.1520175778405, rss: 2139.000000000027,\n",
       " TestingRound - y: C__cat__1, X: [], total samples: 10000, beta: [0.34], current iteration: 3, current deviance: 2244.0000000000005, relative deviance change: 0.0, llf: -6717.759805378748, rss: 2244.0000000002387,\n",
       " TestingRound - y: C__cat__2, X: [], total samples: 10000, beta: [0.3], current iteration: 3, current deviance: 2099.9999999999995, relative deviance change: 0.0, llf: -6386.146590723383, rss: 2099.9999999998067,\n",
       " TestingRound - y: C__cat__3, X: [], total samples: 10000, beta: [0.36], current iteration: 3, current deviance: 2304.0, relative deviance change: 0.0, llf: -6849.693581244724, rss: 2303.999999999953,\n",
       " TestingRound - y: A, X: [], total samples: 10000, beta: [0.00971039], current iteration: 3, current deviance: 10178.690151234594, relative deviance change: 0.0, llf: -14277.941537108325, rss: 10178.69015123462,\n",
       " TestingRound - y: B__cat__1, X: ['A'], total samples: 10000, beta: [ 0.35001023 -0.00105383], current iteration: 3, current deviance: 2274.9886960044682, relative deviance change: 1.9988115262719016e-16, llf: -6786.33528508312, rss: 2274.9886960044546,\n",
       " TestingRound - y: B__cat__2, X: ['A'], total samples: 10000, beta: [0.33998654 0.00138665], current iteration: 3, current deviance: 2243.9804284293837, relative deviance change: 0.0, llf: -6717.716196519644, rss: 2243.9804284293823,\n",
       " TestingRound - y: B__cat__3, X: ['A'], total samples: 10000, beta: [ 0.31000323 -0.00033282], current iteration: 3, current deviance: 2138.998872505843, relative deviance change: 0.0, llf: -6478.149382013432, rss: 2138.998872505852,\n",
       " TestingRound - y: C__cat__1, X: ['A'], total samples: 10000, beta: [ 0.3423127  -0.23816738], current iteration: 3, current deviance: 1666.627021999826, relative deviance change: 0.0, llf: -5230.469050491378, rss: 1666.6270219998282,\n",
       " TestingRound - y: C__cat__2, X: ['A'], total samples: 10000, beta: [ 0.30007891 -0.00812646], current iteration: 3, current deviance: 2099.327806456052, relative deviance change: 0.0, llf: -6384.545873702305, rss: 2099.3278064560504,\n",
       " TestingRound - y: C__cat__3, X: ['A'], total samples: 10000, beta: [0.35760839 0.24629384], current iteration: 3, current deviance: 1686.5539965731994, relative deviance change: 0.0, llf: -5289.896826823212, rss: 1686.5539965732,\n",
       " TestingRound - y: B__cat__1, X: ['C'], total samples: 10000, beta: [0.23676471 0.10190196 0.22962418], current iteration: 3, current deviance: 2182.252133986928, relative deviance change: 0.0, llf: -6578.247029659731, rss: 2182.252133986719,\n",
       " TestingRound - y: B__cat__2, X: ['C'], total samples: 10000, beta: [ 0.33647059  0.01486275 -0.0025817 ], current iteration: 3, current deviance: 2243.437869281046, relative deviance change: 2.0269207714875363e-16, llf: -6716.507129044791, rss: 2243.4378692810847,\n",
       " TestingRound - y: B__cat__3, X: ['C'], total samples: 10000, beta: [ 0.42676471 -0.11676471 -0.22704248], current iteration: 3, current deviance: 2048.864133986928, relative deviance change: 2.219401224957534e-16, llf: -6262.887660120187, rss: 2048.8641339868,\n",
       " TestingRound - y: C__cat__1, X: ['B'], total samples: 10000, beta: [0.23       0.10647059 0.23806452], current iteration: 3, current deviance: 2150.766034155598, relative deviance change: 2.1142523228555796e-16, llf: -6505.58023526494, rss: 2150.7660341554165,\n",
       " TestingRound - y: C__cat__2, X: ['B'], total samples: 10000, beta: [0.29028571 0.01971429 0.00971429], current iteration: 3, current deviance: 2099.3297142857145, relative deviance change: 2.1660517986961144e-16, llf: -6384.550417606284, rss: 2099.3297142855963,\n",
       " TestingRound - y: C__cat__3, X: ['B'], total samples: 10000, beta: [ 0.47971429 -0.12618487 -0.2477788 ], current iteration: 3, current deviance: 2202.8557484413122, relative deviance change: 0.0, llf: -6625.2327974583695, rss: 2202.855748441269,\n",
       " TestingRound - y: A, X: ['B'], total samples: 10000, beta: [0.00664565 0.007216   0.00197194], current iteration: 3, current deviance: 10178.594982872648, relative deviance change: 0.0, llf: -14277.894788064277, rss: 10178.594982872677,\n",
       " TestingRound - y: A, X: ['C'], total samples: 10000, beta: [-0.70329901  0.68543717  1.40938403], current iteration: 3, current deviance: 6702.133846664448, relative deviance change: 1.357002340683119e-16, llf: -12188.589668417782, rss: 6702.133846664422,\n",
       " TestingRound - y: B__cat__1, X: ['A', 'C'], total samples: 10000, beta: [ 0.17611623 -0.08623427  0.16101014  0.35116139], current iteration: 3, current deviance: 2132.4127219066436, relative deviance change: 0.0, llf: -6462.7302268391895, rss: 2132.4127219066477,\n",
       " TestingRound - y: B__cat__2, X: ['A', 'C'], total samples: 10000, beta: [ 0.33875987  0.00325506  0.0126316  -0.00716933], current iteration: 3, current deviance: 2243.3668572652828, relative deviance change: 0.0, llf: -6716.348860477077, rss: 2243.3668572652823,\n",
       " TestingRound - y: B__cat__3, X: ['A', 'C'], total samples: 10000, beta: [ 0.4851239   0.08297921 -0.17364174 -0.34399206], current iteration: 3, current deviance: 2002.7162606832374, relative deviance change: 2.270539538815869e-16, llf: -6148.981814460167, rss: 2002.7162606832358,\n",
       " TestingRound - y: C__cat__1, X: ['A', 'B'], total samples: 10000, beta: [ 0.23158325 -0.23823801  0.10818971  0.23853431], current iteration: 3, current deviance: 1573.0559503448228, relative deviance change: 0.0, llf: -4941.560830272816, rss: 1573.055950344828,\n",
       " TestingRound - y: C__cat__2, X: ['A', 'B'], total samples: 10000, beta: [ 0.29033988 -0.00815064  0.0197731   0.00973036], current iteration: 3, current deviance: 2098.6535209504245, relative deviance change: 0.0, llf: -6382.939660003773, rss: 2098.653520950419,\n",
       " TestingRound - y: C__cat__3, X: ['A', 'B'], total samples: 10000, beta: [ 0.47807687  0.24638865 -0.12796281 -0.24826467], current iteration: 3, current deviance: 1584.9400539758603, relative deviance change: 2.8689959584667367e-16, llf: -4979.192795969493, rss: 1584.940053975855,\n",
       " TestingRound - y: A, X: ['B', 'C'], total samples: 10000, beta: [-0.91996528  0.18126917  0.36477834  0.72533625  1.49267219], current iteration: 3, current deviance: 6495.312286554582, relative deviance change: 1.400210889854629e-16, llf: -12031.86351726382, rss: 6495.3122865545965]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "servers['dag_chain4_1c'].testing_engine.finished_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: C, S: [], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: C, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: A, S: [], p: 0.2867\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: B, x: A, S: ['C'], p: 0.791\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: C, x: B, S: [], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: C, x: B, S: ['A'], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: A, x: B, S: [], p: 0.2867\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: A, x: B, S: ['C'], p: 0.366\n",
      "No matching test in dag_chain4_1c for LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p_val_comparisons, missed_tests = prepare_server_evaluation(ground_truth_tests, server_ci_tests)\n",
    "\n",
    "alpha_tests, equality_tests = evaluate_results(p_val_comparisons, alpha_comparisons, equality_tolerance)\n",
    "\n",
    "records = get_records(servers, alpha_tests, equality_tests, missed_tests, TOTAL_FEATURES, FEATURES_PER_CLIENT)\n",
    "\n",
    "#write_records(99, log_filepattern, records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
