{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "INFO:rpy2.situation:cffi mode is CFFI_MODE.ANY\n",
      "INFO:rpy2.situation:R home found: /opt/homebrew/Caskroom/miniforge/base/envs/promotion/lib/R\n",
      "INFO:rpy2.situation:R library path: \n",
      "INFO:rpy2.situation:LD_LIBRARY_PATH: \n",
      "INFO:rpy2.rinterface_lib.embedded:Default options to initialize R: rpy2, --quiet, --no-save\n",
      "INFO:rpy2.rinterface_lib.embedded:R is already initialized. No need to initialize.\n"
     ]
    }
   ],
   "source": [
    "import fedci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SAMPLES = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP 2\n",
    "\n",
    "possible_dags = [\n",
    "    \"pdsep_g\",\n",
    "    \"collider\",\n",
    "    \"fork\",\n",
    "    \"chain4\",\n",
    "    \"descColl\",\n",
    "    \"2descColl\",\n",
    "    \"iv\"\n",
    "]\n",
    "\n",
    "def get_sample_data(dag_type, num_clients, num_samples, num_vars):\n",
    "    with (ro.default_converter + pandas2ri.converter).context():\n",
    "        ro.r['source']('./app/scripts/example_data.r')\n",
    "        get_example_data_f = ro.globalenv['get_example_data']\n",
    "\n",
    "        result = get_example_data_f(dag_type, num_clients, num_samples, num_vars)\n",
    "        \n",
    "    return result\n",
    "\n",
    "# TOTAL_FEATURES cant be set for this call right now, because the true graph is fixed\n",
    "data = get_sample_data(possible_dags[3], 1, TOTAL_SAMPLES, 4)\n",
    "\n",
    "data = list(data.items())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n"
     ]
    }
   ],
   "source": [
    "servers = {}\n",
    "\n",
    "servers['1 client'] = fedci.Server({0: fedci.Client(data)})\n",
    "\n",
    "clients = {i:fedci.Client(chunk) for i,chunk in enumerate(np.array_split(data, 3))}\n",
    "servers['3 clients'] = fedci.Server(clients)\n",
    "\n",
    "#clients = {i:fedci.Client(chunk) for i,chunk in enumerate(np.array_split(data, 10))}\n",
    "#servers['5 clients'] = fedci.Server(clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fcffc9e98f486ca218df918f3ef6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for server in servers.values():\n",
    "    server.run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import CITests\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, x_label, s_labels, p_val):\n",
    "        self.y_label = y_label\n",
    "        self.x_label = x_label\n",
    "        self.s_labels = s_labels\n",
    "        self.p_val = p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_rounds = {k:v.testing_engine.finished_rounds for k,v in servers.items()}\n",
    "\n",
    "possible_tests = set([(t.y_label, t.x_label, tuple(sorted(t.s_labels)), t.p_val) for t in fedci.get_likelihood_tests(testing_rounds['1 client'])])\n",
    "len(possible_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_tests = []\n",
    "\n",
    "for test in possible_tests:\n",
    "    if len(test[2]) > 0:\n",
    "        #v0 = data[test[0]].values\n",
    "        #v1 = data[test[1]].values\n",
    "        #s = data[list(test[2])].values\n",
    "        #p0 = test[3]\n",
    "        #p1 = citest(v0, v1, s, test_args={'statistic': 'ksg_cmi', 'n_jobs': 8})\n",
    "        \n",
    "        _, p1 = CITests.pearsonr(test[1], test[0], list(test[2]), data, boolean=False)\n",
    "    else:\n",
    "        v0 = data[test[0]]\n",
    "        v1 = data[test[1]]\n",
    "        correlation, p1 = stats.pearsonr(v0, v1)\n",
    "        \n",
    "    p1 = round(p1,4)\n",
    "    \n",
    "    ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), p1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_tests = {k:fedci.get_likelihood_tests(v) for k,v in testing_rounds.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value_comparison = {k:[] for k in likelihood_tests.keys()}\n",
    "missing_test = {k:0 for k in likelihood_tests.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "LikelihoodRatioTest - y: D, x: A, S: ['B'], p: 0.8468\n",
      "LikelihoodRatioTest - y: D, x: A, S: ['B'], p: 0.8467\n",
      "----------\n",
      "LikelihoodRatioTest - y: D, x: A, S: ['B'], p: 0.8468\n",
      "LikelihoodRatioTest - y: D, x: A, S: ['B'], p: 0.2318\n",
      "----------\n",
      "LikelihoodRatioTest - y: D, x: A, S: ['B', 'C'], p: 0.6682\n",
      "LikelihoodRatioTest - y: D, x: A, S: ['B', 'C'], p: 0.3801\n",
      "----------\n",
      "LikelihoodRatioTest - y: C, x: A, S: ['B', 'D'], p: 0.0896\n",
      "LikelihoodRatioTest - y: C, x: A, S: ['B', 'D'], p: 0.5621\n",
      "----------\n",
      "LikelihoodRatioTest - y: A, x: D, S: ['B'], p: 0.8468\n",
      "LikelihoodRatioTest - y: A, x: D, S: ['B'], p: 0.8467\n",
      "----------\n",
      "LikelihoodRatioTest - y: A, x: D, S: ['B'], p: 0.8468\n",
      "LikelihoodRatioTest - y: A, x: D, S: ['B'], p: 0.2318\n",
      "----------\n",
      "LikelihoodRatioTest - y: B, x: D, S: ['A', 'C'], p: 0.4265\n",
      "LikelihoodRatioTest - y: B, x: D, S: ['A', 'C'], p: 0.4264\n",
      "----------\n",
      "LikelihoodRatioTest - y: B, x: D, S: ['A', 'C'], p: 0.4265\n",
      "LikelihoodRatioTest - y: B, x: D, S: ['A', 'C'], p: 0.432\n",
      "----------\n",
      "LikelihoodRatioTest - y: D, x: A, S: [], p: 0.0002\n",
      "LikelihoodRatioTest - y: D, x: A, S: [], p: 0.2023\n",
      "----------\n",
      "LikelihoodRatioTest - y: A, x: D, S: ['B', 'C'], p: 0.6682\n",
      "LikelihoodRatioTest - y: A, x: D, S: ['B', 'C'], p: 0.3801\n",
      "----------\n",
      "LikelihoodRatioTest - y: D, x: B, S: ['A', 'C'], p: 0.4265\n",
      "LikelihoodRatioTest - y: D, x: B, S: ['A', 'C'], p: 0.4264\n",
      "----------\n",
      "LikelihoodRatioTest - y: D, x: B, S: ['A', 'C'], p: 0.4265\n",
      "LikelihoodRatioTest - y: D, x: B, S: ['A', 'C'], p: 0.432\n",
      "----------\n",
      "LikelihoodRatioTest - y: D, x: B, S: ['C'], p: 0.4516\n",
      "LikelihoodRatioTest - y: D, x: B, S: ['C'], p: 0.4515\n",
      "----------\n",
      "LikelihoodRatioTest - y: D, x: B, S: ['C'], p: 0.4516\n",
      "LikelihoodRatioTest - y: D, x: B, S: ['C'], p: 0.506\n",
      "----------\n",
      "LikelihoodRatioTest - y: B, x: D, S: ['C'], p: 0.4516\n",
      "LikelihoodRatioTest - y: B, x: D, S: ['C'], p: 0.4515\n",
      "----------\n",
      "LikelihoodRatioTest - y: B, x: D, S: ['C'], p: 0.4516\n",
      "LikelihoodRatioTest - y: B, x: D, S: ['C'], p: 0.506\n",
      "----------\n",
      "LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0982\n",
      "LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0981\n",
      "----------\n",
      "LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.0982\n",
      "LikelihoodRatioTest - y: C, x: A, S: ['B'], p: 0.3184\n",
      "----------\n",
      "LikelihoodRatioTest - y: A, x: D, S: [], p: 0.0002\n",
      "LikelihoodRatioTest - y: A, x: D, S: [], p: 0.2023\n",
      "----------\n",
      "LikelihoodRatioTest - y: A, x: C, S: ['B', 'D'], p: 0.0896\n",
      "LikelihoodRatioTest - y: A, x: C, S: ['B', 'D'], p: 0.5621\n",
      "----------\n",
      "LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0982\n",
      "LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0981\n",
      "----------\n",
      "LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.0982\n",
      "LikelihoodRatioTest - y: A, x: C, S: ['B'], p: 0.3184\n",
      "----------\n",
      "LikelihoodRatioTest - y: D, x: A, S: ['C'], p: 0.7312\n",
      "LikelihoodRatioTest - y: D, x: A, S: ['C'], p: 0.4403\n",
      "----------\n",
      "LikelihoodRatioTest - y: A, x: D, S: ['C'], p: 0.7312\n",
      "LikelihoodRatioTest - y: A, x: D, S: ['C'], p: 0.4403\n"
     ]
    }
   ],
   "source": [
    "for test in ground_truth_tests:\n",
    "    for k, likelihood_test in likelihood_tests.items():\n",
    "        matching_test = [t for t in likelihood_tests[k] if t.y_label == test.y_label and t.x_label == test.x_label and sorted(t.s_labels) == sorted(test.s_labels)]\n",
    "        if len(matching_test) == 0:\n",
    "            print(f'No matching test in {k} for {test}')\n",
    "            missing_test[k] += 1\n",
    "        assert len(matching_test) == 1\n",
    "        matching_test = matching_test[0]\n",
    "        \n",
    "        # if matching_test.p_val != test.p_val:\n",
    "        #     print('-'*10)\n",
    "        #     print(test)\n",
    "        #     print(matching_test)\n",
    "        \n",
    "        p_value_comparison[k].append((matching_test.p_val, test.p_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([abs(a-b) < 0.0001 for a,b in p_value_comparison['3 clients']]) / len(p_value_comparison['3 clients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct_alpha_thresholdings(data, alpha):\n",
    "    c = sum([1 for a,b in data if (a < alpha and b < alpha) or (a > alpha and b > alpha)]) / len(data)\n",
    "    return c\n",
    "\n",
    "def count_correct_pval(data, tolerance=1e-4):\n",
    "    c = sum([1 for a,b in data if abs(a-b)<tolerance]) / len(data)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_correct_alpha_thresholdings(p_value_comparison['3 clients'], 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_correct_pval(p_value_comparison['3 clients'], 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: gather all stats\n",
    "# TODO: write all stats to csv for evaluation later on\n",
    "# TODO: make easily loopable (single func)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
