{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fedci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain, combinations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from pgmpy.estimators import CITests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, x_label, s_labels, p_val):\n",
    "        self.y_label = y_label\n",
    "        self.x_label = x_label\n",
    "        self.s_labels = s_labels\n",
    "        self.p_val = p_val\n",
    "        \n",
    "class CategoricalLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, t0s, t1s, num_cats):\n",
    "        assert len(t0s) > 0\n",
    "        assert len(t1s) > 0\n",
    "        assert len(t0s[0].X_labels) + 1 == len(t1s[0].X_labels)\n",
    "        # TODO: assert more data integrity\n",
    "        #assert t0s[0].y_label == t1s[0].y_label\n",
    "        \n",
    "        self.y_label = y_label\n",
    "        self.x_label = (set(t1s[0].X_labels) - set(t0s[0].X_labels)).pop()\n",
    "        self.s_labels = t0s[0].X_labels\n",
    "        self.p_val = self._run_likelihood_test(t0s, t1s, num_cats)\n",
    "        self.p_val = round(self.p_val, 4)\n",
    "        \n",
    "    def _run_likelihood_test(self, t0s, t1s, num_cats):\n",
    "        \n",
    "        # t1 should always encompass more regressors -> less client can fulfill this\n",
    "        #assert len(self.t1.providing_clients) < len(self.t0.providing_clients)\n",
    "        \n",
    "        providing_clients = t1s[0].providing_clients\n",
    "        \n",
    "        t0_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t0s])\n",
    "        t1_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t1s])\n",
    "        \n",
    "        # d_y = num cats\n",
    "        # DOF Z = size cond set\n",
    "        # DOF X = 1\n",
    "        t0_dof = (num_cats-1)*(len(self.s_labels)+1) # (d_y - 1)*(DOF(Z)+1)\n",
    "        t1_dof = (num_cats-1)*(len(self.s_labels)+2) # (d_y - 1)*(DOF(Z)+DOF(X)+1)\n",
    "        t = -2*(t0_llf - t1_llf)\n",
    "        \n",
    "        p_val = stats.chi2.sf(t, t1_dof-t0_dof)\n",
    "        \n",
    "        return p_val\n",
    "    \n",
    "class OrdinalLikelihoodRatioTest(fedci.LikelihoodRatioTest):\n",
    "    def __init__(self, y_label, t0s, t1s, num_cats):\n",
    "        assert len(t0s) > 0\n",
    "        assert len(t1s) > 0\n",
    "        #assert len(t0s) == len(t1s)\n",
    "        assert len(t0s[0].X_labels) + 1 == len(t1s[0].X_labels)\n",
    "        # TODO: assert more data integrity\n",
    "        #assert t0s[0].y_label == t1s[0].y_label\n",
    "        \n",
    "        t0s = sorted(t0s, key=lambda x: int(x.y_label.split('__ord__')[-1]))\n",
    "        t1s = sorted(t1s, key=lambda x: int(x.y_label.split('__ord__')[-1]))\n",
    "        \n",
    "        self.y_label = y_label\n",
    "        self.x_label = (set(t1s[0].X_labels) - set(t0s[0].X_labels)).pop()\n",
    "        self.s_labels = t0s[0].X_labels        \n",
    "        self.p_val = self._run_likelihood_test(t0s, t1s, num_cats)\n",
    "        self.p_val = round(self.p_val, 4)\n",
    "        \n",
    "    def _run_likelihood_test(self, t0s, t1s, num_cats):\n",
    "        \n",
    "        # t1 should always encompass more regressors -> less client can fulfill this\n",
    "        #assert len(self.t1.providing_clients) < len(self.t0.providing_clients)\n",
    "        \n",
    "        providing_clients = t1s[0].providing_clients\n",
    "        \n",
    "        t0_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t0s])\n",
    "        t1_llf = sum([t.get_fit_stats(providing_clients)['llf'] for t in t1s])\n",
    "        \n",
    "        # d_y = num cats\n",
    "        # DOF Z = size cond set\n",
    "        # DOF X = 1\n",
    "        t0_dof = (num_cats-1)*(len(self.s_labels)+1) # (d_y - 1)*(DOF(Z)+1)\n",
    "        t1_dof = (num_cats-1)*(len(self.s_labels)+2) # (d_y - 1)*(DOF(Z)+DOF(X)+1)\n",
    "        t = -2*(t0_llf - t1_llf)\n",
    "        \n",
    "        p_val = stats.chi2.sf(t, t1_dof-t0_dof)\n",
    "        \n",
    "        return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_id_pattern = '{}_{}c'\n",
    "\n",
    "max_regressors = 0#None\n",
    "\n",
    "alpha_comparisons = [0.01, 0.05, 0.1]\n",
    "equality_tolerance = 1e-4\n",
    "\n",
    "\n",
    "log_filepattern = './log-{}.ndjson'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_independence_tests_collider = [\n",
    "    EmptyLikelihoodRatioTest('A', 'B', [], 1),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A'], 0),\n",
    "]\n",
    "\n",
    "real_independence_tests_fork = [\n",
    "    EmptyLikelihoodRatioTest('A', 'B', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A'], 1),\n",
    "]\n",
    "\n",
    "real_independence_tests_diamond = [\n",
    "    # cond set 0\n",
    "    EmptyLikelihoodRatioTest('A', 'B', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', [], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', [], 0),\n",
    "    # cond set 1\n",
    "    # start a\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['B'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['C'], 0),\n",
    "    # start b\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A'], 1),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['C'], 0),\n",
    "    # start c\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['B'], 0),\n",
    "    # cond set 2\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['B', 'C'], 1),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['A', 'C'], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['A', 'B'], 0),\n",
    "]\n",
    "\n",
    "real_independence_tests_chain = [\n",
    "    # cond set 0\n",
    "    EmptyLikelihoodRatioTest('A', 'B', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', [], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', [], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', [], 0),\n",
    "    # cond set 1\n",
    "    # start a\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B'], 1),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['B'], 1),\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['C'], 1),\n",
    "    # start b\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['D'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['C'], 1),\n",
    "    # start c\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['A'], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['B'], 0),\n",
    "    # cond set 2\n",
    "    EmptyLikelihoodRatioTest('A', 'B', ['C', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'C', ['B', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('A', 'D', ['B', 'C'], 1),\n",
    "    EmptyLikelihoodRatioTest('B', 'C', ['A', 'D'], 0),\n",
    "    EmptyLikelihoodRatioTest('B', 'D', ['A', 'C'], 0),\n",
    "    EmptyLikelihoodRatioTest('C', 'D', ['A', 'B'], 0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgp\n",
    "\n",
    "# fork\n",
    "node1 = dgp.GenericNode('A')\n",
    "node2 = dgp.GenericNode('B', parents=[node1])\n",
    "node3 = dgp.GenericNode('C', parents=[node1])\n",
    "nc1 = dgp.NodeCollection('generic_fork', [node1, node2, node3])\n",
    "\n",
    "# collider\n",
    "node1 = dgp.GenericNode('A')\n",
    "node2 = dgp.GenericNode('B')\n",
    "node3 = dgp.GenericNode('C', parents=[node1, node2])\n",
    "nc2 = dgp.NodeCollection('generic_collider', [node1, node2, node3])\n",
    "\n",
    "# diamond\n",
    "node1 = dgp.GenericNode('A')\n",
    "node2 = dgp.GenericNode('B', parents=[node1])\n",
    "node3 = dgp.GenericNode('C', parents=[node1])\n",
    "node4 = dgp.GenericNode('D', parents=[node2, node3])\n",
    "nc3 = dgp.NodeCollection('generic_diamond', [node1, node2, node3, node4])\n",
    "\n",
    "# chain\n",
    "node1 = dgp.GenericNode('A')\n",
    "node2 = dgp.GenericNode('B', parents=[node1])\n",
    "node3 = dgp.GenericNode('C', parents=[node2])\n",
    "node4 = dgp.GenericNode('D', parents=[node3])\n",
    "nc4 = dgp.NodeCollection('generic_chain', [node1, node2, node3, node4])\n",
    "\n",
    "\n",
    "### Categorical data test\n",
    "# fork\n",
    "node_restr = [dgp.CategoricalNode]\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1], node_restrictions=node_restr)\n",
    "nc61 = dgp.NodeCollection('categorical_fork', [node1, node2, node3])\n",
    "\n",
    "# collider\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1, node2], node_restrictions=node_restr)\n",
    "nc62 = dgp.NodeCollection('categorical_collider', [node1, node2, node3])\n",
    "\n",
    "# diamond\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1], node_restrictions=node_restr)\n",
    "node4 = dgp.GenericNode('D', parents=[node2, node3], node_restrictions=node_restr)\n",
    "nc63 = dgp.NodeCollection('categorical_diamond', [node1, node2, node3, node4])\n",
    "\n",
    "### Ordinal data test\n",
    "# fork\n",
    "node_restr = [dgp.OrdinalNode]\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1], node_restrictions=node_restr)\n",
    "nc71 = dgp.NodeCollection('ordinal_fork', [node1, node2, node3])\n",
    "\n",
    "# collider\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1, node2], node_restrictions=node_restr)\n",
    "nc72 = dgp.NodeCollection('ordinal_collider', [node1, node2, node3])\n",
    "\n",
    "# diamond\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1], node_restrictions=node_restr)\n",
    "node4 = dgp.GenericNode('D', parents=[node2, node3], node_restrictions=node_restr)\n",
    "nc73 = dgp.NodeCollection('ordinal_diamond', [node1, node2, node3, node4])\n",
    "\n",
    "\n",
    "### ONLY CONTINUOS DATA FOR COMPARISON PURPOSES (TIKHONOV REG.)\n",
    "# fork\n",
    "node_restr = [dgp.Node]\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1], node_restrictions=node_restr)\n",
    "nc81 = dgp.NodeCollection('continuos_fork', [node1, node2, node3])\n",
    "\n",
    "# collider\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1, node2], node_restrictions=node_restr)\n",
    "nc82 = dgp.NodeCollection('continuos_collider', [node1, node2, node3])\n",
    "\n",
    "# diamond\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node1], node_restrictions=node_restr)\n",
    "node4 = dgp.GenericNode('D', parents=[node2, node3], node_restrictions=node_restr)\n",
    "nc83 = dgp.NodeCollection('continuos_diamond', [node1, node2, node3, node4])\n",
    "\n",
    "# chain\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node2], node_restrictions=node_restr)\n",
    "node4 = dgp.GenericNode('D', parents=[node3], node_restrictions=node_restr)\n",
    "nc84 = dgp.NodeCollection('continuos_chain', [node1, node2, node3, node4])\n",
    "\n",
    "\n",
    "# experiments\n",
    "node_restr = [dgp.CategoricalNode]\n",
    "node1 = dgp.GenericNode('A', node_restrictions=node_restr)\n",
    "node2 = dgp.GenericNode('B', parents=[node1], node_restrictions=node_restr)\n",
    "node3 = dgp.GenericNode('C', parents=[node2], node_restrictions=node_restr)\n",
    "node4 = dgp.GenericNode('D', parents=[node3], node_restrictions=node_restr)\n",
    "nc99 = dgp.NodeCollection('experimental', [node1, node2, node3, node4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We considered five combinations of variable types and corresponding regression models:  \n",
    "  * (a) linear-binary (L-B),  \n",
    "  * (b) linear -multinomial (L-M),  \n",
    "  * (c) linear-ordinal (L-O),  \n",
    "  * (d) binary-ordinal (B-O), and  \n",
    "  * (e) multinomial-ordinal (M-O).  \n",
    "For each case, we considered the following simple BN mod- els:   \n",
    "  * (a) X Y (unconditional independence),  \n",
    "  * (b) X → Y and X ← Y (unconditional dependence),  \n",
    "  * (c) X → Z ← Y (conditional dependence of X and Y given Z), also known as collider [37], and  \n",
    "  * (d) X ← Z → Y (conditional indepen- dence of X and Y given Z).  \n",
    "In all cases, Z is continuous.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## L-B CASE\n",
    "# Unc. Indep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "nc911 = dgp.NodeCollection('L-B Unc. Indep.', [node1, node2])\n",
    "# Unc. Dep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "nc912 = dgp.NodeCollection('L-B Unc. Dep.', [node1, node2])\n",
    "# Con. Dep. Case given Z\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "node3 = dgp.GenericNode('Z', parents=[node1, node2], node_restrictions=[dgp.Node])\n",
    "nc913 = dgp.NodeCollection('L-B Con. Dep.', [node1, node2, node3])\n",
    "# Con. Indep. Case given Z\n",
    "node1 = dgp.GenericNode('Z', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('X', parents=[node1], node_restrictions=[dgp.Node])\n",
    "node3 = dgp.GenericNode('Y', parents=[node1], node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "nc914 = dgp.NodeCollection('L-B Con. Indep.', [node1, node2, node3])\n",
    "\n",
    "## L-M CASE\n",
    "# Unc. Indep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.CategoricalNode], min_categories=3)\n",
    "nc921 = dgp.NodeCollection('L-M Unc. Indep.', [node1, node2])\n",
    "# Unc. Dep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.CategoricalNode], min_categories=3)\n",
    "nc922 = dgp.NodeCollection('L-M Unc. Dep.', [node1, node2])\n",
    "# Con. Dep. Case given Z\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.CategoricalNode], min_categories=3)\n",
    "node3 = dgp.GenericNode('Z', parents=[node1, node2], node_restrictions=[dgp.Node])\n",
    "nc923 = dgp.NodeCollection('L-M Con. Dep.', [node1, node2, node3])\n",
    "# Con. Indep. Case given Z\n",
    "node1 = dgp.GenericNode('Z', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('X', parents=[node1], node_restrictions=[dgp.Node])\n",
    "node3 = dgp.GenericNode('Y', parents=[node1], node_restrictions=[dgp.CategoricalNode], min_categories=3)\n",
    "nc924 = dgp.NodeCollection('L-M Con. Indep.', [node1, node2, node3])\n",
    "\n",
    "## L-O CASE\n",
    "# Unc. Indep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "nc931 = dgp.NodeCollection('L-O Unc. Indep.', [node1, node2])\n",
    "# Unc. Dep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "nc932 = dgp.NodeCollection('L-O Unc. Dep.', [node1, node2])\n",
    "# Con. Dep. Case given Z\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "node3 = dgp.GenericNode('Z', parents=[node1, node2], node_restrictions=[dgp.Node])\n",
    "nc933 = dgp.NodeCollection('L-O Con. Dep.', [node1, node2, node3])\n",
    "# Con. Indep. Case given Z\n",
    "node1 = dgp.GenericNode('Z', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('X', parents=[node1], node_restrictions=[dgp.Node])\n",
    "node3 = dgp.GenericNode('Y', parents=[node1], node_restrictions=[dgp.OrdinalNode])\n",
    "nc934 = dgp.NodeCollection('L-O Con. Indep.', [node1, node2, node3])\n",
    "\n",
    "## B-O CASE\n",
    "# Unc. Indep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "nc941 = dgp.NodeCollection('B-O Unc. Indep.', [node1, node2])\n",
    "# Unc. Dep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "nc942 = dgp.NodeCollection('B-O Unc. Dep.', [node1, node2])\n",
    "# Con. Dep. Case given Z\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "node3 = dgp.GenericNode('Z', parents=[node1, node2], node_restrictions=[dgp.Node])\n",
    "nc943 = dgp.NodeCollection('B-O Con. Dep.', [node1, node2, node3])\n",
    "# Con. Indep. Case given Z\n",
    "node1 = dgp.GenericNode('Z', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('X', parents=[node1], node_restrictions=[dgp.CategoricalNode], max_categories=2)\n",
    "node3 = dgp.GenericNode('Y', parents=[node1], node_restrictions=[dgp.OrdinalNode])\n",
    "nc944 = dgp.NodeCollection('B-O Con. Indep.', [node1, node2, node3])\n",
    "\n",
    "## M-O CASE\n",
    "# Unc. Indep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.CategoricalNode])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "nc951 = dgp.NodeCollection('M-O Unc. Indep.', [node1, node2])\n",
    "# Unc. Dep. Case\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.CategoricalNode])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "nc952 = dgp.NodeCollection('M-O Unc. Dep.', [node1, node2])\n",
    "# Con. Dep. Case given Z\n",
    "node1 = dgp.GenericNode('X', node_restrictions=[dgp.CategoricalNode])\n",
    "node2 = dgp.GenericNode('Y', node_restrictions=[dgp.OrdinalNode])\n",
    "node3 = dgp.GenericNode('Z', parents=[node1, node2], node_restrictions=[dgp.Node])\n",
    "nc953 = dgp.NodeCollection('M-O Con. Dep.', [node1, node2, node3])\n",
    "# Con. Indep. Case given Z\n",
    "node1 = dgp.GenericNode('Z', node_restrictions=[dgp.Node])\n",
    "node2 = dgp.GenericNode('X', parents=[node1], node_restrictions=[dgp.CategoricalNode])\n",
    "node3 = dgp.GenericNode('Y', parents=[node1], node_restrictions=[dgp.OrdinalNode])\n",
    "nc954 = dgp.NodeCollection('M-O Con. Indep.', [node1, node2, node3])\n",
    "\n",
    "\n",
    "real_independence_tests_unc_ind = [\n",
    "    EmptyLikelihoodRatioTest('X', 'Y', [], 1)\n",
    "]\n",
    "\n",
    "real_independence_tests_unc_dep = [\n",
    "    EmptyLikelihoodRatioTest('X', 'Y', [], 0)\n",
    "]\n",
    "\n",
    "real_independence_tests_con_dep = [\n",
    "    EmptyLikelihoodRatioTest('X', 'Y', [], 0),\n",
    "    EmptyLikelihoodRatioTest('X', 'Z', [], 0),\n",
    "    EmptyLikelihoodRatioTest('Y', 'Z', [], 0),\n",
    "    EmptyLikelihoodRatioTest('X', 'Y', ['Z'], 1),\n",
    "    EmptyLikelihoodRatioTest('X', 'Z', ['Y'], 0),\n",
    "    EmptyLikelihoodRatioTest('Y', 'Z', ['X'], 0),\n",
    "]\n",
    "\n",
    "real_independence_tests_con_ind = [\n",
    "    EmptyLikelihoodRatioTest('X', 'Y', [], 0),\n",
    "    EmptyLikelihoodRatioTest('X', 'Z', [], 0),\n",
    "    EmptyLikelihoodRatioTest('Y', 'Z', [], 0),\n",
    "    EmptyLikelihoodRatioTest('X', 'Y', ['Z'], 1),\n",
    "    EmptyLikelihoodRatioTest('X', 'Z', ['Y'], 0),\n",
    "    EmptyLikelihoodRatioTest('Y', 'Z', ['X'], 0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ncs = {\n",
    "    1: nc1,\n",
    "    2: nc2,\n",
    "    3: nc3,\n",
    "    4: nc4,\n",
    "    61: nc61,\n",
    "    62: nc62,\n",
    "    63: nc63,\n",
    "    71: nc71,\n",
    "    72: nc72,\n",
    "    73: nc73,\n",
    "    81: nc81,\n",
    "    82: nc82,\n",
    "    83: nc83,\n",
    "    84: nc84,\n",
    "    99: nc99,\n",
    "    # paper tests\n",
    "    911: nc911,\n",
    "    912: nc912,\n",
    "    913: nc913,\n",
    "    914: nc914,\n",
    "    \n",
    "    921: nc921,\n",
    "    922: nc922,\n",
    "    923: nc923,\n",
    "    924: nc924,\n",
    "    \n",
    "    931: nc931,\n",
    "    932: nc932,\n",
    "    933: nc933,\n",
    "    934: nc934,\n",
    "    \n",
    "    941: nc941,\n",
    "    942: nc942,\n",
    "    943: nc943,\n",
    "    944: nc944,\n",
    "    \n",
    "    951: nc951,\n",
    "    952: nc952,\n",
    "    953: nc953,\n",
    "    954: nc954,\n",
    "    }\n",
    "\n",
    "ncs_independences = {\n",
    "    1: real_independence_tests_fork,\n",
    "    2: real_independence_tests_collider,\n",
    "    3: real_independence_tests_diamond,\n",
    "    4: real_independence_tests_chain,\n",
    "    61: real_independence_tests_fork,\n",
    "    62: real_independence_tests_collider,\n",
    "    63: real_independence_tests_diamond,\n",
    "    71: real_independence_tests_fork,\n",
    "    72: real_independence_tests_collider,\n",
    "    73: real_independence_tests_diamond,\n",
    "    81: real_independence_tests_fork,\n",
    "    82: real_independence_tests_collider,\n",
    "    83: real_independence_tests_diamond,\n",
    "    84: real_independence_tests_chain,\n",
    "    99: real_independence_tests_chain,\n",
    "    # paper data\n",
    "    911: real_independence_tests_unc_ind,\n",
    "    912: real_independence_tests_unc_dep,\n",
    "    913: real_independence_tests_con_dep,\n",
    "    914: real_independence_tests_con_ind,\n",
    "    \n",
    "    921: real_independence_tests_unc_ind,\n",
    "    922: real_independence_tests_unc_dep,\n",
    "    923: real_independence_tests_con_dep,\n",
    "    924: real_independence_tests_con_ind,\n",
    "    \n",
    "    931: real_independence_tests_unc_ind,\n",
    "    932: real_independence_tests_unc_dep,\n",
    "    933: real_independence_tests_con_dep,\n",
    "    934: real_independence_tests_con_ind,\n",
    "    \n",
    "    941: real_independence_tests_unc_ind,\n",
    "    942: real_independence_tests_unc_dep,\n",
    "    943: real_independence_tests_con_dep,\n",
    "    944: real_independence_tests_con_ind,\n",
    "    \n",
    "    951: real_independence_tests_unc_ind,\n",
    "    952: real_independence_tests_unc_dep,\n",
    "    953: real_independence_tests_con_dep,\n",
    "    954: real_independence_tests_con_ind,\n",
    "}\n",
    "\n",
    "def get_sample_data(node_collection, num_samples):\n",
    "    node_collection.reset()\n",
    "    return node_collection.get(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def partition_dataframe(df, n):\n",
    "    total_rows = len(df)\n",
    "    partition_size = math.ceil(total_rows / n)\n",
    "    \n",
    "    partitions = []\n",
    "    for i in range(n):\n",
    "        start_idx = i * partition_size\n",
    "        end_idx = min((i + 1) * partition_size, total_rows)\n",
    "        partition = df[start_idx:end_idx]\n",
    "        partitions.append(partition)\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_servers(client_configurations, experiment_name, data, tikhonov_lambda=0, features_per_client=None, max_regressors=None):\n",
    "    servers = {}    \n",
    "\n",
    "    for splits in client_configurations:\n",
    "        if features_per_client is None:\n",
    "            clients = {i:fedci.Client(chunk) for i,chunk in enumerate(partition_dataframe(data, splits))}\n",
    "            #clients = {i:fedci.Client(pl.from_pandas(chunk)) for i,chunk in enumerate(np.array_split(data.to_pandas(), splits))}\n",
    "        else:\n",
    "            clients = {i:fedci.Client(chunk[random.sample(list(chunk.columns),features_per_client)])\n",
    "                       for i,chunk in enumerate(partition_dataframe(data, splits))}\n",
    "            #clients = {i:fedci.Client(pl.from_pandas(chunk[random.sample(list(chunk.columns),features_per_client)]))\n",
    "            #           for i,chunk in enumerate(np.array_split(data.to_pandas(), splits))}\n",
    "\n",
    "        servers[server_id_pattern.format(experiment_name, splits)] = fedci.Server(clients, tikhonov_lambda=tikhonov_lambda, max_regressors=max_regressors+1)\n",
    "    return servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_tests(available_data):\n",
    "\n",
    "    possible_tests = []\n",
    "    max_conditioning_set_size = min(len(available_data), max_regressors) if max_regressors is not None else len(available_data)\n",
    "\n",
    "    for y_var in available_data:\n",
    "        set_of_regressors = available_data - {y_var}\n",
    "        for x_var in set_of_regressors:\n",
    "            set_of_conditioning_variables = set_of_regressors - {x_var}\n",
    "            conditioning_sets = chain.from_iterable(combinations(set_of_conditioning_variables, r) for r in range(0,max_conditioning_set_size))\n",
    "            possible_tests.extend([(y_var, x_var, sorted(list(s_labels))) for s_labels in conditioning_sets])\n",
    "            \n",
    "    return possible_tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycit import citest, itest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mixed_independence(continuous, categorical):\n",
    "    # ANOVA\n",
    "    categories = np.unique(categorical)\n",
    "    groups = [continuous[categorical == category] for category in categories]\n",
    "    _, p_value = stats.f_oneway(*groups)\n",
    "    #print(f\"ANOVA F-statistic: {f_statistic}, p-value: {p_value}\")\n",
    "\n",
    "    # If categorical is binary, you can also use point-biserial correlation\n",
    "    #if len(categories) == 2:\n",
    "    #    point_biserial_corr, p_value = stats.pointbiserialr(categorical, continuous)\n",
    "    #    print(f\"Point-biserial correlation: {point_biserial_corr}, p-value: {p_value}\")\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_local_fci(df, labels, alpha=0.05):\n",
    "    with (ro.default_converter + pandas2ri.converter).context():\n",
    "        ro.r['source']('../scripts/aggregation.r')\n",
    "        aggregate_ci_results_f = ro.globalenv['aggregate_ci_results']\n",
    "        \n",
    "        d = [('citestResults', ro.conversion.get_conversion().py2rpy(df)), ('labels', ro.StrVector(labels))]\n",
    "        od = OrderedDict(d)\n",
    "        lv = ro.ListVector(od)\n",
    "\n",
    "        result = aggregate_ci_results_f([lv], alpha)\n",
    "\n",
    "        pag = [x[1].tolist() for x in result['G_PAG_List'].items()][0]\n",
    "        pag_labels = [list(x[1]) for x in result['G_PAG_Label_List'].items()][0]\n",
    "        \n",
    "    return pag,pag_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from collections import OrderedDict\n",
    "import rpy2.robjects as robjs\n",
    "\n",
    "def get_ground_truth_tests_mxm(data, possible_tests, do_symmetric_tests=True):\n",
    "    ground_truth_tests = []\n",
    "\n",
    "    with (ro.default_converter + pandas2ri.converter).context():\n",
    "        ro.r['source']('./ci.r')\n",
    "        calculate_independence_f = ro.globalenv['independence_test']\n",
    "        \n",
    "        # iterate tests\n",
    "        for test in possible_tests:\n",
    "            if do_symmetric_tests and test[0] > test[1]:\n",
    "                continue\n",
    "            x,y,z = test\n",
    "            z = robjs.r(\"NULL\") if len(z) == 0 else z\n",
    "            \n",
    "            pvalue = calculate_independence_f(data.to_pandas(), x, y, ro.StrVector(z))[0]\n",
    "            pvalue = round(pvalue,4)\n",
    "            ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), pvalue))\n",
    "    return ground_truth_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_tests(data, possible_tests, do_symmetric_tests=True):  \n",
    "    ground_truth_tests = []\n",
    "\n",
    "    for test in possible_tests:\n",
    "        if do_symmetric_tests and test[0] > test[1]:\n",
    "            continue\n",
    "        if len(test[2]) > 0:\n",
    "            X = data[test[0]].to_numpy()\n",
    "            Y = data[test[1]].to_numpy()\n",
    "            Z = data[test[2]].to_numpy()\n",
    "            pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            # if data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.String:\n",
    "            #     #print('A')\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     Z = data[test[2]].to_numpy()\n",
    "            #     pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            # elif data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.Float64:\n",
    "            #     #print('B')\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     Z = data[test[2]].to_numpy()\n",
    "            #     pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            # elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.String:\n",
    "            #     #print('C')\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     Z = data[test[2]].to_numpy()\n",
    "            #     pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            # elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.Float64:\n",
    "            #     #print('D')\n",
    "            #     _, pvalue = CITests.pearsonr(test[1], test[0], list(test[2]), data.cast(pl.Float64).to_pandas(), boolean=False)\n",
    "            # else:\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     Z = data[test[2]].to_numpy()\n",
    "            #     pvalue = citest(X, Y, Z, test_args={'statistic': 'mixed_cmi', 'n_jobs': 8})\n",
    "            #     #assert False, 'no fitting test'\n",
    "        else:\n",
    "            #print(test[0], test[1])\n",
    "            X = data[test[0]].to_numpy().astype(float)\n",
    "            Y = data[test[1]].to_numpy().astype(float)\n",
    "            #print(test[0], test[1])\n",
    "            #print(X, Y)\n",
    "            pvalue = itest(X, Y, test_args={'statistic': 'mixed_mi', 'n_jobs': 8})\n",
    "            \n",
    "            # if data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.String:\n",
    "            #     crosstab = pd.crosstab(data.to_pandas()[test[0]], data.to_pandas()[test[1]])\n",
    "            #     _, pvalue, _, _ = stats.chi2_contingency(crosstab)\n",
    "            # elif data.schema[test[0]] == pl.String and data.schema[test[1]] == pl.Float64:\n",
    "            #     #print('E')\n",
    "            #     X = data[test[0]].to_numpy()\n",
    "            #     Y = data[test[1]].to_numpy().astype(float)\n",
    "            #     pvalue = test_mixed_independence(Y, X)\n",
    "            # elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.String:\n",
    "            #     #print('F')\n",
    "            #     X = data[test[0]].to_numpy().astype(float)\n",
    "            #     Y = data[test[1]].to_numpy()\n",
    "            #     pvalue = test_mixed_independence(X, Y)\n",
    "            # elif data.schema[test[0]] == pl.Float64 and data.schema[test[1]] == pl.Float64:\n",
    "            #     #print('G')\n",
    "            #     v0 = data[test[0]]\n",
    "            #     v1 = data[test[1]]\n",
    "            #     _, pvalue = stats.pearsonr(v0, v1)\n",
    "            # #elif data.schema[test[0]] == pl.Int32 and data.schema[test[1]] == pl.Float64:\n",
    "            # else:\n",
    "            #     X = data[test[0]].to_numpy().astype(float)\n",
    "            #     Y = data[test[1]].to_numpy().astype(float)\n",
    "            #     pvalue = itest(X, Y, test_args={'statistic': 'mixed_mi', 'n_jobs': 8})\n",
    "            #     #assert False, 'no fitting test w/o conditiong set'\n",
    "        pvalue = round(pvalue,4)\n",
    "\n",
    "        #print(test, pvalue)\n",
    "                \n",
    "        ground_truth_tests.append(EmptyLikelihoodRatioTest(test[0], test[1], list(test[2]), pvalue))\n",
    "    return ground_truth_tests\n",
    "# TODO: with and without conditioning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_categories_in_regression_sets(tests, reversed_category_expressions):\n",
    "    #updated_tests = []\n",
    "    for test in tests:\n",
    "        test.X_labels = sorted(list(set([reversed_category_expressions[l] if l in reversed_category_expressions else l for l in test.X_labels])))\n",
    "    return tests\n",
    "\n",
    "def group_categorical_likelihood_tests(tests, category_expressions, reversed_category_expressions):\n",
    "    #category_expressions = servers['dag_chain4_1c'].category_expressions\n",
    "    #reversed_category_expressions = servers['dag_chain4_1c'].reversed_category_expressions\n",
    "    #tests = server_ci_tests['dag_chain4_1c']\n",
    "\n",
    "    updated_tests = []\n",
    "    for test in tests:\n",
    "        if test.y_label not in reversed_category_expressions:\n",
    "            updated_tests.append(test)\n",
    "            continue\n",
    "        \n",
    "        category_label = reversed_category_expressions[test.y_label]\n",
    "        \n",
    "        # Only run if the current test is the first category. This avoids duplicate tests\n",
    "        if category_expressions[category_label][0] != test.y_label:\n",
    "            continue\n",
    "        \n",
    "        categorical_test_group = []\n",
    "        for test_lookup in tests:\n",
    "            if test_lookup.y_label in category_expressions[category_label] and test_lookup.x_label == test.x_label and sorted(test_lookup.s_labels) == sorted(test.s_labels):\n",
    "                categorical_test_group.append(test_lookup)\n",
    "                \n",
    "        lrt = CategoricalLikelihoodRatioTest(category_label, [t.t0 for t in categorical_test_group], [t.t1 for t in categorical_test_group], len(category_expressions[category_label]))\n",
    "        updated_tests.append(lrt)\n",
    "        \n",
    "    return updated_tests\n",
    "\n",
    "\n",
    "def group_ordinal_likelihood_tests(tests, ordinal_expressions, reversed_ordinal_expressions):\n",
    "    #category_expressions = servers['dag_chain4_1c'].category_expressions\n",
    "    #reversed_category_expressions = servers['dag_chain4_1c'].reversed_category_expressions\n",
    "    #tests = server_ci_tests['dag_chain4_1c']\n",
    "\n",
    "    updated_tests = []\n",
    "    for test in tests:\n",
    "        if test.y_label not in reversed_ordinal_expressions:\n",
    "            updated_tests.append(test)\n",
    "            continue\n",
    "        \n",
    "        category_label = reversed_ordinal_expressions[test.y_label]\n",
    "        #print(category_label)\n",
    "        \n",
    "        # Only run if the current test is the first category. This avoids duplicate tests\n",
    "        if ordinal_expressions[category_label][0] != test.y_label:\n",
    "            continue\n",
    "        \n",
    "        categorical_test_group = []\n",
    "        for test_lookup in tests:\n",
    "            if test_lookup.y_label in ordinal_expressions[category_label] and test_lookup.x_label == test.x_label and sorted(test_lookup.s_labels) == sorted(test.s_labels):\n",
    "                categorical_test_group.append(test_lookup)\n",
    "                \n",
    "        lrt = OrdinalLikelihoodRatioTest(category_label, [t.t0 for t in categorical_test_group], [t.t1 for t in categorical_test_group], len(ordinal_expressions[category_label]))\n",
    "        updated_tests.append(lrt)\n",
    "        \n",
    "    return updated_tests\n",
    "\n",
    "\n",
    "def get_server_test_results(servers, do_symmetric_tests=True):\n",
    "    testing_rounds = {k:v.testing_engine.finished_rounds for k,v in servers.items()}\n",
    "    testing_rounds = {k:join_categories_in_regression_sets(v, servers[k].reversed_category_expressions) for k,v in testing_rounds.items()}\n",
    "    likelihood_tests = {k:fedci.get_likelihood_tests(v) for k,v in testing_rounds.items()}\n",
    "    # fix up categorical tests\n",
    "    likelihood_tests = {k:group_categorical_likelihood_tests(v, servers[k].category_expressions, servers[k].reversed_category_expressions) for k,v in likelihood_tests.items()}\n",
    "    \n",
    "    likelihood_tests = {k:group_ordinal_likelihood_tests(v, servers[k].ordinal_expressions, servers[k].reversed_ordinal_expressions) for k,v in likelihood_tests.items()}\n",
    "    \n",
    "    if do_symmetric_tests:\n",
    "        likelihood_tests = {k:fedci.get_symmetric_likelihood_tests(v) for k,v in likelihood_tests.items()}\n",
    "    \n",
    "    return likelihood_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_server_evaluation(ground_truth_tests, server_ci_tests, max_regressors):\n",
    "    p_value_comparison = {k:[] for k in server_ci_tests.keys()}\n",
    "    missing_test = {k:0 for k in server_ci_tests.keys()}\n",
    "    \n",
    "    for test in ground_truth_tests:\n",
    "        for k in server_ci_tests.keys():\n",
    "            if max_regressors < len(test.s_labels):\n",
    "                continue\n",
    "            matching_test = [t for t in server_ci_tests[k] if t.y_label == test.y_label and t.x_label == test.x_label and sorted(t.s_labels) == sorted(test.s_labels)]\n",
    "            if len(matching_test) == 0:\n",
    "                print(f'No matching test in {k} for {test}')\n",
    "                #print(test)\n",
    "                #print(server_ci_tests[k])\n",
    "                #raise Exception('lol')\n",
    "                missing_test[k] += 1\n",
    "                continue\n",
    "            assert len(matching_test) == 1\n",
    "            matching_test = matching_test[0]          \n",
    "            p_value_comparison[k].append((matching_test.p_val, test.p_val))\n",
    "        \n",
    "    missing_test = {k:v/len(ground_truth_tests) if len(ground_truth_tests) > 0 else 0 for k,v in missing_test.items()}\n",
    "    return p_value_comparison, missing_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct_alpha_thresholdings(data, alpha):\n",
    "    # first element in data is from fedci\n",
    "    tp = sum([1 for a,b in data if (a > alpha and b > alpha)]) / len(data)\n",
    "    tn = sum([1 for a,b in data if (a < alpha and b < alpha)]) / len(data)\n",
    "    # t1: false positives\n",
    "    # positive: calling independence\n",
    "    fp = sum([1 for a,b in data if (a > alpha and b < alpha)]) / len(data)\n",
    "    # t2: false negatives\n",
    "    fn = sum([1 for a,b in data if a < alpha and b > alpha]) / len(data)\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def count_correct_pval(data, tolerance=1e-4):\n",
    "    c = sum([1 for a,b in data if abs(a-b)<tolerance]) / len(data)\n",
    "    return c\n",
    "\n",
    "def evaluate_results(p_value_comparison, alphas, tolerance):\n",
    "    result_alpha = {}\n",
    "    result_equality = {}\n",
    "    for k,v in p_value_comparison.items():\n",
    "        result_alpha[k] = {}\n",
    "        result_equality[k] = count_correct_pval(v, tolerance)\n",
    "        for alpha in alphas:\n",
    "            result_alpha[k][alpha] = count_correct_alpha_thresholdings(v,alpha)\n",
    "            \n",
    "    return result_alpha, result_equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records(servers, name, total_samples, alpha_tests, equality_tests, p_val_comparisons, missed_tests, total_features, features_per_client, comparison_category, tikhonov_lambda, do_symmetric_tests, max_regressors, llf_neg_prob_fix):\n",
    "    results = []\n",
    "    for server_id in servers.keys():\n",
    "        server = servers[server_id]\n",
    "        alpha_test = alpha_tests[server_id]\n",
    "        \n",
    "        r = {\n",
    "            'name': name,\n",
    "            'num_clients': len(server.clients),\n",
    "            'num_samples': total_samples,\n",
    "            'comparison_category': comparison_category,\n",
    "            'same_p_val': equality_tests[server_id],\n",
    "            'missed_tests': missed_tests[server_id],\n",
    "            'max_regressors': max_regressors,\n",
    "            'total_features': total_features,\n",
    "            'features_per_client': features_per_client,\n",
    "            'tikhonov_lambda': tikhonov_lambda,\n",
    "            'llf_neg_prob_fix': llf_neg_prob_fix,\n",
    "            'symmetric': do_symmetric_tests,\n",
    "            'predicted_p_vals': [p[0] for p in p_val_comparisons[server_id]],\n",
    "            'true_p_vals': [p[1] for p in p_val_comparisons[server_id]],\n",
    "        }\n",
    "\n",
    "        for alpha, alpha_result in alpha_test.items():\n",
    "            rc = r.copy()\n",
    "            rc['alpha'] = alpha\n",
    "            tp, tn, fp, fn = alpha_result\n",
    "            rc['tp'] = tp\n",
    "            rc['tn'] = tn\n",
    "            rc['fp'] = fp\n",
    "            rc['fn'] = fn\n",
    "            results.append(rc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def write_records(i, file, data):\n",
    "    if len(data) == 0:\n",
    "        return\n",
    "    with open(file.format(i), 'a') as f:\n",
    "        for d in data:\n",
    "            ds = json.dumps(d) + '\\n'\n",
    "            f.write(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(i, client_configurations, llf_neg_prob_fix=None, total_samples=1000, max_regressors=None, features_to_reduce=None, tikhonov_lambda=0, do_symmetric_tests=True, do_write_records=True):\n",
    "    #print('Step 1/6 --> Setup')\n",
    "    #data = pl.read_parquet(f'./fedci/testdata-{i}.parquet')\n",
    "    #TOTAL_SAMPLES = len(data)\n",
    "    data = get_sample_data(ncs[i], total_samples)\n",
    "    \n",
    "    total_features = len(ncs[i].nodes)\n",
    "    features_per_client = total_features if features_to_reduce is None else total_features - features_to_reduce\n",
    "    \n",
    "    experiment_name = ncs[i].name\n",
    "    \n",
    "    servers = get_servers(client_configurations, experiment_name, data, tikhonov_lambda, features_per_client, max_regressors)\n",
    "    #servers = get_servers([3], experiment_name, data, tikhonov_lambda, features_per_client)\n",
    "\n",
    "    #print([len(c.data) for c in servers[f'dag_{experiment_name}_{3}c'].clients.values()])\n",
    "\n",
    "    #print('Step 2/6 --> Run Tests')\n",
    "    for server in servers.values(): server.run_tests()\n",
    "\n",
    "    #print('Step 3/6 --> Collect Results')\n",
    "    possible_tests = get_possible_tests(set(data.columns))\n",
    "    server_ci_tests = get_server_test_results(servers, do_symmetric_tests=do_symmetric_tests) \n",
    "    \n",
    "    comparison_tests_collection = []\n",
    "    ground_truth_tests = get_ground_truth_tests_mxm(data, possible_tests, do_symmetric_tests=do_symmetric_tests)\n",
    "    comparison_tests_collection.append(('pooled_citest', ground_truth_tests))\n",
    "    \n",
    "    if i in ncs_independences:\n",
    "        real_independences = ncs_independences[i].copy()\n",
    "        if not do_symmetric_tests:\n",
    "            real_independences += [EmptyLikelihoodRatioTest(t.x_label, t.y_label, t.s_labels, t.p_val) for t in real_independences]\n",
    "        comparison_tests_collection.append(('ground_truth', real_independences))\n",
    "    #ground_truth_tests = real_indep3 # todo: maybe add addition call of prepare_server_evaluation with prefix for different types of ground truth tests\n",
    "    \n",
    "    for comparison_name, comparison_tests in comparison_tests_collection:\n",
    "\n",
    "        #print('Step 4/6 --> Prepare Evaluation')\n",
    "        p_val_comparisons, missed_tests = prepare_server_evaluation(comparison_tests, server_ci_tests, max_regressors)\n",
    "\n",
    "        #print('Step 5/6 --> Run Evaluation')\n",
    "        alpha_tests, equality_tests = evaluate_results(p_val_comparisons, alpha_comparisons, equality_tolerance)\n",
    "\n",
    "        #print('Step 6/6 --> Log Results')\n",
    "        records = get_records(servers,\n",
    "                              experiment_name,\n",
    "                              total_samples,\n",
    "                              alpha_tests,\n",
    "                              equality_tests,\n",
    "                              p_val_comparisons,\n",
    "                              missed_tests,\n",
    "                              total_features,\n",
    "                              features_per_client,\n",
    "                              comparison_name,\n",
    "                              tikhonov_lambda,\n",
    "                              do_symmetric_tests,\n",
    "                              max_regressors,\n",
    "                              llf_neg_prob_fix)\n",
    "        \n",
    "        if do_write_records:\n",
    "            write_records(i, log_filepattern, records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove non-zero correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIKHONOV_LAMBDA = 1e-7\n",
    "TIKHONOV_LAMBDA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_tests = [\n",
    "    911,912,913,914,\n",
    "    921,922,923,924,\n",
    "    931,932,933,934,\n",
    "    941,942,943,944,\n",
    "    951,952,953,954\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_regressors = 1#None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "llf_neg_prob_fix = 'clipping'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37723ce0cd324036bf4bca67a3ec44fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac23f5db83a458783c592775e82d092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tkl in [1e-10, 0]:#, 1e-7]:\n",
    "    for i in tqdm(range(10), leave=False):\n",
    "        for sym in [True]:\n",
    "            for samples in [300, 400, 500, 600, 700, 800, 900, 1000]:#, 500, 600, 700, 800, 900, 1000]:\n",
    "                for id_ in [913]:#paper_tests:#[1,2,3,4,61,62,63,64,71,72,73,74,81,82,83,84]:#paper_tests: #1,2,3,4,81,82,83,84  # 72 # ,81,82,83,84\n",
    "                    #for reductions in [0,1]:\n",
    "                    process(id_,\n",
    "                            [1,3,5],\n",
    "                            total_samples=samples,\n",
    "                            do_symmetric_tests=sym,\n",
    "                            tikhonov_lambda=tkl,\n",
    "                            features_to_reduce=None,\n",
    "                            max_regressors=max_regressors,\n",
    "                            llf_neg_prob_fix=llf_neg_prob_fix,\n",
    "                            do_write_records=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 1000\n",
    "features_to_reduce = None\n",
    "client_configurations = [3]\n",
    "tikhonov_lambda = 0\n",
    "do_symmetric_tests = True\n",
    "i = 63\n",
    "\n",
    "\n",
    "\n",
    "data = get_sample_data(ncs[i], total_samples)\n",
    "    \n",
    "total_features = len(ncs[i].nodes)\n",
    "features_per_client = total_features if features_to_reduce is None else total_features - features_to_reduce\n",
    "\n",
    "experiment_name = ncs[i].name\n",
    "\n",
    "servers = get_servers(client_configurations, experiment_name, data, tikhonov_lambda, features_per_client)\n",
    "#servers = get_servers([3], experiment_name, data, tikhonov_lambda, features_per_client)\n",
    "\n",
    "#print([len(c.data) for c in servers[f'dag_{experiment_name}_{3}c'].clients.values()])\n",
    "\n",
    "#print('Step 2/6 --> Run Tests')\n",
    "for server in servers.values(): server.run_tests()\n",
    "\n",
    "#print('Step 3/6 --> Collect Results')\n",
    "possible_tests = get_possible_tests(set(data.columns))\n",
    "server_ci_tests = get_server_test_results(servers, do_symmetric_tests=do_symmetric_tests) \n",
    "\n",
    "comparison_tests_collection = []\n",
    "ground_truth_tests = get_ground_truth_tests_mxm(data, possible_tests, do_symmetric_tests=do_symmetric_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc3.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3_000, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>D</th><th>B</th><th>C</th><th>A</th></tr><tr><td>str</td><td>f64</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>-0.116479</td><td>1</td><td>-0.816289</td></tr><tr><td>&quot;1&quot;</td><td>-0.609824</td><td>1</td><td>1.344061</td></tr><tr><td>&quot;2&quot;</td><td>1.99322</td><td>2</td><td>-0.160638</td></tr><tr><td>&quot;1&quot;</td><td>0.11312</td><td>1</td><td>-0.291801</td></tr><tr><td>&quot;1&quot;</td><td>-1.337795</td><td>2</td><td>0.215637</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;1&quot;</td><td>-0.959623</td><td>2</td><td>1.372909</td></tr><tr><td>&quot;1&quot;</td><td>-0.937223</td><td>2</td><td>0.875208</td></tr><tr><td>&quot;1&quot;</td><td>-0.535653</td><td>1</td><td>0.036071</td></tr><tr><td>&quot;1&quot;</td><td>-0.186797</td><td>1</td><td>0.326439</td></tr><tr><td>&quot;2&quot;</td><td>0.280547</td><td>1</td><td>-1.700486</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3_000, 4)\n",
       "┌─────┬───────────┬─────┬───────────┐\n",
       "│ D   ┆ B         ┆ C   ┆ A         │\n",
       "│ --- ┆ ---       ┆ --- ┆ ---       │\n",
       "│ str ┆ f64       ┆ i32 ┆ f64       │\n",
       "╞═════╪═══════════╪═════╪═══════════╡\n",
       "│ 1   ┆ -0.116479 ┆ 1   ┆ -0.816289 │\n",
       "│ 1   ┆ -0.609824 ┆ 1   ┆ 1.344061  │\n",
       "│ 2   ┆ 1.99322   ┆ 2   ┆ -0.160638 │\n",
       "│ 1   ┆ 0.11312   ┆ 1   ┆ -0.291801 │\n",
       "│ 1   ┆ -1.337795 ┆ 2   ┆ 0.215637  │\n",
       "│ …   ┆ …         ┆ …   ┆ …         │\n",
       "│ 1   ┆ -0.959623 ┆ 2   ┆ 1.372909  │\n",
       "│ 1   ┆ -0.937223 ┆ 2   ┆ 0.875208  │\n",
       "│ 1   ┆ -0.535653 ┆ 1   ┆ 0.036071  │\n",
       "│ 1   ┆ -0.186797 ┆ 1   ┆ 0.326439  │\n",
       "│ 2   ┆ 0.280547  ┆ 1   ┆ -1.700486 │\n",
       "└─────┴───────────┴─────┴───────────┘"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc3.get(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc3.get(3000)[:1000].write_parquet('./wicked-data-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# for f in glob.glob('./log-9*.ndjson'):\n",
    "#     df = pl.read_ndjson(f)\n",
    "#     if 'max_regressors' in df.schema:\n",
    "#         df = df.filter(pl.col('max_regressors') != 1)\n",
    "#     df.write_ndjson(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
